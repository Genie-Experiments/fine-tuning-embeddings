{
    "queries": {
        "d0945bf1-6f53-4ff0-88b2-28ecb573d3a2": "Compare and contrast the features and functionalities of the Advance RAG tools mentioned in the document, such as Query Optimization, Document Optimization, Embedding Models, Context Expansion, and Context Rerankers. How do these tools contribute to improving the retrieval and ranking of information in a search system?",
        "a6595782-4e99-4766-b616-e0fd56440b35": "Discuss the implementation and benefits of integrating CodeHawk with GitLab as outlined in the document. How does running CodeHawk scripts with Docker in GitHub Actions enhance the code review process, and what are the potential features that CodeHawk offers for code analysis and improvement?",
        "c95eeae1-3905-43b5-995e-fad0c67c9b79": "Explain the significance of RAG/LLM based log analysis in incident management and root cause analysis. How can GraphRAG and multimodal RAG & open source embedding models enhance this process?",
        "391be456-47a7-4990-bf5a-83356addb16a": "Describe the steps involved in LLM testing, including exploring Confluence pages, running existing Github repositories, and conducting manual testing for PR review. How can fine-tuning embeddings and utilizing SQL agents contribute to the effectiveness of this testing process?",
        "16bf3e4c-dc14-4d90-8f05-9c599c1e8983": "How can structuring research and development tasks be beneficial for a project, according to the provided guidelines?",
        "86796804-452a-45cc-8b20-38f35eedcd55": "Why is it important to add a brief conclusion/findings in a Confluence page linked to each task before marking it as complete, as outlined in the document?",
        "ad3f265a-5fac-44bd-b322-30f11434d804": "How does the research project for masters in Data analytics aim to build a smart question answering system for PDF files using BERT-based transformers?",
        "0d2336d9-a5fd-425f-97d5-a6dad7c775aa": "Can you explain the two metrics used from evaluators, Response Matching/Answer Relevance and Context Relevance, in the context of evaluating the question answering system for PDF files?",
        "4ce471e3-ef67-4165-b756-d32778237f91": "Explain the purpose and functionality of the Document class defined in the code for comparing LLMS for Rag Over Code. How does it contribute to the overall process of splitting documents using Langchain?",
        "a71ef6b9-c7a6-463f-92d2-88e9183315e8": "Compare and contrast the pipelines GENIE-5 and GENIE-6 in terms of the language models and embeddings used, as well as the specific tasks they aim to accomplish. How do they differ in their approach to evaluating answer quality and performing text summarization?",
        "f70a276a-aca5-44ec-ac2f-ddf993478800": "How is the llm Claude - V2 from bedrock service utilized in the RAG evaluation process?",
        "4829589f-5f47-4e25-b361-d2848cec3b53": "What are the metrics used to evaluate the dataset in the RAG evaluation, and how do they contribute to assessing the quality of the generated answers?",
        "0c89a36a-2b08-4604-a1b8-571ff87d5d4f": "Explain the main goal of developing a comprehensive taxonomy for retrieval augmented generation models and how it can benefit the RAG pipeline. Provide examples of tools and techniques used in different components of the RAG pipeline.",
        "b46c48eb-b6a6-437e-9d68-4af6ab407f8f": "Describe the six components of the RAG pipeline, including their roles and functions. How do these components work together to enhance the retrieval and generation processes in advanced RAG models?",
        "0018ab57-2d16-41af-aa50-ac8bcc6cd6f4": "Compare and contrast the main focus and key features of the tools LlamaIndex, LangChain, Haystack, and Hugging Face based on the information provided in the document.",
        "2bbd2718-ad33-49fd-a810-2b4a4b514012": "Discuss the pros and cons of using LlamaIndex, LangChain, Haystack, and Hugging Face for different NLP tasks, considering their supported languages, main focus, key features, and customization options.",
        "2999dbc9-6cc4-477a-9d36-e71a545a43f1": "How does the performance optimization of the Advance Rag tool compare to other tools in terms of efficient indexing and retrieval? Provide examples of specific tasks where tailored solutions may outperform Advance Rag.",
        "2c0c8c23-cbbe-4509-9e9d-e995c129b2c1": "Compare and contrast the integration capabilities of Advance Rag with various model providers and databases to those of other tools like LangSmith and Transformers. How does the availability of pre-trained models from different providers impact the usability of Advance Rag in different use cases?",
        "9b4ccf44-ef03-48a2-a57b-58877454b89b": "Explain the difference between chunking strategies and hierarchical index structure in the context of information retrieval. How do these two concepts contribute to the efficiency of search algorithms?",
        "ad1ed110-fab9-46ff-a297-9a3a16859702": "Compare and contrast the reranking techniques mentioned in the document - Cohere reranker, BGE reranker, ColBERT reranker, and Jina reranker. How do these techniques enhance the relevance and accuracy of search results in different scenarios?",
        "f62460ae-2c85-4b9b-9960-7b52ce9d3b13": "How do the query routing techniques offered by Llamaindex differ from those offered by Langchain and Haystack?",
        "b6847ac3-17eb-4dea-bb8e-4f12ed72f3a5": "Can you explain the concept of \"Hybrid LLM: Cost-Efficient and Quality-Aware Query Routing\" as presented by Hugging Face in the context of query routing engines?",
        "e9e35a3d-667c-409f-ae96-f9dd8d3f4e32": "Explain the different evaluation tools and techniques mentioned in the context information for query optimization. How do these tools and techniques contribute to improving query performance?",
        "e54de0d2-8d19-4f45-aa5d-b82c84f381d7": "Compare and contrast the metrics for evaluation, namely Answer Similarity, Context Precision, and Context Similarity. How are these metrics important in assessing the effectiveness of query optimization strategies?",
        "59c987d9-0a7e-44f2-9b84-ff3cdc66defc": "How does the HYDE technique outperform other techniques in the context of expanding search space and reducing hallucinations?",
        "4aaeb00d-bc2c-4fc3-9315-748d34823c37": "In what ways do the techniques mentioned in the conclusion differ in their performance on queries, and why do Hyde and query rewriting stand out from the rest?",
        "17938002-8247-4aba-a50f-ba24aba0ddae": "How does the experiment results for document optimization vary when using PDF documents compared to code files from a GitHub repository? Provide specific percentages for both total queries and precision baseline.",
        "5ee3491d-72d9-4a22-88a3-3219fa806fe3": "Compare and contrast the results of the experiment between semantic chunking and hierarchical indexing in terms of context precision and answer similarity. Include specific percentages for each type of chunking/indexing method mentioned in the context information.",
        "533d1c8d-907b-452b-81b1-72c22150aaea": "Compare and contrast the performance of the chunking techniques used in the experiment, highlighting the advantages of character text splitting over other methods. How did metadata integration impact the accuracy scores in the experiment?",
        "394d1aaa-ae13-420f-b3cc-25f2dffe9435": "Discuss the differences between hierarchical indexing and KI graphs in terms of storage, retrieval, and complexity. How do these structures compare to traditional chunking techniques in the context of the experiment?",
        "a9fc5cf3-6b1d-48e0-ab13-f2933ee54f30": "What are the different types of data used in the experiment results, and how many queries were conducted for each type of data?",
        "d1ea147d-d599-469e-aabc-daef8d47f4a3": "Compare the performance metrics (precision, validation scores, etc.) of the various embedding models (Ragas, Tonic, Uptrain, etc.) mentioned in the context information.",
        "faaff5d4-2af2-4775-b30d-0f9d5da7e3dd": "Compare the performance of bedrock embeddings to minillm embeddings in the experiment mentioned in the conclusion. How do BGE-m3 embeddings stand out in terms of scores and parameters?",
        "956bc927-9c72-499f-bc46-8bb2c32550f6": "Why is it mentioned that BGE-m3 and LLM embedders cannot be integrated into the original pipeline and require GPU units for processing? How does the T4-GPU perform in terms of embedding formation time?",
        "393c12b8-3906-42a1-b756-5bfb1bdafb20": "How does the Recursive Retrieval technique work in improving relevance and accuracy in information retrieval processes?",
        "c624018a-c7f2-46b1-8bb6-51840efba4bc": "In what scenarios would the Auto Merging Retrieval technique be most suitable for use, and how does it help in creating comprehensive reports or synthesizing information from diverse documents?",
        "0b8592bd-49d0-4b61-a30c-4bff7d79c30a": "Explain the benefits of using Sentence Window Retrieval and Auto Merging Retrieval in the context of code analysis and PR reviews. How do these techniques improve the overall performance compared to the baseline experiment?",
        "6796707e-7ad6-4151-b97d-3b370dfec9ab": "Discuss the limitations of Recursive Retrieval in the given context and why it was not tested in the pipeline. How does the partial satisfaction of its concepts still make it a better option than the baseline experiment?",
        "980b0dcb-d9ac-4e8e-939f-54b10e93f9dc": "Explain the process of context reranking in the context of document retrieval. How does it impact the ranking of documents based on relevance scores?",
        "8f991b3a-e6f8-4ace-90cb-e8a4d5eff3e6": "Compare and contrast the experimental results of the Uptrain and Tonic validation sets in the context of context rerankers. What insights can be drawn from the differences in performance between the two sets?",
        "7cddd99b-e715-4907-a2ee-6c4be487fdd3": "Compare and contrast the performance of the Colbert reranker and the Cohere reranker based on the conclusions drawn from the data provided. How do their response times on GPU and CPU differ?",
        "de6a0c0e-82bf-4272-bad5-85d3f99974f8": "Discuss the impact of using rewritten queries on the scores generated by the Rag Fusion reranker. How does the quality of the rewritten queries affect the reranking process, and what alternative method can be used to improve the scores?",
        "53fbd997-0f1e-4455-9395-f2fc6f3776c2": "Compare and contrast the efficiency of Bert-based Rerankers and GPT-based Rerankers in terms of time taken on CPU and GPU. How does the architecture of each model contribute to their performance on these platforms?",
        "7ffa5cb3-6d41-4580-ba10-d266dbe7ab52": "Analyze the performance of different models, such as Jina AI, BGE, and Long Context Reorder, in terms of the time taken for ranking. How do the characteristics of each model, such as size, nature, and processing requirements, impact their efficiency in comparison to each other?",
        "d65837b4-2755-43bb-bcbd-f8ab7ed76beb": "Compare and contrast the performance and reliability of TruLens, Falcon Evaluate, and Ragas based on the accuracy scores and consistency of their evaluations. How do these frameworks differ in their ability to integrate with RAG pipelines and provide nuanced evaluations?",
        "91a72c92-3963-441e-9223-eef63ec0cc83": "Discuss the strengths and weaknesses of Tonic Validate, Uptrain, and DeepEval as testing frameworks for evaluation. How do these frameworks differ in terms of performance, consistency, and integrability into custom pipelines?",
        "99a0e454-90ef-4a72-9b2c-dc863329d8e1": "How does the Llama index concept of multiple query engines help in implementing Retrieval Augmented Generation over different types of data?",
        "db024edf-88b6-4cd0-9503-86aab0cecdb9": "What are the challenges faced in directing incoming questions to the correct QueryEngine using the RetrieverRouterQuery engine, and how can these challenges be addressed?",
        "0e96b496-3f65-492d-9324-11f0d6ca6a1d": "How does the Pandas Query Engine differ from the Retriever Router Query Engine in terms of functionality and use cases?",
        "d1b5aea1-79f1-4a8d-816d-f4a717b99bfc": "Can you explain the significance of LlamaIndex in the context of both the Pandas Query Engine and the Retriever Router Query Engine?",
        "2a8507ff-f643-4912-9be5-5a890bab1004": "Explain the difference between a Retriever Router Query Engine and a Structured Planning Agent in the context of resolving queries.",
        "342a0c54-9943-4ddb-a33f-a6df04bdfcd8": "Provide an example of a query that would be suitable for a Retriever Router Query Engine and explain why it is appropriate for that type of query.",
        "d8918fef-b887-40de-8e76-0345ce84bc68": "How does the Structured Planning Agent work in conjunction with a capable LLM, and why is it important to use the best available LLM for optimal performance?",
        "941e8df5-276e-470c-8ab9-253d8500f685": "Can you explain the impact of using a better model like GPT4-turbo compared to llama3-70b in the experimentation process of the Structured Planning Agent, and how it resulted in significantly better results?",
        "4fb19bfd-1cc8-440c-a04a-d2bbeb31f13c": "How does G-Eval differ from RAGAS in terms of their evaluation criteria and methodologies for assessing generated text? Provide specific examples to support your answer.",
        "c8608e02-44d5-4718-ba2b-a001f5fefb33": "Discuss the potential impact of prompt-based evaluation on the outcomes of G-Eval and RAGAS. How might the construction of prompts influence the evaluation results in each methodology?",
        "b44cc2a9-c6cf-4b74-98a5-94d9468b08b5": "How can the TruLens tool be utilized to improve the quality of text retrieval and optimize app configuration?",
        "0b7f60b7-2901-49c3-9dcd-0253ea612d8d": "Explain the importance of using separate evaluation LLMs, such as the Phoenix LLM Eval Library, in assessing the performance of LLM applications.",
        "b7fb0d9d-e3e5-4783-a3c3-3fc75d37310d": "How does G-Eval differ from RAGAS in terms of evaluation criteria and methodology for assessing generated text?",
        "7d333a5d-0816-4192-9f25-17e8677b1cde": "Discuss the potential impact of prompt-based evaluation on the outcomes of G-Eval and RAGAS evaluations, considering the nuances in scoring granularity and criteria emphasis.",
        "02f46966-98b8-4fc9-a192-c6c1976877ca": "How does TruLens help in detecting and mitigating hallucination, improving retrieval quality, optimizing app configuration, and verifying summarization quality?",
        "1c211ffc-150f-4aec-9e14-c6b7d258e672": "In the context of content moderation, what are the key evaluation parameters for recall and precision on toxicity and bias?",
        "de2c98d1-6f09-497d-a2db-508a95c0f8b7": "How does the Retriever Router Query engine from llama-index function in terms of routing incoming questions to the correct query engine? Discuss its efficacy and any limitations observed during the experimental setup.",
        "1abe0ce6-dae0-4c5e-b6ec-dc27764e9cb4": "As a Teacher/Professor, explain the primary purpose of the Retriever Router Query engine and its suitability for systems with different types of data. How does it differ in handling questions compared to systems built on the same underlying data but different types of indexes?",
        "fd1f475b-e36e-4679-8726-b16ec3966ecd": "Explain the concept of recursive retrieval and its benefits when dealing with hierarchical or unstructured data. How does the IndexNode play a role in this process?",
        "149bfb34-be76-48ed-ae7a-5e68e3647205": "Discuss the limitations of using LLMS for answering questions from structured forms of data, using the example of a table containing information about billionaires. How can LlamaIndex query engines, such as pandasQueryEngine, improve the effectiveness of querying such data?",
        "f5903cb6-fa73-4faa-ac67-1718bba113f8": "Explain the concept of Recursive Retrieval and how it is utilized in the LlamaIndex system. Provide examples to support your explanation.",
        "fe16a8dd-1ddb-4ab5-a05d-d91a01468e1f": "How does Jerry Liu contribute to the development and promotion of Recursive Retriever + Query Engine Demo - LlamaIndex? Discuss his role and impact in the field based on the information provided in the context.",
        "f1febec1-df80-408f-8a5c-f3e1cd4d6969": "Explain the process of creating a query engine in llama-index, including the steps involved and the components required.",
        "a0c5d550-1131-4548-aab1-3852fbe40f86": "Provide an example of a query that can be processed by the JSON Query Engine in llama-index, and explain how the engine generates a valid JSON Path for the query.",
        "13377529-434b-4620-ae52-979577abb082": "What is an example of a simple, straightforward question that the JSON query engine can effectively answer?",
        "7b731fce-55be-41cb-9243-2f87750e0d14": "How does the JSON query engine perform when faced with slightly more complex questions that require fetching multiple pieces of information and using 'and' or 'or' conditions?",
        "3d44faf5-2c73-42fc-b1ff-3fc5caee79cd": "How does JSONAnalyze Query Engine help in analyzing JSON data retrieved from an API? Provide an example to illustrate its functionality.",
        "fbc8f99e-ff37-4714-a020-c302a3edae3e": "Can you explain the process of how the JSONAnalyze query engine works under the hood, as depicted in the image provided in the context?",
        "0899e55a-9573-428a-86c2-f19209bfe0f6": "How does the JSONAnalyze Query engine store data in an SQLite database?",
        "57a9d2b0-8d3d-4110-9823-d2f785be7250": "What happens when a query sent to the JSONAnalyze Query engine contains information not present in the JSON data?",
        "1e860744-49cb-4b29-874e-15ae7ad47635": "How does the LLM generate an SQL query in the context mentioned above, and what issue arises when executing this query on the SQLite database?",
        "bc7782e6-3bfe-46c1-88ad-fd0d5a30dda1": "Explain the role of the JSONalyze Query Engine in handling queries related to company names in the given scenario.",
        "ef7b2e19-5e5c-4a51-827c-98eacbbbe2be": "How can customized models be utilized in building RAG-based LLM applications, and what techniques are commonly used for fine-tuning and modifying these models?",
        "8f822742-ff90-4f0e-a841-4ce60538c321": "Explain the challenges and strategies involved in the data ingestion process for RAG-based LLM applications, including issues related to data quality, document chunking, and embedding model dependency.",
        "1bd0055c-1fd7-4df7-aba4-b3a1223bdfd2": "How can the challenges of retrieval in the context of multi-step retrieval be addressed through solutions such as hybrid search and metadata integration?",
        "1c99a588-00e4-45c8-982c-c6e4e93b8c46": "Discuss the problems and solutions related to the generation of responses in a chat engine, including issues like information overload, domain adaptation, and logically incoherent responses.",
        "477e0fe0-6b69-426a-a371-defcb21dbb35": "How does the Llama-index text-to-SQL Query engine answer questions over SQL data effectively?",
        "4bea6f3b-3020-4bf7-8e7f-75c9167f6d4a": "Can you explain the pipeline followed by the Llama-index text-to-SQL Query engine when generating SQL queries to answer user questions?",
        "e438eb46-f7d2-4400-a2b8-aee61bad742e": "Explain the difference in the pipeline used for Q/A over unstructured data versus structured data, highlighting the key steps involved in each.",
        "75a77951-bbe5-485d-afc2-b68e92ddc72c": "Provide an example of how Q/A is conducted over structured data, using the scenario of an SQL table with specific columns and a question about population data.",
        "a640e41d-8991-4e75-aa2b-ff214c930756": "What is the significance of the number 45 in the context provided?",
        "1247d2ac-0c16-405c-bda1-60e9515198f7": "How might the number 45 be relevant to a specific topic or subject area based on the context information given?",
        "77966b9a-51d8-420f-a72a-ba2a15eba6d3": "How does the ReAct agent function as a query routing entity, and what is its main purpose in the system described?",
        "803a20cb-70c5-4478-8ddb-88e1e20c1b79": "Can you explain the different query engines used in the system, including the vector query engine for PDF data, the vector query engine for tabular data, and the text-to-SQL query engine?",
        "10966c53-d7fe-4536-8822-ef98e054ff63": "How does the system correctly route the information to the PDF Query engine?",
        "774e82da-084b-4a80-9517-b63e8b763731": "What is the significance of the system correctly routing the information to the text-to-SQL engine multiple times in the given context?",
        "f719afca-76e3-4c24-96f2-66030d0ba790": "How can the system be prevented from answering questions using its own knowledge, as shown in the images provided?",
        "9d859f88-38d9-465f-a176-93f30af272fb": "Describe a task that the agent handles which involves reading a file and fetching data from a database.",
        "7c23e1be-e735-4cb7-9c71-17d75e2a1d32": "How does the ReAct agent perform in terms of routing incoming questions/tasks to the correct underlying tool?",
        "dab16b72-e218-47a7-ad79-28db6de91003": "Can you explain the advantage of combining function calling with Q/A systems in the ReAct agent?",
        "ee0f2801-3d73-4bdf-a07f-fe57146270ed": "Compare and contrast the performance of Retriever Router Query Engine, ReAct agent, and Structured Planning Agent in terms of their ability to route queries to the correct query engine.",
        "eb9c6577-6734-4348-8d3f-9e140d9e57bc": "Explain how ReAct agent stands out among the different techniques tested for query routing, highlighting its strengths in breaking down complex tasks, integrating function calling ability, and performing agentic reasoning.",
        "5ca145ab-9f62-47f7-9b96-b4fd74c24b26": "Explain the setup of the Mini-Assistant project, including the different tools developed and the role of the ReAct agent in executing tasks.",
        "3d8a7d7c-8ee2-4c65-b752-a2828a7212c7": "Describe the testing process of the Mini-Assistant system, including the tasks given, tools utilized, and the end results and reasoning behind them.",
        "28641d4d-9bab-4f9b-9418-51b5b02eb7e8": "What is the significance of utilizing ChatDB in determining the population of Tokyo and the total number of employees in the employees_info table?",
        "5adb647a-53ed-415b-9add-b5166e052c0d": "How does the policy regarding lunch and employee referrals at emumba contribute to employee satisfaction and engagement?",
        "88f208db-760f-4a6e-ae75-33683cfb5932": "In the database, how many cities have a population greater than 3 million?",
        "73259d3d-5fab-4164-9e70-09660d4c2cd8": "How often are emumba annual reviews conducted?",
        "a47f7b20-3dcd-4b32-9836-8bb6dd593f95": "How would you evaluate the effectiveness of a code review process in a software development team?",
        "06d9fcd1-9162-4d4d-9aba-1270929afcfe": "As a teacher, how would you explain the importance of code reviews to students in a computer science class?",
        "46303d79-da22-46ee-96de-8e51155d2399": "Explain the significance of the transition from Recurrent Neural Networks to the transformer architecture in the context of generative AI and language understanding. How has this revolutionized the field?",
        "95bfb124-a6d3-43a7-9f02-d7a606530903": "Describe the process of fine-tuning a language model and its importance in improving performance on specific tasks. How does instruction fine-tuning contribute to enhancing the model's abilities?",
        "ecc75297-8a9a-4d09-8557-bd166dbb22fa": "Explain the process of fine-tuning a model using Lamini as described in the implementation description. What are the key steps involved in this process?",
        "3920bb0e-2afb-440e-ad59-063867453081": "How does the use of generative AI, such as LLMs, impact businesses and society according to the course notes? Provide examples of how these programs can create new opportunities and save money.",
        "3710801d-88bb-4fa0-a59d-d16bab787af1": "How does CodeHawk Emumba facilitate faster iteration in code reviews, and what specific features enable this process?",
        "cffbbdf8-a94e-457f-865d-5b11f8ce9390": "Discuss the potential benefits of integrating CodeHawk Emumba with Jira for monitoring pull requests in relation to linked Jira issues.",
        "a4102290-9ad6-4228-823b-8d777a063d0d": "Explain the process of automating the pull request review process using LLMs via GitHub Actions. What information is dynamically retrieved and how is feedback provided on the code?",
        "204c02fc-55f8-49a2-9bd5-14792e6dcdbf": "How are open source LLMs like llama3-70b and Llama3-8b utilized in the PR review process? Discuss the integration with GitHub and the use of Docker for running scripts.",
        "056928d3-c269-4056-9fe2-5d00bf256809": "Explain the purpose of the `category.remove` function in the provided code snippet. What is the expected outcome when this function is called?",
        "860570b5-b3a0-4313-bbd7-8bc0a964684b": "Describe the role of the `errorHandler` function in the context of the code provided. How does it help in handling errors during category operations?",
        "47cd971c-0fd1-4cf9-ac49-20ddef94cc51": "Explain the purpose and functionality of the `exports.remove` function in the provided Node.js code snippet. How does it handle the removal of a category from the database, and what response does it send back upon successful deletion?",
        "a2bb15bd-8bc6-4c42-827b-f519aae408a1": "Discuss the significance of passing the entire repository context to a Language Model (LLM) for code reviews, as mentioned in the context information. How does this approach enhance the quality and accuracy of code reviews compared to only providing the file context of changed code?",
        "462248cd-62cc-40ef-89e8-5ee68789691a": "How can the LLM help identify and flag inconsistencies in code changes that deviate from established repository conventions?",
        "458b32fb-c381-4433-9140-36c9975673df": "Can you provide some examples of the improved output that the LLM can generate in relation to identifying inconsistencies in code changes?",
        "f6fe1aae-1cdf-494b-baa2-d52ca3d912a7": "What is the significance of the number 62 in the context provided?",
        "101d23b0-4a81-4703-aed2-fb2797a80b1e": "How might the number 62 be relevant to the subject matter being studied in this document?",
        "bf912abe-ac77-4c2a-9605-86050e15d7bb": "What is the significance of the number 63 in the context provided?",
        "38af6a5a-e54c-4756-8843-57606e87e3c6": "How might the number 63 be relevant to the subject matter being studied in this document?",
        "50005a22-073f-4e15-a4d3-363b1139f99b": "How does CodeHawk automate code review processes using GitLab's API and a language model? Explain the process of retrieving changed files, generating reviews, and posting them as comments on merge requests.",
        "06e1c0d4-d07c-43c5-98ae-e80772035598": "Describe the functionality of the Review Label in the CodeHawk GitLab Implementation. How does it ensure that only relevant merge requests are reviewed, and what triggers the pipeline to process a merge request for review?",
        "eed178e8-019b-4524-9048-4c6b29b7e6f3": "How can you build a Docker image to run CodeHawk scripts in GitHub Actions?",
        "7884a6d1-16d0-4a33-84db-dbab5be8d1d5": "In the GitHub Actions workflow provided, what is the purpose of the \"Get commit SHA\" step and how is it implemented?",
        "c4680fb8-6013-4b2e-9d1a-797de5734525": "Explain the purpose of the script_name step in the Docker container setup process. How does it determine which script to run based on the GitHub event name?",
        "0f31e7a2-cab3-418b-94e8-759769489cd5": "Describe the conditions under which the Docker container runs either the generate_response.py or reply_thread.py script. How does the container pass environment variables and execute the appropriate script based on the conditions specified in the GitHub actions workflow?",
        "74964bf4-7f52-423c-9770-362b84635f04": "How can a Docker image be built with the necessary environment and scripts for executing CodeHawk scripts consistently in a controlled environment?",
        "3ad08af1-5d19-4961-b221-4d20f5bfca43": "Describe the process of configuring a GitHub Actions workflow to pull and run a Docker container based on events triggered by pull requests and pull request comments.",
        "767d745d-5463-461c-825b-5922631838b1": "How does the feature of Interactive Clarification in the code review process aim to improve communication between developers and the LLM?",
        "b990cd89-472c-46b8-aa7d-8eb807793e73": "Can you explain the process by which developers can seek clarification on code review suggestions using the @CodeHawk tag in a pull request review?",
        "b406314a-d0e6-4665-9ff9-c7abd40da352": "How does the LLM-generated review differ from the developer's request for clarification in the given context?",
        "8b13d4d9-d03a-423b-a8f2-599b57aaff32": "What is the significance of the LLM's reply back to the developer in the context of the document provided?",
        "a740ee86-d782-4dc1-b592-d01d14b819f9": "What is the significance of the number 70 in the context provided?",
        "29d09f48-cdd0-47d5-89f2-58757bf80b90": "How might the number 70 be relevant to a specific topic or subject area based on the context information given?",
        "16787493-070c-42a0-a299-4ec97ceb4525": "How does the feature described in the context information work when triggered by a new pull request being created?",
        "9b950f47-fd2c-4e8e-a9a2-8be179b3b36d": "What are the two possible flows for triggering the feature that generates a summary of a pull request, as mentioned in the document?",
        "d82f82a3-d36e-4c62-9f09-1f9cfbf4e3ae": "How can integrating with JIRA Issue Tickets benefit project management processes?",
        "cc478788-7aa4-49d0-850d-88a761fc20ba": "What are some key considerations to keep in mind when setting up integration with JIRA Issue Tickets for a project?",
        "d01ea830-d79b-4a4d-94fe-b19e953401b2": "Compare and contrast the features of Code Rabbit, Maverick, and Codium AI in terms of their capabilities for providing automated code reviews and feedback on pull requests.",
        "72b06787-f5b8-41d7-bd9b-ea7f2ce0f050": "Discuss the advantages and limitations of using the free tier of Code Rabbit compared to the free version of Maverick for conducting code reviews on GitHub repositories.",
        "d17a8619-a880-4403-a616-356fe3dab3cf": "How does the Custom Prompt feature in the CodiumAI PR-Agent tool differ from the Generate Tests feature? Provide examples to illustrate the distinction between the two functionalities.",
        "ff672806-e1dc-4321-bc31-0cc7b21f2e32": "Compare and contrast the learning capabilities of Feature Code Rabbit, Maverick, and CodiumAI PR-Agent. How do these tools differ in terms of their ability to provide actionable suggestions and integrate with GitHub, GitLab, and Bitbucket?",
        "c6a625ff-6059-4b2f-8df8-7792237910d6": "How does the Development Automation PoC aim to streamline the process of code review within Github Pull Requests using advanced automation techniques? Provide a detailed explanation of the basic workflow outlined in the planning phase.",
        "cf014595-12e7-4dba-b745-ea86c50e2eeb": "Among the choices for Large Language Models (LLMs) listed, such as GPT-4, CodeLlama, Llama-2, and Mixtral, which one would you recommend selecting for the implementation plan? Justify your choice and explain how the selected LLM will be tested for its effectiveness in code generation and documentation.",
        "786c0872-7196-47f4-b8ee-ca343df4957e": "How can mutation testing be used to identify bugs in code, and what is the potential benefit of this approach?",
        "4e37487c-a3aa-468e-ab17-17d7d2076809": "Describe the proposed approach of using a smaller, faster LLM for initial code review and then passing identified issues to a larger LLM for supervision. How can this method improve code quality?",
        "410cd6d6-7dc3-464d-bbbc-3adcaba325d5": "How can you deploy your LLM on an EC2 instance using a T4 GPU on AWS? Provide a step-by-step guide starting from logging in to the AWS console to deploying the model using vllm.",
        "9af17fdd-f0ce-46bd-958f-ed24c780c242": "What are the benefits of using half (FP16) datatype and setting the tensor-parallel-size to 1 when deploying an LLM using vllm? How does this configuration impact memory usage and computational speed?",
        "ea9ecf43-9f10-4756-9530-382ec612df8d": "How can you troubleshoot connection issues when encountering a \"connection refused\" error while deploying LLMs using Ollama?",
        "8b5fd1d8-0a64-4306-89a3-b82c26a8155c": "Describe the steps involved in setting up and starting the Ollama service for deploying LLMs on an Azure instance.",
        "f02dcbfb-f5c6-4bdd-9cd4-7e1bc9d907a3": "What are the key differences between OpenLLM and Litellm in terms of deployment options and potential issues that may arise during deployment?",
        "4e2dff90-93be-45c7-acc4-bf85aa022246": "How can one resolve the issue of \"Connection Timeout\" when trying to access an endpoint after successfully deploying a model using Litellm? Provide step-by-step instructions on adding inbound rules in AWS Security Groups.",
        "414d4d85-8dd9-43e2-a067-79372fdb0d67": "Explain the purpose of using the df command and how it can help in managing disk space in an instance.",
        "da29eb5e-e891-4292-8d84-c4f4908a378b": "How does the du command differ from the df command in terms of estimating file space usage in directories and files?",
        "4f377f0b-268b-4cfb-9f34-faa77b102e6c": "Explain the difference between fine-tuning and parameter-efficient fine-tuning in machine learning. How does parameter-efficient fine-tuning help reduce computational costs compared to full fine-tuning?",
        "3d6f43c6-c3c4-4de5-bd21-86d3ac94970f": "Compare and contrast the Additive PEFT and Selective PEFT categories in the context of fine-tuning pre-trained models. How do these approaches differ in terms of updating parameters and reducing computational resources?",
        "b497d3e8-2c0e-4639-bbf7-a388a113d3eb": "Explain the concept of Parameter-Efficient Fine-Tuning (PEFT) and discuss how Reparametrized PEFT and Hybrid PEFT contribute to enhancing model performance while minimizing complexity. Provide examples of specific techniques mentioned in the document.",
        "6193851e-d643-49e7-a8bb-5aea04e04092": "Compare and contrast the GPU requirements of different fine-tuning techniques for models such as RoBERTa, T5, and Llama-7B-Alpaca. Discuss the implications of these GPU requirements on the efficiency and effectiveness of fine-tuning these models for downstream tasks.",
        "57f776f8-196b-4403-8b73-5a86a4b504f0": "Compare and contrast the performance metrics of the RoBERTaLARGE HIWI 48 model and the RoBERTa-base MoA 80 model based on the information provided. What factors might contribute to the differences in their performance?",
        "55eee233-cbfa-4fca-b9dd-af9070ac98fb": "Discuss the significance of the LLaMA-2-7B QLoRA 48 model in the context of natural language processing. How does it differ from other models mentioned in the document, and what potential applications could it have in the field?",
        "4e4b2e35-873b-445f-bbeb-1e2139ab859a": "Explain the challenges faced when attempting to fine-tune the Llama 3-8b and Mixtral-8x7b models, and why standard troubleshooting steps were ineffective in resolving the \"CUDA out of memory\" errors.",
        "2b61bbc1-098b-4754-b6ec-d5896732fdd9": "Describe the configurations used for fine-tuning the Llama2-7b model, including the base model, number of training examples, and specific parameters such as lora_r, lora_alpha, and learning rate. Discuss the results obtained from this fine-tuning process.",
        "ca2d67a7-b0fa-4885-b66d-fe0f2e03985b": "What are some common issues that a software engineer should look for when conducting code reviews of junior developers?",
        "3c527155-6f3e-485f-87e0-05267a8f0acb": "How can the code snippet involving the calculation of \"result\" be simplified and improved for better efficiency?",
        "1884cb96-030c-4148-9fa4-89d07420d5c8": "Explain the importance of following the PEP 8 style guide for code formatting in Python, using specific examples from the provided context information.",
        "4aeaf5da-1554-47b6-be60-e9dd142fc9f1": "Identify and correct the errors in the code snippet that defines the 'ages' dictionary. Explain why the corrections are necessary for better code readability and maintainability.",
        "71c84745-3c58-4f62-ae7a-31fbfd76c1b7": "Explain the importance of using string literals when calculating the length of a string in Python. Provide an example of how to correctly use a string literal with the len() function.",
        "5db839c7-afaf-47f4-8a18-7f2914587cb9": "Discuss the potential issues that can arise when using the range() function in a Python loop without specifying an end index. How can these issues be mitigated to ensure proper resource cleanup and prevent bugs in the code?",
        "18f97ffd-0f9c-4156-b145-8541b665bc37": "Explain the importance of validating user input before processing it, and provide an example of how this could prevent unexpected results in the given code snippet.",
        "46d31a11-7fa1-499e-b1a9-03584f098473": "Discuss the potential drawbacks of using a dictionary with a single key-value pair, as mentioned in the context information. How can this practice lead to confusion and errors in code development?",
        "363dda3c-4601-4c17-9335-5f6be5d61cb6": "How did the response of Llama3-70b compare to the previous finetuned version of Llama2-7b in terms of performance, according to the evaluation scores provided?",
        "0f1379a2-92e8-41f4-a5b2-a64e1865f0ec": "What are three suggestions given in the context for improving the quality of the code provided, specifically focusing on variable names, data types, and calculating the mean?",
        "c8102b51-1635-47c8-8a72-f04cc09e91b2": "In the provided code snippet, what is the purpose of using the `len()` function in line 5 and how does it affect the subsequent execution of the code?",
        "7eba2005-98a8-4d28-9a23-eefe65ac02d0": "Explain the significance of using f-strings over string concatenation as mentioned in line 1 of the context information. How does it improve code readability and efficiency?",
        "9479c3b1-1a10-4e66-b3d5-dd8c96229e01": "Explain the issue with the code snippet `len('hello') = 5` and provide the correct syntax for assigning the length of a string to a variable.",
        "8687984a-f231-43e8-b7cb-07ac850da725": "Describe the purpose of the `for i in range(10):` code snippet and how it can be improved by providing more context.",
        "69d52aa7-1410-4ad5-82fc-9063c4aab368": "Explain the importance of using correct syntax in programming, citing examples from the provided responses from the model.",
        "2c31620f-be43-4a65-aeae-3650c689d61d": "Calculate the result of the expression \"result = (4 + 2 * 3)\" and explain the order of operations followed in the calculation.",
        "1fea9057-eec3-40da-bead-68093c8c186f": "In the context of Python programming, explain the importance of properly initializing variables before using them in conditional statements. Provide an example from the given code snippet to support your explanation.",
        "46a774bb-9748-4d9c-99dc-9b3ca5d7a41a": "Discuss the significance of using commas to separate key-value pairs in dictionaries in Python. How does the absence of a comma affect the syntax and functionality of a dictionary, as illustrated in the provided code snippet?",
        "1e1f4af8-aca1-449b-a22f-9f140fc7735f": "Explain the error encountered during the finetuning of the Mistral-7B chat model and suggest a solution to resolve it.",
        "61370cf5-167c-433d-af74-918b563a3436": "Analyze the code snippet provided for the variable \"ages\" and identify and correct any syntax errors or conventions that should be followed in Python programming.",
        "c988d174-e0d2-4707-b858-6b8e5a205494": "How can you ensure safe serialization in the configuration for Phi-3 finetuning?",
        "981f003f-0d58-4cec-880e-6e801e696700": "In the code snippet provided, what correction should be made to ensure proper syntax and avoid precedence issues?",
        "6e6f4d76-76e7-48e4-bc6a-122e32eada7a": "How can the code snippet be improved to adhere to the PEP 8 style guide for Python code?",
        "dfb62a47-e9ff-4afd-9347-6b031f6192ea": "What are some potential issues with the code snippet in terms of error handling and exception management?",
        "7fac38c6-389e-4aec-8d48-8a609e62d962": "In the code snippet provided, what improvement suggestions were given for the conditional statement checking if the variable x is equal to 0? How would adding a comment and using a more descriptive variable name enhance the code?",
        "763f6932-2abd-4b14-b8e8-2c152b234881": "Identify the error in the dictionary initialization for the 'ages' variable. What correction was recommended for the syntax error, and what additional suggestions were provided for improving the readability and maintainability of the code snippet related to the dictionary keys and variable naming?",
        "648310fa-08fa-4ed5-a49d-cc11eca4ca37": "In the context of Python programming, what is the recommended approach for validating dictionary values, especially if they are supposed to be integers? Provide a suggestion for adding a validation check in such cases.",
        "e94997fb-6e95-4f25-8c12-85ebdc85ec8b": "Identify and correct the syntax error in the code snippet provided for assigning a value to a variable using the `len('hello')` function. Explain the correct way to assign a value to a variable in Python.",
        "6312f7d7-8a5f-4d2f-9205-d52de69d1681": "In the code review section, what suggestion is given to avoid a `KeyError` when accessing a key that does not exist in a dictionary?",
        "5a875f1c-b1e6-457f-93f7-beefb0e6d315": "Based on the evaluation scores provided, compare the performance of the Fine-Tuned Model Phi-3 with the original Phi-3 model.",
        "72438bdf-554b-4b27-b190-099d1691edd6": "Explain the difference between the functions `returnns_a()` and `foo()` based on the provided code snippets. How does each function handle return values?",
        "a4d18acc-04e9-43aa-a91c-8bdcacfb3be2": "Identify and correct any syntax errors in the code snippets provided. Provide a brief explanation for each correction made.",
        "c74626f2-3918-4b00-aa1c-0135a546723b": "How does the LlamaIndex tool support a wide range of NLP tasks, and what are some key features that make it a versatile tool for developers and researchers?",
        "ebdc8a10-e9cf-4d06-a489-f72c856f33a7": "Compare and contrast the pros of using the RAG approach for context-aware applications with the benefits of the extensive ecosystem and customizable workflows offered by the Hugging Face platform.",
        "fccb643f-f54d-4c91-aac2-59e77c87d461": "How does the tool described in the context information differ from specialized solutions in terms of performance and use cases?",
        "62026ce4-5253-479c-a73b-9db108014e1b": "Can you explain the significance of vendor integration and community support in the context of NLP tools like the one mentioned in the document?",
        "f83edd10-62e2-4cdc-890d-72ad1e999798": "Explain the difference between multi-query expansion and sub-query expansion in the context of the features provided by LlamaIndex, LangChain, Haystack, and Hugging Face. How do these techniques contribute to improving search results in information retrieval systems?",
        "798b9353-e3bd-4e30-8121-8837e8fa6782": "Discuss the significance of the Hyde-adapter for embeddings in the context of query optimization. How does this feature enhance the performance of language models and improve the accuracy of search results?",
        "defa0716-be65-4804-a0bc-e393ad700c74": "Explain the difference between chunking strategies and hierarchical index structure in the context of information organization. How do they each contribute to efficient data retrieval?",
        "c7c9818e-7761-418e-a272-b5fa595c4245": "How does the use of a knowledge graph index differ from metadata attachment in terms of organizing and accessing information? Provide examples to illustrate the differences between the two approaches.",
        "47b3b8c3-6419-4032-bddc-38b198314216": "Compare and contrast the availability of embeddings across the different models listed in the context information. Which models have all four types of embeddings (Ollama, Cohere, Jina, Voyage, OpenAI) and which models have limited availability (LLM Embedder, BGE Embeddings)?",
        "55db86e7-f347-45a1-8c19-038f1c5317ea": "Discuss the significance of having a wide range of embeddings available in a model for natural language processing tasks. How does the presence or absence of certain types of embeddings impact the effectiveness of a model in processing and understanding text data?",
        "cbdb82e0-28ab-4d0d-bd58-464c110fee7e": "Explain the difference between window retrieval and recursive retrieval in the context of information retrieval. Provide examples to illustrate each type of retrieval method.",
        "b2f39790-cc0c-45bf-8e7b-1e9ab6e25868": "Compare and contrast the Cohere reranker, BGE reranker, and ColBERT reranker in the context of reranking algorithms. Discuss the advantages and disadvantages of each reranker and when they would be most effective in a search scenario.",
        "1acdcd7c-5ff0-41f0-b9ca-6146509dc960": "Explain the concept of query rewriting and its importance in the context of the given information. How does query rewriting play a role in improving the performance of database queries?",
        "def2dea6-12e3-4449-9eba-8a275a61df10": "Compare and contrast the validation percentages for the Tonic and Uptrain methods based on the provided data. How do these validation percentages impact the effectiveness of the respective methods in the experimental setup?",
        "13a76cfc-ebf7-4ad0-a410-5c56069b8d1b": "Explain the concept of query rewriting and its importance in improving precision in a database system. How does the use of techniques like Multiquerry and Subquery contribute to enhancing query performance?",
        "b85eeea4-115c-4aad-8692-d3b4f39fbe6f": "Compare and contrast the validation results of the Tonic and Uptrain methods, as indicated by the percentages provided. How do these validation percentages impact the overall effectiveness of the query optimization process?",
        "b7e8c89f-4bcf-45c6-bc48-0bce1baba82d": "Compare and contrast the performance of the HYDE, DeepEval, and Trulens systems based on the similarity, multiquery, subqueries, and prompting capabilities as indicated in the context information.",
        "9cfec596-0008-4b83-b3b4-7cda514c272d": "Discuss the validation ratings of the Tonic system and the evaluation ratings of the Falcon system, highlighting the differences in their effectiveness according to the given percentages.",
        "099a9686-3d64-4696-bc65-5720b65b930e": "Explain the difference between semantic chunking and character text chunking in the context of the experiment mentioned. How do these two methods contribute to the precision and baseline of the results?",
        "94b6e98d-88e3-49e8-80f5-393ad0ac2d2d": "Compare the training and validation results for the Tik token chunking in the experiment. What can be inferred from the differences in percentages for each stage of the experiment?",
        "d42dcfd1-5c9d-4cf1-9588-df2c8b189ade": "What is the percentage of answer similarity in the baseline experiment?",
        "cca9e900-8b88-47b3-be8d-818bcc73d57b": "How does the validation accuracy differ between the Up train and Tonic Validate experiments?",
        "4304cfc8-7409-4877-bf3c-cb6d7966ef10": "What are the precision values for the \"Up train\" and \"Tonic Validate\" experiments based on the baseline experiment results provided in the context information?",
        "112fbebb-1886-4f9e-b891-128fd4f7244f": "How do the metadata percentages for \"Up train\" and \"Tonic Validate\" experiments compare to their respective baseline experiment results?",
        "a19f9969-d95d-47c1-bfac-f7e2c0c51a3d": "Explain the concept of hierarchical indexing and its significance in the context of answer similarity and baseline experiments.",
        "334bab01-b9f7-4e5a-bf6b-a9bc971e3ed5": "How does the validation process differ between the \"Up train\" and \"Tonic Validate\" experiments, and what are the respective success rates for each?",
        "662ba9d8-5548-4f4c-a8f1-aedb4c7a931a": "Explain the concept of hierarchical indexing and its significance in information retrieval systems. How does it contribute to improving search precision in a database?",
        "3f972c9f-e556-4777-82e1-c643c6791260": "Compare and contrast the results of the baseline experiment with the results of the Tonic Validate experiment. What factors may have contributed to the differences in the percentage values obtained in each experiment?",
        "12fedda1-4650-4d94-b525-5f38f51d3a82": "How does Recursive Chunking differ from Semantic Chunking in text analysis?",
        "21a64d5b-a870-44e3-8c61-565fef7c7750": "Explain the concept of Hierarchical Indexing and its significance in organizing and retrieving information from large datasets.",
        "e3c9ef22-eb6e-43cd-a42e-621e79e4a064": "Compare and contrast the performance metrics of Ragas, Tonic, and Trulens based on the given data. How do their validation scores, deep evaluation scores, and up train percentages differ from each other?",
        "e6bcb95c-ebe0-406d-959f-758a111d99bc": "Analyze the trend in Falcon's evaluation scores over the given data points. What factors may have contributed to the significant increase in evaluation score from 45% to 96%?",
        "da72b5d0-90af-4792-82bc-946abf4368e4": "Explain the significance of the Baseline Experiment (ollama) in the field of AI research. How does it compare to other AI models such as OpenAI and JinaAI?",
        "62b7e2e9-0634-4c92-b242-059e3948d2df": "Compare the performance metrics of the Tonic Validate tool with the LLM embedder. How do the percentages of 44%, 60%, 54%, 60%, 53%, 53%, 48%, and 50% relate to the validation results of these tools?",
        "591fc819-b8e6-4d8e-b507-65396e6bca0c": "Explain the significance of Precision, Baseline Experiment, OpenAI, JinaAI, BGE3, Cohere, Voyage AI, LLM embedder, and Bedrock in the field of artificial intelligence. How do these technologies contribute to advancements in AI research and development?",
        "67afffbb-7f34-487d-8842-31c7a21e6711": "Compare and contrast the training and validation percentages mentioned in the context information. What do these percentages represent in the context of machine learning models and experiments? How do these percentages impact the overall performance and reliability of AI systems?",
        "b1ce0362-768d-4a3b-9d0b-b44e08e1ea8e": "Explain the significance of similarity metrics such as Ragas, Tonic, and Falcon-evaluate in the context of AI models like Openai, Bge, and Cohere. How do these metrics contribute to the evaluation and validation of AI models?",
        "ad123b14-0298-4ad8-be49-582d76e01561": "Compare and contrast the performance metrics of Deepeval, Uptrain, and Trulens in assessing the effectiveness of AI models. How do these metrics differ in their evaluation criteria and what insights can be gained from their respective scores?",
        "82847faa-9fbb-4b04-9f1d-b696a906ef95": "How does Sentence Window Retrieval work in scoring and providing context based on query relevance? Provide an example of a use case where this technique would be beneficial.",
        "02a15388-b172-4143-a40f-180a5d9d0fcc": "Explain the concept of Auto Merging Retrieval and its significance in combining relevant passages from multiple sources. How is this technique useful in creating comprehensive reports or synthesizing information from diverse documents?",
        "361ce8b4-6a72-4257-91ec-5dd8177d62c1": "What are the different experimental techniques mentioned in the context information, and what are the corresponding percentages for each technique in the \"Up train\" category?",
        "fd2ffb26-30cd-4dd3-b3aa-b36ca73d3719": "Compare the validation results for the \"Auto Merging\" technique in the \"Up train\" and \"Tonic Validate\" categories. What can be inferred from the differences in the percentages?",
        "e5bc965b-c0ba-46b0-8ed8-b8df93f2e491": "How does the concept of relevance play a role in the context information provided?",
        "aa033f2b-26db-4ffc-8942-184ad8a05aa8": "Can you explain the significance of the percentages mentioned in the \"Up train\" and \"Tonic Validate\" sections of the context information?",
        "0b243fc5-65ea-40cb-8b9e-54b5a6349e03": "How does the concept of context similarity play a role in the various tools mentioned, such as Ragas, Deep Eval, and Tru lens?",
        "02ce2407-611f-49d4-848f-71010ba83d5f": "Compare and contrast the effectiveness of the Sentence window and Auto Merging features in the tools mentioned, such as Up Train and Falcon-evaluate.",
        "d79148cc-d27f-4405-8d6a-55d2b7f4f62f": "Explain the significance of baseline experiments in the field of natural language processing, and how they are used in comparison to other models such as Colbert, Cohere, and JIna.",
        "f6d4ae34-8e36-4da0-ab4f-8b5dd6af12a0": "Compare and contrast the training percentages and validation scores of the models Gpt, Long context reorder, and Tonic. How do these metrics impact the overall performance and reliability of the models?",
        "2a1f3a19-1c5f-4e47-b2f1-a2b5d0aa84cf": "Explain the significance of baseline experiments in the field of natural language processing, and how they are used in comparison to more advanced models such as GPT and Cohere.",
        "97e5be0d-b4ef-434b-91c7-9795d88c8480": "Compare and contrast the performance metrics of the models mentioned in the context information (Colbert, Cohere, JIna, BGE, Rag Fusion, Rank Gpt) based on the validation results provided (40%, 83%, 77%, 88%, 66%, 75%, 87%, 67%).",
        "1f7e8a4c-b5a4-4f61-b6db-db9460434ab2": "Explain the significance of rerankers such as Colbert, Cohere, and Rag-fusion in the context of information retrieval and ranking algorithms. How do these rerankers improve search results compared to traditional ranking methods?",
        "4cc004d1-e265-46e1-8eda-1d3e0a7827b2": "Compare and contrast the performance metrics of Tonic, Deepeval, Uptrain, and Trulens in evaluating the effectiveness of different models or systems. How do these metrics help in assessing the quality and reliability of the results generated by various algorithms?",
        "eb0021e3-a185-41e2-91c1-4bf89565af9e": "Compare and contrast the average scores of the CPU and GPU for the re-ranker, Colbert, and Rag Fusion models. How do these scores reflect the performance of each model in terms of processing speed?",
        "671d5b6e-e64a-421f-bc6d-3cbfae493579": "Analyze the efficiency of the Cohere model compared to the other models based on the average scores for both CPU and GPU. How do these scores suggest the computational capabilities of the Cohere model in relation to the re-ranker and Rag Fusion models?",
        "1100f1d6-c70f-4aaa-b85e-9bea257b9c6b": "How does the ranking system differ between Rank GPT, Jina AI, and BGE based on the given information?",
        "2c5d47c0-ca07-43c5-9b36-ca1e4549c860": "Explain the significance of the time differences in processing for Long Context Reorder compared to Jina AI and BGE.",
        "8a5149ae-0577-414e-9b33-8350fa4b0ee2": "What is the significance of the commas in the context information provided? How do they contribute to the overall structure of the document?",
        "b79c227a-7b33-49a7-9bb9-ef8d503dbe22": "How would you analyze and interpret the spacing between the commas in the context information? What impact does this spacing have on the readability and organization of the text?",
        "e3b0935d-0947-4183-baa7-bd5f479a1f4b": "What is the significance of the repeated commas in the context information provided? How do they contribute to the overall structure or meaning of the text?",
        "6b3d6c68-bba8-41db-9c70-86f0d3754b94": "How might the absence of any specific content or information in the context information impact the reader's understanding or interpretation of the document as a whole?",
        "bd5e6c99-2e0d-4f6a-be5d-d10a4b4f1a68": "What is the significance of the repeated commas in the context information provided?",
        "36bc4a41-02b3-48fa-bfdb-4f7da5ee9c9d": "How would you interpret the pattern of commas in the context information and what could it potentially signify?",
        "598f3647-724b-4c2c-957a-ee80f195d7e5": "What are the possible interpretations of the series of commas in the context information provided?",
        "8e394a9d-08aa-45ec-942e-490383a42ad0": "How can the use of punctuation marks impact the overall meaning and readability of a text?",
        "807dcf7c-fe50-44bd-9c2b-390464e05bd6": "What is the total number of rows and columns in the given context information?",
        "d4945448-ebca-426c-98b5-aedebd16e668": "Can you identify any patterns or sequences in the arrangement of commas in the document? If so, please describe them.",
        "a9a8a9eb-2c15-4276-8b76-4e7387f5810f": "Can you identify the pattern or relationship between the pairs of numbers provided in the context information?",
        "020f2c19-6d27-48f6-bcd7-18b92f0977f0": "How would you continue the sequence of pairs of numbers based on the given examples?",
        "94e29284-444e-459c-8eb1-39a72529316c": "What is the significance of the repeated commas in the context information provided?",
        "e68ec953-2a81-4628-a7d4-411375399c7b": "How would you analyze and interpret the pattern of commas in the given document?",
        "81adbabd-cb32-4079-ab82-bf11a41bff53": "How can context information be used to enhance understanding and interpretation of a document?",
        "5f0480e5-837d-474a-aa33-efbbf4bf67ea": "In what ways can prior knowledge impact the way we approach and analyze a given text or information?",
        "1e55bd07-fbfd-4c8d-ade2-ea93df787294": "What is the significance of the repeated commas in the context information provided?",
        "a2f84ecf-e11c-4df9-ae10-3b048d763e3d": "How would you interpret the pattern of the commas in the context information and what could it potentially represent?",
        "36953587-27ee-4c2b-9ea4-87f17acd7aeb": "What are the possible interpretations of the series of commas in the context information provided?",
        "506656ec-af4f-4f53-8c57-e167017a8a35": "How can the use of punctuation marks impact the overall meaning and readability of a text?",
        "a3bb9cc4-495a-4913-87a6-59c111518d4b": "What is the significance of the repeated commas in the context information provided?",
        "804edd78-7083-439c-8206-9bd9b3c167a3": "How would you analyze and interpret the pattern of commas in the given document?",
        "90e12505-5a77-4c7d-a815-0e15606594fc": "What is the pattern or structure of the data provided in the context information?",
        "6d5a8440-c802-479d-a368-c91d85db8b7c": "Can you identify any potential relationships or trends within the data based on the information given?",
        "67d04257-6769-4486-8ab9-4852acda47cb": "How would you evaluate the performance of a content moderation system in terms of toxicity and bias, using recall and precision metrics?",
        "ab85aa33-f73d-4b77-9373-95e800dfc794": "As a teacher, how would you assess the effectiveness of a dialogue system, such as a chatbot or Q&A platform, in terms of faithfulness and relevance to the given context?",
        "d65bb469-d124-4cba-8fe2-285decd66b3e": "How would you evaluate the performance of a content moderation system in terms of toxicity and bias, using recall and precision metrics?",
        "b47e8c06-c9e8-454b-b778-c7458a3c757f": "As a teacher, how would you assess the effectiveness of a dialogue system, such as a chatbot or Q&A system, in terms of faithfulness and relevance to the given context?",
        "652cd54c-5300-4aa0-a14f-7468d5a7b6c4": "Identify the pattern in the arrangement of commas in the context information provided. Explain how the pattern can be used to predict the placement of commas in a similar document.",
        "709eca75-17f4-41b9-9836-acffc0b60c81": "How can the context information be visually represented using a grid or matrix format? Provide a step-by-step explanation of how the information can be organized in a grid.",
        "b58c9c98-a8b2-422a-a46f-beab7c2f5e30": "How can context information be used to enhance understanding and interpretation of a document?",
        "a5873295-3986-42e9-8af1-7081dfbca2d7": "In what ways can context information impact the overall meaning and significance of a text?",
        "111e78d1-4870-4e3d-ba67-24bfcb575e1a": "How does the concept of context information impact the understanding and interpretation of a given query?",
        "09810ff8-9d3c-4ddb-a9c8-b6e99c082a06": "Can you provide an example of how context information can influence the outcome of a query in a real-world scenario?",
        "a1c7d705-61d8-48bf-b470-341d067572e3": "What is the pattern of the characters in the context information provided? Can you identify any specific sequence or arrangement?",
        "21b89b4a-8238-4336-a11d-4a6e5f6391a4": "How would you continue the sequence of characters in the context information? Provide the next three characters in the sequence.",
        "c759d469-edd1-497c-81a2-c81943152bbd": "What is the significance of the repeated commas in the context information provided?",
        "21deea53-b7bd-41b6-a98a-d774104d9dfd": "Can you identify any patterns or structures within the arrangement of commas in the context information? If so, describe them.",
        "f8d90d86-0dd4-4f0a-b57c-c2f546f8dfb2": "How can context information be used to enhance understanding and interpretation of a document?",
        "007f4028-835e-45f2-849e-073724730847": "In what ways can context information impact the overall meaning and significance of a text?",
        "e8a50b0e-9d73-4ca5-924d-d1a0ab702f11": "What are the three elements listed in the context information?",
        "a005d229-585e-4460-877c-aaf0aab3a60d": "As a teacher/professor, how would you use the context information provided to guide your students in their learning process?",
        "3171a2d2-7ac8-4707-8026-c3bad3c397c7": "Can you identify the common theme or topic that is being discussed in the document based on the context information provided?",
        "02546afd-f607-4e5e-b11b-1f20af705d40": "How would you summarize the main points or key ideas presented in the document based on the limited information given?",
        "d60f2ea6-d3f3-419d-ab0f-e17cb231094f": "Can you identify the common theme or topic that connects all the items listed in the context information?",
        "285266e8-ce49-456d-994b-b1c553d11ad9": "How would you categorize or classify the items listed in the context information based on their similarities or differences?",
        "86ffb117-7ecf-418e-a4db-e8a4eac61d42": "What is the total number of rows and columns in the given context information?",
        "a063ffaf-c85c-44ae-960c-85bdb279386f": "How many empty spaces are there in the grid provided?",
        "96530ba3-5ef7-420f-8924-ceab675e0935": "What is the total number of rows and columns in the given context information?",
        "2df4ccd8-c0c7-4204-9d93-bd09b9419fb2": "Can you identify any patterns or sequences in the arrangement of commas in the document? If so, describe them.",
        "4c02b228-e54b-41b4-bbd5-5e2aec493320": "What is the total number of commas present in the context information provided?",
        "e9ec718c-2693-4abc-b911-2150139ae98d": "How many rows and columns are there in the document based on the context information given?",
        "54199cbe-5642-4ace-b456-e13fc128e1bd": "Based on the context information provided, can you identify any patterns or trends that you observe in the data?",
        "d4485d76-b16e-4ab5-b5cb-b670e664f815": "How would you analyze the data in order to draw meaningful conclusions or insights from it?",
        "6d94c5bd-eb27-467b-940d-380881fc96a6": "What is the total number of columns in the document provided in the context information?",
        "d3cb892e-f960-4579-8d89-044cd21ed73f": "How many rows are there in the document provided in the context information?",
        "0244b4b6-3458-4016-8b2f-75762b8cbd9d": "What is the total number of cells in the grid provided in the context information?",
        "80022e66-9d98-4921-a1c1-74f1622fa50f": "How many rows and columns are there in the grid?",
        "63c042de-fe3a-44ac-83db-51e85d814fce": "What is the significance of the commas in the context information provided?",
        "7826dfb7-3aa5-4d47-b25f-c4526dae921c": "How would you differentiate between the three sets of commas in the context information?",
        "38402a21-5be1-4ca2-bab7-934273d8bec5": "What is the total number of rows and columns in the given context information?",
        "76fad93f-00a2-40ec-b816-82f58beeb9f0": "Can you identify any patterns or relationships within the data based on the provided context information?",
        "2fbbd033-c5ff-4d06-a004-c65cc5f8424a": "How does the Maverick code review tool differ from the CodiumAI PR-Agent in terms of its integration capabilities with version control platforms like GitHub, GitLab, and Bitbucket?",
        "8816bf6d-8e07-4cf3-8330-0a87822d9dae": "What are the key features offered in the paid tier of the Feature code review tool, and how do they enhance the code review process compared to the free tier and the limited features available?",
        "c42d4b39-3d99-4a91-ac14-33a3cdb90a01": "Compare and contrast the memory usage of different methods for fine-tuning the RoBERTa model. Which method has the lowest memory usage and which method has the highest memory usage?",
        "03ef5ddf-fbdc-4597-875e-74b57b8432ba": "Analyze the impact of different tuning methods on memory usage for the Llama-7B-Alpaca model. How does the memory usage vary between Fine-Tuning, IA3, LoRA, and QLoRA methods?",
        "511f8e6a-da0b-43b9-b083-6320a3b825a7": "What are some of the different models mentioned in the context information, and what are their respective complexities in terms of parameters?",
        "220a9f15-3ca4-4b23-9250-90a15e6b4515": "Compare and contrast the models RoBERTa-large and RoBERTa-base in terms of their associated tasks and complexities.",
        "dc636933-808c-4f4e-97ea-875a7a634083": "In JavaScript, why is it important to include a value after the \"return\" statement in a function? Provide an example to illustrate this concept.",
        "e9ae5c57-2856-4849-834c-65c98562bf2a": "How can the calculation \"result = (4 + 2 * 3)\" be simplified without using the multiplication operator? Explain the reasoning behind this simplification and provide the revised code snippet.",
        "0ea97bd9-41fe-4a1b-9a28-26fee07d9471": "In the context of code formatting and style guidelines, explain the importance of consistent indentation and the inclusion of docstrings in Python code. How do these practices contribute to the readability and maintainability of code?",
        "e7a70e13-eb3e-4e82-839e-7644ea38dcf5": "Analyze the code snippet provided for the dictionary 'ages'. Identify and explain two specific issues related to code formatting and best practices that need to be addressed in order to improve the readability and maintainability of the code.",
        "fe080e24-3cd9-4d35-bb3b-8be4d28a4fbe": "In the context of the provided code snippets, explain the potential issue with using the `len()` function on a string that is not enclosed in a string literal. How can this issue be fixed to ensure correct calculation of the string length?",
        "ce33a0b9-3abb-48bc-81f8-371c096ad656": "Discuss the importance of properly defining and using the `range()` function in Python loops. Provide an example of how the `range()` function should be used in a `for` loop to iterate over a sequence of numbers.",
        "2edaa5b6-eaaf-4c80-9fdd-3e1ffb2c972f": "Explain the potential issues that may arise from not validating user input before processing it, as mentioned in the context information. How can error handling and logging help in such situations?",
        "1f294182-cb51-44d2-b0ec-3469ec88b1e4": "In the context information, a code snippet is provided that involves a dictionary with a single key-value pair. Discuss the drawbacks of using a dictionary in this manner and suggest an alternative data structure that would be more suitable.",
        "aeb674eb-21ac-4fec-99b5-894dc440de88": "How can using more descriptive variable names improve the quality of code according to the context information provided?",
        "7e142b28-6390-4b3b-b1d9-a6a3fe2ebfef": "Why is it important to convert all numbers to float type before performing math operations, as suggested in the context information?",
        "e4489876-6a55-450b-b1c5-f56d55d72f47": "In the provided code example, explain the importance of using comments to improve code readability and understanding. How can well-placed comments enhance the quality of code for both the developer and potential reviewers?",
        "fc3c66d3-647f-4c21-a371-23a9ad050a85": "Identify and correct the syntax errors present in the code snippets provided in the context information. Explain the significance of proper syntax in programming and how syntax errors can impact the functionality of a program.",
        "b119822e-61af-4194-bb41-e19eefab9895": "Explain the correct way to initialize a dictionary in Python with individual age values for each person, as shown in the context information provided.",
        "94a9bee9-daf1-4f10-b51e-10d6e44f7c29": "Identify and correct the syntax error in the code snippet `def foo(): return [1, 2, 3 print(foo())`.",
        "8a99234b-b426-4d3e-aa6f-9a9be49c651b": "How can the use of proper coding practices, as mentioned in the context, contribute to the overall quality of code?",
        "8cead3ca-4a5e-48e4-8c22-b2cfe64a699f": "Based on the evaluation scores provided for Phi-2 Finetuned and Phi-2, what areas of improvement can be identified for the code being assessed?",
        "1a23268f-df06-46a3-8ddd-ea2bc32e778b": "What correction should be made to the code snippet involving the expression `result = (4 + 2 * 3)` to achieve the desired outcome of 12?",
        "19167292-4d74-4df0-b143-ea99dfa5a80a": "In the context of programming syntax, what is the significance of using `==` instead of `=` in the code snippet `if x=5:`?",
        "03d13434-f401-40e9-99e6-c5d4a8f81408": "In the context of Python programming, what potential issues can arise when a variable like 'x' is not properly defined before an if statement? Provide two examples of such issues and explain how they can impact the code's functionality.",
        "ac532322-9db3-49c9-912a-cd0ccc86c6f3": "How can syntax errors in Python dictionaries, such as missing quotes around keys and missing commas between key-value pairs, affect the overall structure and functionality of the code? Provide an example from the given context and explain the importance of correctly formatting dictionaries in Python.",
        "ebe24df6-bc9d-4aec-ae8b-fe5aae041915": "In the code review section, what suggestion was made to improve the readability of a for loop using the range function? How does the corrected example using 'for i := 0; i < 10; i++' differ from using 'range(10)'?",
        "56c78ab7-60d1-4a1e-aac3-e602f1c643e4": "In the section discussing variable names and data structures, what recommendation was given regarding the 'ages' variable being a dictionary? How should the dictionary declaration be modified to adhere to the suggestion for simplicity and better readability?",
        "b330f4c9-0c27-4411-a07c-a2c61904b4e0": "In the first model input, what are the three specific issues highlighted in the code snippet provided?",
        "32970952-767b-47bd-9106-06e21781a550": "For the second model input, what specific correction is suggested for the expression (4 + 2 * 3) to avoid syntax errors and ensure proper calculation?",
        "8b934969-cf81-4385-b1ca-2efc1b031fce": "How can the code be improved to adhere to the PEP 8 style guide for Python code?",
        "3d8461d0-0ffb-44bf-b7fb-dee466ede049": "What are some potential issues with the code in terms of error handling and exception management?",
        "b987e75d-f99f-449a-ae55-c11e8250e939": "In the context of code readability and maintainability, why is it important to include comments in your code? Provide an example from the given code snippet where a comment could be added for clarity.",
        "a4814bc6-4d25-4c01-99c6-0229adab4dde": "How can the code snippet be improved in terms of variable naming and organization? Provide specific suggestions based on the feedback given in the context information.",
        "99cc8dad-114f-441d-bb8c-9038c17be2c4": "In Python, what is the correct syntax for assigning a value to a variable? Provide an example from the context information to illustrate the correct syntax.",
        "e89c1ee9-afb7-4f08-bda5-a04096abd363": "How can you improve the readability and maintainability of code by using variables? Explain with reference to the suggestion provided in the context information.",
        "e5caa735-9376-4095-bb28-498ca9bb4617": "Explain the importance of using the `get` method when accessing keys in a dictionary. How does it help prevent a `KeyError` from occurring?",
        "dde8b845-ab59-4bbc-8018-38ada8ebe557": "Identify and correct the syntax errors in the code snippet provided for the function `foo()`. Explain why proper indentation is crucial in Python programming.",
        "7b1f9d11-2dde-4cef-8598-cdd9eb17fbe7": "Based on the configurations provided, compare and contrast the Llama2-7b and Mistral-7b models in terms of their Lora_r, Lora_alpha, Lora_dropout, Learning Rate, and number of training examples. How do these differences potentially impact the performance of each model?",
        "bc214b78-0cba-43dc-bdeb-59ee8064be13": "If a student were to train the Phi-3 model with 2000 training examples, a Lora_r of 32, a Lora_alpha of 64, a Lora_dropout of 0.1, a learning rate of 5e-5, and for 3 epochs, what are some potential challenges they may face during the training process? How could they potentially overcome these challenges to improve the model's performance?",
        "a5bf7053-9c4c-498f-9632-8f69f43c5773": "Explain the difference between the functions `returnns_a()` and `foo()` based on the code snippets provided. Provide examples to support your explanation.",
        "aa999430-dd6d-4a31-9ecf-807320ce173a": "Identify and correct any syntax errors in the code snippets provided. Give reasons for your corrections and explain the expected output for each corrected code snippet."
    },
    "corpus": {
        "cb053f10-2e2d-4f77-a5cb-45c45ae50fb0": "Genie Research and POCs Home3\n      Untitled Smart Link 14\n      Advance RAG5\n           Comparing LLMS for Rag Over Code6\n           Advanced RAG Taxonomy8\n           AdvanceRAG Tools Comparison9\n           Advance RAG : Query Optimization13\n           Advance RAG : Document Optimization15\n           Advance RAG : Embedding Models17\n           Advance RAG : Context Expansion19\n           Advance RAG : Context Rerankers21\n           Testing Frameworks24\n           RAG over different types of Data25\n           Structured Planning Agent Llama-index27\n           Explore Evaluation Metrics29\n           Evaluation Metrics31\n           Q/A with RetrieverRouter engine from llama-index33\n           Recursive Retrieval Techniques34\n           Exploring JSON query engines of llama-index36\n           Capture Explanations and Compare Our RAG Mindmap41\n           Exploring text-to-SQL Query engine of llama-index43\n           Q/A over unstructured vs structured data44\n           Q/A with ReAct agent for routing46\n           RetrieverRouterQueryEngine vs StructuredPlanningAgent vs ReAct for query routing50\n                Mini-Assistant : Combining Q/A and function calling tools with ReAct on top for agentic reasoning51\n      Code Review54\n           Arshik's Day-by-Day Summary of the Genie Learning Path55\n           Potential Features for CodeHawk Emumba57\n           CodeHawk GitHub Implementation58\n           CodeHawk GitLab Implementation64\n           Running CodeHawk Scripts with Docker in GitHub Actions65\n           Interactive Clarification (Feature 1)68\n           Pull Request Summaries (Feature 2)71\n           Integrating with JIRA Issue Tickets (Feature 3)72\n           Comparison of PR Reviewer with other similar tools73\n           Development Automation PoC75\n           Self Hosting LLM77\n           Fine Tuning Techniques-LLM81\n           Fine tuning LLMs using Lora84\n           Comparison of Codegen2-7B and Mistral 7b for Code Review Tasks102\n           List Down the Resources Required for the Deployment of a Language Model on AWS EC2105\n                Control RAG Output107\n                Retrievers Comparsion109\n           Context Window Solution112\n           Stress Testing EC2 Endpoint114",
        "aa768b7f-5d59-41dd-954a-a59904c9afe9": "Literature Review of  RAG/LLM Based Log Analysis or Incident Management or Root Cause Analysis116\n      RAG Failure Causes124\n      GraphRAG127\n      Multimodal RAG & Open Source Embedding Models129\n      Topic : Embeddings137\n      SQL Agents141\n      RAG Over Confluence143\n      Fine Tuning Embedding's145\n      LLM Testing148\n           Confluence Pages Exploration149\n           Run Existing Github Repositories151\n           Test Cases for PR Review152\n           Test Cases for Advance RAG153\n           Manual Testing for PR Review154",
        "3a5fda7f-762e-47d1-a0b9-5af3bb23b00d": "3\nGenie Research and POCs Home\n \nStructuring Research and Development Tasks\nWe will broadly have two types of tasks in this project, either research/learning related or development (proof of concept). We will follow the \npractices below:\n Welcome to your new space!\nSpaces help your team structure, organize, and share work, so every team member has visibility into institutional knowledge and access \nto the information they need to do their best work.\nA completion criteria should be specified with each task\nDue dates are optional but should always be specified with time-bound tasks\nBefore marking a task as complete, it is compulsory that you add a brief conclusion/findings in a Confluence page linked to the task  \ndescription. This applies to both research and development tasks. e.g. if you were tasked with research on a new idea, summarise your  \nunderstanding. If you tried something new in code, summarise your conclusion and any instructions, set up etc for reproducing your  \nfindings.\nAll notebooks should be maintained in the Genie Github repo and must be committed to the repo before a task/subtask is marked  \ncompleted. Also link the notebook in the description of the task or the associated Confluence page.",
        "5320f796-79cd-4ba5-bf2a-e0409dd2a65a": "4\nUntitled Smart Link 1",
        "87ccf1ad-67c1-4690-a60b-3db13b3fc16b": "5\nAdvance RAG\nRag over pdf \nNumber of pdfs - 3 (all from different domains) \nQueries - 50 \nDataset fields - (Queries, Contexts, Ground Truth)\nGitHub - Rutuja161/Pdf-based-question-answering: This is the research project for my masters in Data analytics. This aims to build s\nmart question answering system for PDF files using BERT - based transformers. \nEvaluators used : 2\n  Rag over Code POC\nNumber of repos  - 1\nQueries - 10\nGitHub - emumba-com/emumba-nx-boilerplate-plugin \nhttps://github.com/meshalJcheema/Advance-Rag-Mindmap-usecases  \nEvaluators used : 5\n \nEvaluators\nFollowing are the two metrics  used from evaluators\nResponse Matching/Answer Relevance : Measures the percentage of responses that exactly match the expected answer. (uses \nground truth)\nContext Relevance :  measures if the retrieved context has enough information to answer the question being asked. how good is the \ncontext retrieved\n \n \n \n \n \n \n \n \n \n \n \nVISIT US\nConnect your Github account",
        "be8253b7-d682-49aa-aef9-3bf1d42eb545": "6\nComparing LLMS for Rag Over Code\nExtract content from Github Folder and Split using Langchain\nThis function does the following:\nDefines a Document class to store file content and metadata.\nReads files from a specified path and creates Document objects for each file.\nSplits the original documents into smaller chunks using the text splitter, creating a new list of documents.\nPIPELINE - 1\nGENIE-3 & GENIE-4 & GENIE-9 : Pipeline With Different Bedrock LLMs and Embeddings\nFollowing Pairs were made for analysis:\n1. Model Claude-V2, Embeddings - model_id=\"amazon.titan-embed-text-v1\"\n2. Model llama 2 70b, Embeddings -  model_id=\"amazon.titan-embed-text-v1\n3. Model llama 2 70b, Embeddings - model_id=\"cohere.embed-english-v3\"\n4. Model llama 2 70b, Embeddings - model_id=\"text-embedding-ada-002\"\n The following pipeline uses FAISS as its vectorDB.\nGENIE-5 : Evaluation of pipeline using LLM\nThe llm used is the claude V2 from bedrock service. \nThe process automates the evaluation of answer quality, using a language model to assign relevance scores to pairs of original and \ncandidate answers based on their context and query.\nPIPELINE - 2\nGENIE-6 : RAPTOR OVER RAG\n The following pipeline uses openai embeddings model and bedrock model claude as its llm\nImports OpenAIEmbeddings and ChatOpenAI from langchain_openai, and various other classes and functions from langchain for \nembedding and model interaction purposes.\nIntroduces a recursive clustering and summarization function that aims to embed texts, perform clustering at multiple levels, and \nsummarize the texts within each cluster.\nThe summarization part is intended to generate a summary for each cluster of texts by utilizing a language model (presumably Claude-\nv2), following a specified prompt template and output parsing.\nContains a loop at the end to process a list of queries, generate responses using a chain that involves document retrieval, prompting, \nand the Claude-v2 model for summarization, and then saves each response to a separate JSON file.\nPIPELINE - 3\nGENIE-8 : OLLAMA EVALUATION\nThe llm is used is llama2 and embeddings used are GPT4allembeddings.",
        "40f84dab-34b5-402a-a8bb-e04b43f79b1e": "7\nGENIE-7 : RAG EVALUATION\nThe llm is used is Claude - V2 from bedrock service, OpenAI embeddings and Weaviate vectorDB for embeddings manipulation. \nAssembles the queries, answers, contexts, and ground truth answers into a dataset.\nEvaluates the dataset using several metrics (context precision, context recall, faithfulness, and answer relevancy) to assess the quality \nof the generated answers.\nConverts the evaluation results into a pandas DataFrame and prints it.\nSaves the evaluation results to a CSV file named \"ragas_results.csv\".\nOverall, this pipeline demonstrates a workflow for answering queries with context from a vector database, generating answers with a \nlanguage model, and evaluating the quality of those answers against ground truth data.",
        "3c9188f2-4a38-4302-8571-7b42e2f4a386": "8\nAdvanced RAG Taxonomy\nThe task is designed to develop a comprehensive taxonomy for retrieval augmented generation models. The main \ngoal of the taxonomy is to provide a structured framework by categorising the tools and techniques used in \ndifferent components of RAG pipeline.\nLink to Mind Map:  https://mm.tt/app/map/3231556693?t=U72qrjV59q\nApproach: \nA comprehensive literature review was conducted to understand the RAGs and its pipeline. This involved studying various survey papers, \narticle and frameworks.\nBased on the the gained insight, different components were identified and characterised.\nThe identified components were carefully fitted into the pipeline of the advanced rag model.\nThe Rag pipeline is mainly divided into 6 components:\n1- Pre-Retrieval: It is used before the retrieval process. Main purpose is to pre-process the user query and documents.\n2- Retrieval: This component is used in the overall retrieval process that includes embedding models and retrieval algorithms.\n3- Generation: It comprises of the post-retrieval part. That further involves relevance enhancement and LLM- FINE TUNING. Relevance \nenhancement involves context expansion, response re-ranking and embedding adapter.\n4- Augmentation: It mainly comprises of 3 different retrievals that helps in the augmentation process.\n5- RAG Evaluation: Various evaluations methods are categorised along with their implementations.\n6- Tools: Famous tools that are being used for RAG implementation are classified along with their implementations.\nInformation Sources Categorised by Colors: \nGreen: Git hub Repository\nBlue: Research Paper\nYellow: Official Library\nPink: Blog/ Website/ Colab notebook",
        "a470b594-36f9-4aee-a482-25840aaab98e": "9\nAdvanceRAG Tools Comparison\nHere's a comparative tables based on comparison for the following tools\n1. LlamaIndex\n  2. LangChain\n3. Haystack\n4. Hugging Face\nDefinition Analysis between Tools\nSupported \nLanguages       Python\nTypeScript                                                        Python\nJavaScript                                                                                     Python                                                                                       Python                                                          \nMain Focus                Indexing and \nefficient retrieval                                          General-\npurpose LLM \napplications\n RAG approach\ncontext-aware \napps                                     Semantic \nsearch and \nQ&A systems                                                              Pre-trained \nmodels, transfer \nlearning\nwide range of \nNLP tasks                                 \nKey Features              Integration with \ndiverse data \nsources (160+)\nlist index\ncustomizable \nworkflows Modular design\nflexible \nintegration\ndevelopment\ndeployment\nobservability\naccess to \nvarious LLMs Vendor \nneutrality\ninteroperability\ncustomizability\nextensibility \nspecialized \ncomponents Pre-trained \nmodels\nextensive \ndocumentation\nlarge \ncommunity easy \ntransfer learning\ncustomization \nPros                      Handles wide \nrange of data \nformats\nAdvanced \nindexing \ntechniques\nCustomizable \nRAG process Modular \narchitecture \nExtensive \necosyste\nAccess to \nvarious LLM \nproviders\n Ease of use Designed for \nsearch and \nQ&A\nSpecialized \ncomponents\nIntegrated \nworkflow Wide range of \nstate-of-the-art \nmodel\nUser-friendly \nAPI\nLarge \ncommunity\nCustomizable Category LlamaIndex LangChain Haystack Hugging \nFace",
        "312a819d-3274-4e0f-9903-708c211045fc": "10\nAdvance Rag Use-Cases comparisonCons                      Limited to data \nretrieval, less \nsuitable for \ncomplex NLP \ntasks Generalized, \nnot specialized- \nPerformance \nmay vary based \non use case                           Narrower scope\n Primarily \nfocused on \ndocument \nunderstanding \nand retrieval \ntasks         Not specifically \nfocused on \nsearch or \nretrieval \nsystems                                      \nUse Cases                 Q&A chatbots \nDocument \nunderstanding \nand extraction \nAutonomous \nagents Prototyping and \nexperimentation \nEnterprise-level \napplications                                   Enterprise \nsearch and QA \nSpecialized \nknowledge \nmanagement                              Text \nclassification\n Question \nanswering \nLanguage \ntranslation                         \nVendor \nIntegration        Customizable with \ndifferent retrieval \nstrategies                           Integrates with \nmultiple LLM \nproviders like \nOpenAI, Cohere, \nHugging Face Hub                          Integration with \nvarious model \nproviders and \ndatabases                                       Provides a wide \nrange of pre-\ntrained models \nfrom different \nproviders                           \nCommunity and \nEcosystem    Growing \ncommunity, \nlightweight \ninterface                                 Large and active \ncommunity, well-\ndocumented \nexamples                                                  Well-documented \nAPI, growing \ncommunity                                                       Extensive \ncommunity and \nresources, active \nuser base                                            \nPerformance \nOptimization  Focused on \nefficient indexing \nand retrieval                              Flexible but may \nnot match \nperformance of \ntailored solutions \nfor specific tasks                       Optimized for \nsearch and Q&A, \nbetter performance \nin semantic search                           Optimized for \nvarious NLP tasks, \ntransfer learning \ncapabilities                                 \nExample \nLibraries/Tools    LangSmith, \nLangServe                                                                                    Transformers                                                \nQuery \nOptimisation                                                               \na) Multi-query \nexpansion                  \u2705          \u2705                  \u274c        \u274c              \nb) Sub-query \nexpansion                    \u2705 \u2705               \u274c        \u274c              \nc) Hyde-adapter for \nembeddings            \u2705          \u2705                  \u2705       \u274c             \nd) Rewrite queries                        \u274c         \u2705               \u274c        \u2705           \nDocuments \nOptimisation                                                           Feature LlamaIndex LangChain Haystack Hugging Face",
        "218b7775-9ce5-4810-a908-15cc1aacdff2": "11\na) Chunking \nstrategies                    \u2705          \u2705          \u274c        \u274c           \nb) Metadata \nattachment                    \u2705          \u2705                   \u274c        \u274c         \nc) Hierarchical \nindex structure           \u2705         \u274c         \u274c        \u274c            \nd) Knowledge \ngraph index                  \u2705   \u274c         \u2705       \u274c           \na) Ollama                                 \u2705          \u2705         \u2705         \u2705            \nb) Cohere                                 \u2705          \u2705         \u2705         \u2705            \nc) Jina                                   \u2705          \u2705         \u2705         \u2705            \nd) Voyage                                 \u2705          \u2705         \u2705         \u2705            \ne) OpenAI                                 \u2705          \u2705         \u2705         \u2705            \nf) LLM Embedder                           \u274c            \u274c             \u274c         \u2705          \ng) BGE \nEmbeddings                         \u274c           \u274c             \u274c        \u2705           Embeddings \nModel                                                                 \na) Sentence \nwindow retrieval              \u2705           \u274c        \u274c         \u274c            \nb) Recursive \nretrieval                    \u2705          \u274c          \u274c         \u274c            \nc) Auto merging                           \u2705          \u274c          \u274c         \u274c            \nContext \nRerankers                                                                \na) Cohere reranker                        \u2705             \u2705            \u2705          \u274c  \nb) BGE reranker                            \u274c\u274c       \u2705          \u2705               \nc) Rank fusion                            \u2705          \u2705          \u2705                     \nd) ColBERT \nreranker                       \u2705            \u2705           \u2705          \u2705             \ne) Long context \nreorder                   \u2705          \u2705         \u274c   \u274c          \nf) Jina reranker                          \u2705          \u2705        \u2705        \u2705            Context \nExpansion",
        "85f87a94-60ac-41a4-83a6-eda96f3304d9": "12\nTools and Query Router Engines\nLlamaindex \nLlamaindex offers following query routing techniques\n1. Pydantic Single Selector\n2. LLM Single Selector \n3. Pydantic Multi Selector\n4. Query engine tools (define description in tools definition)\nLangchain\nLangchain offers following query routing techniques\n1. Routing with function calling models\n2. Routing to multiple indexes\n3. Dynamic Routing based on custom logic related to what is your input\nHaystack\nHaystack offers following query routing techniques\n1. Classifying keywords, questions and statement queries\n2. Zero shot classification\n3. Routing based on content type and metadata field\n4. Conditional routers\n5. Filetype routers\n6. Text Language router\n7.  Transformerszeroshottextrouters\nHugging Face\nHugging face offers following query routing techniques\n1. Hybrid LLM: Cost-Efficient and Quality-Aware Query Routing (paper)\n2. Decomposing Complex Queries for Tip-of-the-tongue Retrieval (similar to subquery idea)\n \n g) RankGPT                                \u2705               \u2705              \u2705             \u2705",
        "9d5bec3d-bcdc-461e-8d5b-dd2e1fbd1a06": "13\nAdvance RAG : Query Optimization\nDATA : PDF documents, link mentioned on main page\nTotal queries : 50\nEvaluation Tools are Uptrain & Tonic Validate\nEvaluation Techniques are Baseline Experiment, Multiquery, Subquery, HYDE, Multi step, Step back, Query Rewriting.\nMetrics for evaluation are Answer Similarity, Context Precision, Context Similarity.\nDATA : Code files from a GitHub repository, link mentioned on main page\nTotal queries : 10Uptrain     25% 28%\n  15%\n      48%   21%     27% 39%\n \nTonic \nValidate    44%       44%     30%      64%   48%     44%     50%Answer \nSimilarity  Baseline \nExperime\nnt Multiquer\ny Subquery HYDE Multi stepStep backQuery \nRewriting\nUptrain     45%    51%   22%    100%   44%     43%     49%\nTonic \nValidate    40%    64%   29%    80%   65%     63%     53%Context \nPrecision Baseline \nExperime\nnt Multiquer\ny Subquery HYDE Multi\nstepStep backQuery \nRewriting\nRagas 31% 82% 91% 91% 90%\nTonic\nValidate1.5/5 3.2/5 4.4/5 1.2/5 1/5\nDeepEval 10% 40% 85% 10% 45%\nUpTrain 14% 34% 57% 23% 21%\nTrulens 41% 56% 64% 76% 76%\nFalcon-\nevaluate25% 50% 96% 40% 33%Context \nsimilarityMultiquery Subqueries   HYDE Multi step Step back \nprompting",
        "c82733a7-38c8-4a3f-bd09-b7f0b1b04b1d": "14\nConclusion \nHYDE performs best! -  Expands search space, reduces hallucinations, we perform answer to answer embedding vector stores similarity \nthat is resulting in better results. \nAll other techniques,\nThey highly depend on type of query user is going for.\nThese techniques result in a lot of hallucinations.\nTherefore, on fairly decent  queries present in our dataset, their scores lie near our baseline     experiment except Hyde and query \nrewriting.",
        "8d70ef5e-c9e8-42a1-9472-20ef5e6a3a3c": "15\nAdvance RAG : Document Optimization\nExperiment Results\nDATA : PDF documents, link mentioned on main page\nTotal queries : 50\nDATA : Code files from a GitHub repository, link mentioned on main page\nTotal queries : 10Up train     45%    67%   69%    81%\nTonic Validate     40%    74%   70%    74%Context \nPrecision Baseline \nExperimentSemantic \nChunkingTik token \nChunkingCharacter text \nChunking\n \nUp train     25%    52% \nTonic Validate     44%    56%Answer Similarity Baseline Experiment Metadata\nAttachment \nUp train     45%    73% \nTonic Validate     40%    75%Context Precision Baseline Experiment Metadata\nAttachment \nUp train     25%    56% \nTonic Validate     44%    68%Answer Similarity Baseline Experiment Hierarchical Indexing \nUp train     45%    80% \nTonic Validate     40%    75%Context Precision Baseline Experiment Hierarchical Indexing \nContext \nsimilarityRecursive \nChunking\n Meta Data \nInclusionSemantic \nChunkingCharacter \nText \nsplittingTik Token \nBased \nSplittingHierarchical \nIndexing",
        "f67cb70c-a403-458d-95b1-4f0c784571e1": "16\nConclusion \nThere are multiple chunking techniques out there, depending upon use cases, code languages, extensions etc, we tried for our \nexperiment the generic ones, all performed better, then baseline experiment, but Character text splitting got maximum scores!\nMetadata integration with baseline experiment  boosted up the accuracy scores\nHierarchical indexing dynamic parsing and creation of multiple layers of indices, not only gives you good results but it also has an \noptimized Response Time.\nInclude metadata, necessary for scenarios where we have domain specific queries. Even include metadata with no domain specific, its \ngood to have it\nThe hierarchical and KI graphs can\u2019t be compared to chunking, they have different retrievers, nodes are there which are stored \nprocessed differently. Their main Plus seems to be their very fast storage and retrieval because of the way their structures are build but \nat the same time, they are much more complex to be understand-ed and integrated to our RAG pipelineRagas 80% 91% 77% 88% 71.08% 82%\nTonic \nValidate2/5 1.9/5 1/5 2.1/5 0.9/5 4.4/5\nDeep eval 35% 30% 49% 50% 40% 54%\nUp train 26% 26% 22% 40.8% 16% 67%\nTrulens 70% 72% 74% 75.5% 76% 76%\nFalcon-\nevaluate50% 46% 45% 59% 59% 96%",
        "257cc462-1f84-49b7-afa3-4569cdff3bba": "17\nAdvance RAG : Embedding Models\nVariants and Embedding Size\n< TO BE INCLUDED > SOON\nExperiment Results\nDATA : PDF documents, link mentioned on main page\nTotal queries : 50\nDATA : Code files from a GitHub repository, link mentioned on main page\nTotal queries : 10Uptrain   25%   36%  38% 50%   37%   34%  28% 24%\nTonic \nValidate  44%   60%  54% 60%   53%   53%  48% 50%Answer \nSimilarit\nyBaseline \nExperim\nent \n(ollama) OpenAI  JinaAI    BGE3   Cohere Voyage\nAILLM \nembedd\nerBedrock\nUp train   45%   88%   54%  86%   52%   66%   52% 45%\nTonic \nValidate  40%   76%   69%  79%   74%   67%   55% 61%Context \nPrecisio\nnBaseline \nExperim\nent \n(ollama) OpenAI  JinaAI    BGE3   Cohere Voyage\nAILLM \nembedd\nerBedrock\nRagas 91% 82.5% 80 77% 78% 79% 75% 76%\nTonic \nValidate1.9/5 3.4/5 3/5 1.1/5 2.5% 3.1% 2.8/5 2.9/5\nDeepeval40% 33% 38 44% 36% 34% 39% 37%\nUptrain 33% 33% 35 24% 30% 32% 31% 32%\nTrulens 75% 76.22% 76.22% 76% 76.22%\n 76.22%\n 76.22%\n 76.22%\n \nFalcon-\nevaluate55% 46% 48 33% 50% 47% 52% 49%Context \nSimilarityOpenai Bge - m3llm-\nembedd\nerCohere all-\nminillmMxbai- \nembed \nlarge Voyage  JIna Ai",
        "2f97842d-a2b8-4e00-9bdc-c887d4e43b47": "18\nConclusion\nAll performed better than baseline experiment that uses minillm ollama embeddings except bedrock embeddings they were nearly equal \nto minillm ones.\nBGE-m3 embeddings got maximum scores  \u2014 (multilingual, trained over short, long texts texts, 568 million parameters! \nBetween all others there does not lie much difference.\nTry them with a reranker for boosted results?\nBGE-m3, LLM embedders can\u2019t be integrated into our original pipeline, i made their own RAG pipeline and they need a GPU units, \nprocessing on CPU is very slow, in terms of model loading and initialization. Embedding formation time of T4-GPU is given above.",
        "859bfc2d-e07e-4a62-b714-79d2cde119e4": "19\nAdvance RAG : Context Expansion\nFollowing techniques are implemented, there is a general overview, how these techniques work.\n \nExperiment Results\nDATA : PDF documents, link mentioned on main  page\nTotal queries : 50Sentence Window Retrieval Segmenting the corpus into \noverlapping sentence windows \nand scoring based on query \nrelevance.\nScoring each window based on its \nrelevance to the input query.\nProviding highest scoring windows \nas contextSuitable for use cases where \ndetailed context from specific \nsegments of text is necessary,\nquestion answering \nsummarization.\nAuto Merging Retrieval Extracting relevant passages from \nmultiple sources and combining \nthem into a cohesive context.\nAutomatically combining \npassages into a single context.\nFeeding the merged contextIdeal for scenarios where \ninformation is scattered across \ndifferent sources, like\ncreating comprehensive \nreports \nsynthesis from diverse \ndocuments.\n \nRecursive Retrieval Iteratively refining the retrieval \nprocess to improve relevance and \naccuracy.\nUsing retrieved information to \nrefine the input query or context.\nIterative Retrieval: Repeating the \nretrieval process with refined \nqueries or contexts.\n Best suited for complex queries \nwhere initial retrievals may need \nfurther refinement, such as \nmulti-turn dialogue systems\n iterative problem-solving tasks\nPdfs with tabular data\n            Technique           Description             RAG use case\nUp train     25% 52% 54% 35%\nTonic Validate     44% 67% 68% 39%Answer SimilarityBaseline \nExperimentSentence \nWindow  Auto Merging Recursive",
        "018dd329-c88b-4c5c-b46c-a3f6df41d2d0": "20\nDATA : Code files from a GitHub repository, link mentioned on main page\nTotal queries : 10\nConclusion\nEvery technique is better than the baseline experiment, and auto merging and sentence window both gave us quite good results! \nSentence Window Retrieval : Good for extracting specific lines or sections from PR descriptions, comments, or diffs that are relevant to \na query\nAuto Merging Retrieval : Useful for combining information from multiple PRs, comments, and reviews to provide a comprehensive \nresponse\nRecursive Retrieval  : Highly suitable for iterative queries where initial results may need further refinement, such as detailed PR review \nor code analysis\nThey  are provided by framework llama index, and can be integrated with any LLM and embedding model\nFor sentence window retrieval, what if we consider base nodes as well? This May  increase the performance even more\nWe can not test Recursive as the pipeline does not give us the retrieved context, and evaluation matrix required that to generate a score, \nbut in pdf based testing, there are scores defined for it because the pipeline used partially satisfies its concepts, that's why it is \nhighlighted red as well but still its better than baseline experiment. \n \n Up train     45% 69% 77% 33%\nTonic Validate     40% 74% 73% 55%Context \nRelevanceBaseline \nExperimentSentence \nWindow  Auto Merging Recursive\nRagas 92% 96% -\nTonic\nValidate4/5 4/5 -\nDeep Eval 46% 40% -\nUp Train 71% 65% -\nTru lens 76.25% 76% -\nFalcon-evaluate 96 100% -Context similarity Sentence window    Auto Merging Recursive",
        "c5209c94-e998-4803-bdda-a0d469e02bd8": "21\nAdvance RAG : Context Rerankers\nOverview\nThe initial list of retrieved documents is reordered based on the new relevance scores generated by the re ranking model.\nThe documents that are deemed more relevant in the given context are moved higher in the ranking.\nThe top n ranked items for our experiments is kept four!\nThe n retrieved items are kept six\nExperiment Results\nDATA : PDF documents, link mentioned on main page\nTotal queries : 50\n Up train   25%   47%    49%  48%  33%     42%    49%    39%\nTonic \nValidate  44%   63%    54%  61%  51%       \n               \n               \n               \n               \n               \n               \n               \n               \n               \n               \n               \n               \n               \n               \n               \n               \n               \n               \n               56%    60%    49%Answer \nSimilarit\nyBaseline \nExperim\nent Colbert  Cohere JIna  BGE Rag\nFusionRank \nGptLong \ncontext \nreorder\nUptrain   45%   76%   67%   82%  43%   82%   78% 63%\nTonic \nValidate  40%   83%   77%   88%  66%   75%   87% 67%Answer \nSimilarit\nyBaseline \nExperim\nent Colbert  Cohere  JIna BGE Rag \nFusionRank \nGptLong \ncontext \nreorder",
        "4fe2d271-a4e9-47ce-82fa-de04cea3c28a": "22\nAGE RESULTS\nDATA : Code files from a GitHub repository, link mentioned on main page\nTotal queries : 10\nConclusion\nAll rerankers performed better than baseline experiment.\nColbert re ranker provides a comparison between a retrieval score and a ranker score, for all cases ranker score is lower than retrieval \nscore \nCohere re-rank contradicts its score with relevancy score, as score difference is huge!\nRag Fusion allows you to re-rank context from different rewritten queries as well but it decreases the scores when you have rewritten \nqueries, mainly cause of the quality of rewritten query\u2019s, can change it to sub queries method too, as the quality from it is concluded to be \ngood\nRankgpt provides you with new ranked order of contexts like 1,2,3,4,..n\nLong context reorder does not provides scores, also gives window issues when pass to llm for QA, typically because of too long \ncontexts, so k is limited to 3 in this case\nBGE reranker pipeline is customized and ran on t4-GPU, also it gives non normalised scores\nBert base rerankers have a better answer relevance.\nColbert re ranker seems to perform better than other rerankers, also provides a differences for estimation. \nColbert, jina re ranker both standout in their scores, and both of them don't rely on llm for re ranking cross encoders based re ranker are \nyielding good results than the ones which are based on llm - optimized for a pair (query to document) - you get a better score, and if your \nevaluator is checking only top few relevant documents, it will rate cross encoders based re ranker a high score\n \nComparing Rerankers response time on GPU and CPURagas 82% 90% 87% 88% 82.5% 93.0% 93%\nTonic \nValidate4.5/5 3.2/5 3.6/5 3.8/5 3.4/5 4/5 3.8/5\nDeepeval 70% 40% 68% 50% 33% 45.8% 63%\nUptrain 70% 39% 51% 53% 33% 45.2% 48%\nTrulens 75.68% 75.68% 75.6% 76.25% 76.22% 76.22% 76.01%\nFalcon-\nevaluate96% 56% 42% 57% 55% 42% 42%Context \nRelevanceColbert \nrerankerCohere \nrerankerRag-\nfusionRankGPT Bge \nranker Long \ncontext \nreorder JIna Ai \nreranker\nColbert 44s 60s\nCohere 61s 120s\nRag Fusion 80s 60sRe-ranker CPU Avg Score GPU Avg  Score",
        "e42b5e2d-2cd6-4428-9192-467c2216d965": "23\nBert based Rerankers took less time on CPU, Bert architecture does involve matrix multiplication as well as layered operations and \u2154 \nGPT based Rerankers took less time on GPU\nBERT-based models: Their smaller size, bidirectional nature, and extensive CPU optimizations make them efficient on CPU's.\nGPT-based models: Their larger size, auto regressive nature, and the need for extensive parallel processing make them better suited \nfor GPU s.\n \n \n \n \n \n \n \n Rank GPT 300s 180s\nJina AI 47s 60s\nBGE - 9s\nLong Context Reorder 58s 180s",
        "07127a49-e7bb-4611-a551-a4c009ce489b": "24\nTesting Frameworks\nTruLens: TruLens offers complex pipelines that vary for each metric, making integration into RAG pipelines challenging. While you could \nbuild a RAG pipeline already integrated with TruLens, this approach limits the ability to incorporate multiple advanced RAG tools. The \naccuracy score provided by TruLens is inconsistent, typically fluctuating between 74-76%, which undermines reliability.\nFalcon Evaluate: Falcon Evaluate provides an extensive array of metrics and is unique in utilizing traditional ML models for evaluation. \nHowever, its performance on negative testing is subpar, producing inaccurate results. Additionally, its accuracy scores are erratic, often \nfalling between 40-60%, and if a response is exceptionally good, the score jumps above 90%. It rarely outputs scores between these \nextremes, indicating a lack of nuance in its evaluations.\nRagas: Usually becomes inconsistent, higher values of responses are observed when other frameworks are giving low score for similar \nresponse\nDeepEval, Tonic Validate, Uptrain: These frameworks offer essential metrics for evaluation and are easily integrable into custom pipelines. \nIf ranked by performance and consistency, the order would be:\n1. Tonic Validate\n2. Uptrain\n3. Deepeval\nThe results from these frameworks are not directly comparable due to the use of different models, threshold values, and formulas for \ncalculating metrics.",
        "17feaf41-37ea-4426-86c2-2ea24f13a716": "25\nRAG over different types of Data\nKey Findings\n\u2192 Implementing Retrieval Augmented Generation over different types of data, e.g if underlying data contains pdf documents, SQL \ndatabase, JSON files etc. Exploring how can we create a single RAG over that.\n\u2192 Llama index provides the concept of multiple query engines, each specifically designed to work with a certain type of data. For example, \nif you have SQL, JSON, tabular data, then there are different query engines specifically made to handle each kind of data.\n\u2192 Llama index also provides a Retriever Router Query engine which can be used to route an incoming question to the right query engine.\n\u2192 The idea was to develop different query engines specific to each data, then plate the RetrieverRouterEngine on top of them and observe \nwhether it routes the question to the correct query engine or not.\n\u2192 In my example, I built a PandasQueryEngine over a tabular data, a VectorQueryEngine over a pdf data, placed the RetrieverRouterQuery \nengine on top of them and experimented whether it routes the question to the proper QueryEngine or not.\nResults\n\u2192 For the most part, the RetrieverRouterQuery engine seemed to be directing the query to the correct QueryEngine.\n\u2192 An issue that was originating was that the LLM was not restricting its responses to the knowledge base that we had provided. Instead it \nwas generating responses from its own knowledge. To resolve this, we need to adhere to \u201cStrict Prompting\u201d\n\u2192 Another issue was how to restrict unrelated questions from going down the pipeline, because RetrieverRouterQuery engine tries to fit an \nincoming question into one of the query engines, so even if a question that is not related to the knowledge about which we have created our \nRAG system comes in, it will eventually be passed into a query engine which will try to answer it.\n\u2192 To cater this problem, we can either use an LLM or an ML model to filter out incoming questions, and pass only the relevant one down \nthe pipeline.",
        "7771b031-5a66-4c3e-aacc-386dd0248e82": "26\nReferences\n\u2192 Pandas Query Engine - LlamaIndex\n\u2192 Retriever Router Query Engine - LlamaIndex",
        "649d234e-5ed9-48b9-80cd-117844f651d2": "27\nStructured Planning Agent Llama-index\nStructured Planning Agent\nStructured Planning Agent essentially works by resolving the incoming query into a list of sub tasks, executing each sub task through an \nappropriate tool and then combining the results.\nThe above image illustrates the difference between RAG with RetrieverRouterQuery engine and Structured Planning Agent. The \nRetriever Router analyzes the incoming query and forwards it to O N E of the underlying query engines, whereas a Structured Planning \nAgent break down the incoming query into sub tasks and uses appropriate tools to call relevant underlying query engines to process the \nsub queries, then combines the results.\nRetriever Router Query Engine is suitable for queries like:\n\u201cWhat was the net worth of Bill Gates?\u201c\nThis information is present in the tabular data of billionaires (see image above) and only needs access to that.\n\u201cDescribe the career of Paul Graham.\u201c\nThis information is present in the PDF doc of Paul Graham and only needs access to that.\nOn the other hand, Structured Planning Agent is suitable for following queries:\n\u201cTell me the net worth of Bill Gates and describe the career of Paul Graham.\u201c\nOne single data source does not contain information to answer the above question. Structured Planning Agent will break down the \nquery into sub tasks:\n1) Get net worth of Bill Gates.\n2) Get career information regarding Paul Graham.\nThen use tool1 built on tabular data of billionaires to answer query1 and use tool2 built on PDF data of Paul Graham to answer \nquery2.\nHow does the system know which tool and query engine to use to answer which sub query? We specify that information while \nbuilding the system. In the image below, while building the tool we associate the relevant query engine with it along with a \ndescription about which kind of data it is useful to answer a question from.",
        "72e5afdc-1812-440d-8cef-93c96b232d8c": "28\nAfter retrieving the results, the Structured Planning Agent combines them and outputs a single answer.\nKey Takeaways\nStructured Planning Agent works well only with a capable LLM. This is because under the hood, a lot of reasoning is happening in order \nto select the appropriate tools so it is suggested to use the best available llm.\nDuring experimentation, llama3-70b was not performing well. When GPT4-turbo, which is a better model, was used the results were \nsignificantly better.\nBecause of agentic reasoning, this approach is slightly more time consuming in yielding answers.\nReferences\nStructured Planning Agent - LlamaIndex",
        "7b4a2698-3481-4db4-b87c-c31aefc16e50": "29\nExplore Evaluation Metrics\nGiskard:\nAutomatic evaluation methodology. Covers automatic evaluation as well as manual evaluation by providing human reviewers to evaluate the \nresults provided through automatic evaluation.\nCan be used in identifying hallucinations or any kind of harmful responses or legal issues (in case the model generates)\nWhy G-eval and RAGAS produced different results?\nG-Eval:\n\u00b7        Coherence: G-Eval is designed to evaluate specific criteria such as coherence through detailed prompts and chain-of-thoughts (CoT). \nIt focuses on how well the generated text maintains a logical and structured flow.\n\u00b7        Prompt-Based Evaluation: The evaluations are driven by a predefined prompt that specifies what aspects of the text to evaluate, such \nas coherence, conciseness, or grammar. This can make G-Eval sensitive to the specific wording and focus of the prompts.\n\u00b7        Scoring Function: The scoring function in G-Eval uses the probabilities of output tokens from LLMs, aiming to provide fine-grained, \ncontinuous scores.\nRAGAS:\n\u00b7        Faithfulness: Measures how well the answer is grounded in the given context, which is crucial for ensuring that the information is \nfactually correct and relevant to the retrieved context.\n\u00b7        Answer Relevance: Evaluates if the generated answer addresses the given question appropriately without considering factuality.\n\u00b7        Context Relevance: Assesses the relevance of the retrieved context, focusing on minimizing redundant information and maximizing \nthe usefulness of the context provided to answer the question.\nG-Eval: Primarily designed for Natural Language Generation (NLG) tasks, it may weigh criteria like coherence and conciseness more \nheavily. The auto CoT provides a step-by-step approach that focuses on logical flow and structure.\nRAGAS: Designed for Retrieval-Augmented Generation (RAG) systems, it emphasizes the factual consistency of the generated text with the \nretrieved context (faithfulness), how well the generated answer addresses the question (answer relevance), and the pertinence of the \nretrieved context (context relevance).\nG-Eval: Since it is prompt-based, the quality and detail of the prompt can significantly affect the evaluation outcome.\nRAGAS: Utilizes predefined prompts for extracting statements and assessing relevance and faithfulness.\nG-Eval: Evaluates context expansion by how well it improves the coherence, structure, and overall quality of the generated text. If context \nexpansion leads to more logically structured and coherent text, G-Eval might score it higher.\nRAGAS: Focuses on how context expansion impacts the faithfulness, relevance of the answer, and relevance of the context. If the \nexpanded context improves the factual consistency and relevance without adding redundancy, RAGAS would score it higher.\n1. Coherence vs. Faithfulness: G-Eval might score context expansion highly if it improves the logical flow and coherence of the text, even if \nthe expanded context introduces some factual inconsistencies. RAGAS, on the other hand, might penalize the same context expansion if \nit perceives a decrease in faithfulness or an increase in irrelevant information.\n1. Prompt-Based Nuances: The way prompts are constructed in G-Eval can lead to different evaluation outcomes based on the specific \ncriteria emphasized in the prompt. RAGAS\u2019s structured approach to extracting and verifying statements might provide a different \nperspective on the same content.\n1. Scoring Granularity: G-Eval's use of token probabilities for scoring might result in more nuanced scores compared to RAGAS's binary \nverification process, which might lead to discrepancies in the final evaluation.",
        "c713eb68-d950-42de-9858-54092d6f021b": "30\nTruLens\nUse Cases:\n\u00b7        Detect and Mitigate Hallucination\n\u00b7        Improve Retrieval Quality\n\u00b7        Optimize App Configuration\n\u00b7        Verify the Summarization Quality\nArize AI Pheonix: Quickstart: Evals | Phoenix \nThe standard for evaluating text is human labeling. However, high-quality LLM outputs are becoming cheaper and faster to produce, and \nhuman evaluation cannot scale. In this context, evaluating the performance of LLM applications is best tackled by using a separate \nevaluation LLM. The Phoenix LLM Eval Library is designed for simple, fast, and accurate LLM-based evaluations.\n DeepCheck: LLM Evaluation Solutions | Deepchecks \nHelps in reducing the risk in llm pipelines by flagging significant issues and also helps in safeguarding the model from the problem of \nhallucinations.\nIt also provides auto-annotation which helps in understanding whether the response is good or bad.\n \n Content moderation Recall and precision on toxicity and bias\nQuery generation Correct output syntax and attributes, extracts the \nright information upon execution\nDialogue (chatbots, summarization, Q&A) Faithfulness, relevanceTask Evaluation parameters",
        "798b9c7a-3c1e-4f81-bcd6-c98afb8fc3d1": "31\nEvaluation Metrics\nGiskard:\nAutomatic evaluation methodology. Covers automatic evaluation as well as manual evaluation by providing human reviewers to evaluate the \nresults provided through automatic evaluation.\nCan be used in identifying hallucinations or any kind of harmful responses or legal issues (in case the model generates)\nWhy G-eval and RAGAS produced different results?\nG-Eval:\n\u00b7        Coherence: G-Eval is designed to evaluate specific criteria such as coherence through detailed prompts and chain-of-thoughts (CoT). \nIt focuses on how well the generated text maintains a logical and structured flow.\n\u00b7        Prompt-Based Evaluation: The evaluations are driven by a predefined prompt that specifies what aspects of the text to evaluate, such \nas coherence, conciseness, or grammar. This can make G-Eval sensitive to the specific wording and focus of the prompts.\n\u00b7        Scoring Function: The scoring function in G-Eval uses the probabilities of output tokens from LLMs, aiming to provide fine-grained, \ncontinuous scores.\nRAGAS:\n\u00b7        Faithfulness: Measures how well the answer is grounded in the given context, which is crucial for ensuring that the information is \nfactually correct and relevant to the retrieved context.\n\u00b7        Answer Relevance: Evaluates if the generated answer addresses the given question appropriately without considering factuality.\n\u00b7        Context Relevance: Assesses the relevance of the retrieved context, focusing on minimizing redundant information and maximizing \nthe usefulness of the context provided to answer the question.\nG-Eval: Primarily designed for Natural Language Generation (NLG) tasks, it may weigh criteria like coherence and conciseness more \nheavily. The auto CoT provides a step-by-step approach that focuses on logical flow and structure.\nRAGAS: Designed for Retrieval-Augmented Generation (RAG) systems, it emphasizes the factual consistency of the generated text with the \nretrieved context (faithfulness), how well the generated answer addresses the question (answer relevance), and the pertinence of the \nretrieved context (context relevance).\nG-Eval: Since it is prompt-based, the quality and detail of the prompt can significantly affect the evaluation outcome.\nRAGAS: Utilizes predefined prompts for extracting statements and assessing relevance and faithfulness.\nG-Eval: Evaluates context expansion by how well it improves the coherence, structure, and overall quality of the generated text. If context \nexpansion leads to more logically structured and coherent text, G-Eval might score it higher.\nRAGAS: Focuses on how context expansion impacts the faithfulness, relevance of the answer, and relevance of the context. If the \nexpanded context improves the factual consistency and relevance without adding redundancy, RAGAS would score it higher.\n1. Coherence vs. Faithfulness: G-Eval might score context expansion highly if it improves the logical flow and coherence of the text, even if \nthe expanded context introduces some factual inconsistencies. RAGAS, on the other hand, might penalize the same context expansion if \nit perceives a decrease in faithfulness or an increase in irrelevant information.\n1. Prompt-Based Nuances: The way prompts are constructed in G-Eval can lead to different evaluation outcomes based on the specific \ncriteria emphasized in the prompt. RAGAS\u2019s structured approach to extracting and verifying statements might provide a different \nperspective on the same content.\n1. Scoring Granularity: G-Eval's use of token probabilities for scoring might result in more nuanced scores compared to RAGAS's binary \nverification process, which might lead to discrepancies in the final evaluation.",
        "c940a669-ae53-45bf-8d65-30358a293d0b": "32\nTruLens\nUse Cases:\n\u00b7        Detect and Mitigate Hallucination\n\u00b7        Improve Retrieval Quality\n\u00b7        Optimize App Configuration\n\u00b7        Verify the Summarization Quality\n \n \n Content moderation Recall and precision on toxicity and bias\nQuery generation Correct output syntax and attributes, extracts the right information \nupon execution\nDialogue (chatbots, summarization, Q&A) Faithfulness, relevanceTask Evaluation parameters",
        "c96e0bd3-2f96-4585-a678-799145076101": "33\nQ/A with RetrieverRouter engine from llama-index\nKey Points\nUsing the llama-index retriever router query engine to observe its efficacy in routing the incoming question to the correct query engine \nbelow.\n \nThe image above shows the experimental setup.\nPDF1 contains information regarding Emumba Provident fund policy. PDF2 is related to Titanic. Tabular data consists of a table which \nhas names, net worths and nationalities of top billionaires. Json data contains dummy data related to employees.\nEach kind of data has a respective query engine from llama-index built on top of it.\nFinally, the Retriever Router Query engine is used to route the incoming question to the correct query engine.\nResults\nThe results achieved were not very promising. \nThe system was routing the question to the incorrect query engine at times.\nIt was forcing to answer the question using one of the underlying query engines despite the question being out of context.\nIt does not have the ability to break down the question and use multiple query engines to answer parts of it, if that is required.\nPrimarily, it is most evident that Retriever Router Query engine is designed for systems built on the same underlying data but different \ntypes of indexes, rather than systems which are built on different types of data.\n \n References\nRetriever Router Query Engine - LlamaIndex",
        "2398d568-8034-44de-ae15-e29c197d8a4f": "34\nRecursive Retrieval Techniques\nKey Takeaways\n\u2013 Recursive retrieval is beneficial when dealing with hierarchical data or unstructured forms of data, for example data in a tabular form,\n\u2013 Has the concept of IndexNode which contains NodeData+underlying query engine.\n\u2013 During retrieval, if an IndexNode is retrieved, the underlying query engine is called which gets executed.\n\u2013 QueryEngines:\nThe concept of query engines is interesting. In general, we\u2019ve observed that LLMS are not very effective at answering questions from \nstructured forms of data (e.g a table) effectively. Consider an example table containing information regarding billionaires. We want to answer \nthe question \u201cHow many people are from India and France \u201c using the data of the following table. (Image1\u2192 Table , Image2\u2192 Query and \nResponse)\nNow inputting the content of the table and asking the LLM this question directly does not yield good results. LLMS are not great at \nmanoeuvring around this tabular data.\nSolution ? : Llamaindex query engines !!\nIn this specific case we can use pandasQueryEngine from LlamaIndex. Under the hood, it inputs the dataframe column names and asks it to \nwrite python code to extract the information (rather than giving a direct answer). This yields significantly better result than asking the LLM to \nanswer the question directly.\n\u2192 Similarly, there are specific query engines available for each specialised task (JSONQueryEngine for JSON related data, SQLEngine for \nSQL).\nReferences",
        "e2fc7775-287e-4d08-9b8c-ed1a1623933b": "35\n\u2192 Recursive Retriever + Query Engine Demo - LlamaIndex\n\u2192 Advanced RAG: Recursive Retrieval with llamaindex\n\u2192 Jerry Liu on Twitter / X\nJerry Liu on Twitter / X",
        "b08cee1d-c764-4a13-b511-60354b56a5e8": "36\nExploring JSON query engines of llama-index\nKey Points\nLlama-index provides two kinds of query engines to query JSON data. \n( JSON Query Engine , JSONAnalyze )\nJSON Query Engine\nIt is ideally used over a nested JSON data object with a JSON schema as shown in the picture below.\n \nIn the image below, we can understand how does the JSON query engine works under the hood.\n   \n1) We create the query engine by providing it with the schema and the data\n2) At runtime, a query is sent to the query engine to be processed. (e.g What did Jerry comment ?)\n3) The JSON Query engine passes the query and the JSON Schema to the LLM and asks it to generate a \n     valid JSON Path.\n4) LLM returns a valid JSON Path according to the question provided.\n    (e.g $.comments[?(a.username=='jerry')])",
        "35a7c84b-f669-4893-b77a-b34817572599": "37\n5) JSON Query engine executes that JSON Path on the JSON doc to retrieve the answer of the question. \n    (e.g the answer to the above question : \u201cNice Post!\u201c)\nResults:\nFor simple, straightforward questions that involved answering only a single piece of information, the JSON query engine worked fine. \n(e.g Question: Who commented \u201cLoved reading this!\u201d?)\n \nFor slightly more complex questions, where multiple pieces of information need to be fetched from the JSON doc and an \u2018and' or an 'or\u2019 \ncondition is required under the hood, the JSON Query engine doesnt respond well. See image below. \u201cWhat did jerry and simon \ncomment?\u201c was asked.",
        "4a074b70-6008-41c2-9bfd-3bc332b9eb5e": "38\nJSONAnalyze Query Engine\nJSONAnalyze Query engine is provided by llama-index to query over typical JSON data that is retrieved from an api(see example in \nimage below). JSONAnalyze is helpful is answering statistical questions over this JSON data. \n \nIn the image below, we can see how JSONAnalyze query engine works under the hood.",
        "2e10e13d-c317-46da-a60d-1c78a8a242da": "39\n1) We create the query engine by providing it with JSON Data and it stores that data in an SQLite \n    database.\n2) At runtime, a query is sent to the query engine to be processed. (e.g What is the distribution of ages \n   among the individuals ?)\n3) The JSONAnalyze Query engine passes the query and the SQL database schema to the LLM and asks it \n   to generate a valid SQL Query.\n4) LLM returns a SQL Query according to the question provided.\n5) JSONAnalyze Query engine executes that SQL query on the SQLite table to retrieve the answer of the \n    question. \n Results:\nFor most of the questions that conform to the fields and available data in the JSON object (which is converted to an SQLite table under \nthe hood), JSONAnalyze query engine performs well. In the image below, it was able to answer all the these questions without giving an \nerror.\n \nBut for questions that contain any kind of information that is not present in the JSON, JSONAnalyze query engine will through an error.",
        "ec80a41b-53d7-4494-b746-9010593ff244": "40\nIn the image above, the question mentions company name, so the LLM generates an SQL query containing a filter on company name, \nbut the original data doesnt contain any field corresponding to a company name, hence when the JSONAnalyze query engine executes \nthis SQL query on the SQLite database, it throws an error.   \nReferences\nJSON Query Engine - LlamaIndex  \nJSONalyze Query Engine - LlamaIndex",
        "99e48fba-80e8-45de-8a6c-d8608458b991": "41\nCapture Explanations and Compare Our RAG Mindmap\nReference: 10 Challenges in Building RAG-Based LLM Applications - YouTube\nCustomized Models:\nTechniques: Fine-tuning, modifying, transfer learning, etc.\nAlternate Approach:\nTurn inference problem into retrieval plus inference problem aka semantic search.\nUtilize traditional information retrieval to provide context.\nStage One (Data Ingestion):\n1. Data Ingestion:\nStore data efficiently for retrieval.\nBreak down into chunks for manageable processing.\nConvert documents into vector representations using an encoder.\nStore embedding vectors in a vector database.\n2. Challenges:\nData quality issues (garbage data, sensitive data, etc.).\nInaccurate data ingestion (e.g., tables, diagrams, different formats).\nLoss of continuity and high-level context due to document chunking.\nDependency on embedding model quality.\nUnambiguous or incomplete user queries.\nIncorrect document retrieval and utilization (hallucination).\nData Ingestion:\n1. Data Preprocessing:\nData loading and quality checks.\n2. Chunking:\nStrategies: Document-based, semantic, recursive.\nSelection depends on knowledge base nature, model capabilities, and task.\n3. Embeddings.\n4. Scalable Data Ingestion:\nConsider scalability and planning.\nData cleaning for large datasets.\n5. Indexing in Vector Database.\nPre-Retrieval:\nQuerying Challenges:\nHandling ambiguous queries.\nSolution:\nQuery optimisation techniques like \nQuery transformations\nSub query decomposition\nStep back prompting\nMulti query retrieval",
        "eb02bd7b-a925-4f5b-9c49-56f0097ed0f1": "42\nMulti step retrieval\nQuery rewriting\nQuery Routing.\nRetrieval:\nChallenges:\nToo many or too few chunks.\nMissing context, irrelevant chunks, query ambiguity.\nSemantic ambiguity, vector similarity issues.\nRetrieval optimizations.\nSolutions:\nHybrid search\nMetadata integration\nSentence window retrieval\nHypothetical document embedding\nParent child chunk retrieval\nHierarchical index retrieval\nMulti document agents\nChat engine context\nChat engine condense plus context\nProblem: Retrieval of Irrelevant Chunks.\nPost-Retrieval:\nReranking to address information overload and hallucination.\nSolutions:\nUtilize re-rankers like cross encoder reranking, cohere, private reranking APIs.\nGeneration:\nProblems:\nInformation overload, insufficient context window.\nDomain adaptation, chaotic contexts.\nStructuring retrieved information, hallucination, irrelevant responses.\nLogically incoherent responses.\nSolutions:\nInformation compression\nGenerator fine-tuning\nAdapter methods\nThroat of thought\nChain of note\nChain of verification\nReact (reason + act)\nExpert prompting.",
        "5a7f4d38-7883-4931-9c83-e9928811ec1b": "43\nExploring text-to-SQL Query engine of llama-index\nKey Points\nLlama-index text-to-SQL Query engine is designed to answer questions over SQL data effectively.\nIt follows the following pipeline (which other techniques of q/a over structured data follow as well)\n \n1.  The user\u2019s question, and the schema of the table are sent to the LLM.\n2. LLM is prompted to generates an SQL query to answer the user\u2019s question.\n3. LLM generates an SQL query to be run. \n4. The generated SQL query is executed on the relevant table.\n5. The returned value is formatted into a natural language response.\nReferences\nText-to-SQL Guide (Query Engine + Retriever) - LlamaIndex",
        "d80af725-5a52-4390-b1ea-851a95dcdea1": "44\nQ/A over unstructured vs structured data\nKey Points\nLately, two different techniques and pipelines are being used in order to perform Q/A over structured and unstructured data. They utilise \nthe power of LLMs in different ways.\nUnstructured Data\nChat with unstructured data (e.g PDFs, word files etc) is done through standard Retrieval Augmented Generation. The pipeline is shown \nin the above image.\nThe user\u2019s question is converted into a vector embedding (using the same embedding model through which the document chunks were \nconverted into embeddings).\nThere\u2019s a similarity search (e.g cosine similarity) between the question\u2019s vector and vectors of all chunks in the vector database.\nThe most similar chunks are retrieved from the vector database\nThe embeddings of the question and the chunks are converted back into their textual forms and then fed into an LLM to answer the \nquestion. \nStructured Data\nChat with structured data (Tabular data, SQL data, JSON) does not involve the traditional RAG pipeline because, fundamentally, its \nunderlying workings do not favour Q/A over structured data.\nThe idea is that rather than asking the LLM to directly answer a question from structured data, we ask the LLM to write a data-specific \nquery and then execute that query separately on the data.\n \nFor example, if we have an SQL table with columns \u201cCountries,Population\u201c and we have a question \u201cWhat is the total Population of the \ntop three most populous countries ?\u201c \nNow rather than asking the LLM this question directly, we ask the LLM to generate an SQL query to answer the question, and we \nexecute that query separately in the table to get the answer.\n \nThe image above illustrates this flow. \nThere is \nno conversion of data into chunks\nno storage of chunks' embeddings  in vector databases\nno conversion of runtime question into vector embedding\nno similarity index matching and top-k retrieval from database \nAll of the above mentioned steps are part of standard RAG which performs well on unstructured data.",
        "3b2810cb-705e-4c5c-8d05-4c4345d0d538": "46\nQ/A with ReAct agent for routing\nKey Points\nUsing the ReAct agent as a query routing entity. Its main purpose is to route the incoming question from the user to the correct \nunderlying tool/function.\nOf all the query routing mechanisms tested, ReAct gives the most promising results.\nThe image above shows the setup.\nA vector query engine built on PDF. This is a RAG pipeline. (PDF data is about Emumba\u2019s policies mentioned on internal site)\nA vector query engine built on Tabular data (Emumba\u2019s info regarding OPD centers).\nA text-to-sql query engine built on SQL data. (Dummy data containing a table with countries and their population information and another \ntable containing data about employees).\nA code-reader function which reads code from a file. Is added to the agent to test whether Q/A works alongside agentic functionality.\nResults\nThe results were were promising.\nThe following images illustrate correct responses from the above system.\n(Verbose==True is set so that we can see how the ReAct agent is reasoning)",
        "ebbe6c2b-69a1-45d0-a181-337986db9001": "47\n \n \n \n \n(Verbose==False for the following)\n \n \nCorrectly routes all of the above to PDF Query engine\nCorrectly routes to text-to-sql\nCorrectly routes to text-to-sql\nCorrectly routes to text-to-sql",
        "c3dbec98-fc2e-4f49-9a38-2a431807e5a6": "48\n \n \n \n \n \n \nroutes to text-to-sql and correctly answers the question (there no info regarding Karachi  \nin the database)\nThis question involves retrieving information from two query engines.\nThe above images show how we can stop the system from answering question from its own knowledge by  \nimproving system prompts.\nThis shows how the agent handles a task which requires it to read some file and also involves fetching data  \nfrom the database.",
        "65735b46-0515-4f2f-9aef-08235822585c": "49\n \nAll in all, the ReAct agent performs well for routing the incoming question/task to the correct underlying tool.\nIt also provides us with the ability to combine function calling with Q/A systems.",
        "894d3c5e-5b95-4b80-831e-3dfc41a3b790": "50\nRetrieverRouterQueryEngine vs StructuredPlanningAgent vs ReAct for query \nrouting\nKey Points\nComparing the performance of different techniques used as routing entities in order to route the incoming query to the correct query \nengine.\nRetriever Router Query Engine\nWorks well only with different indexes (e.g VectorIndex, SummaryIndex) built on top of the same underlying data.\nDoesnt break down a query into sub tasks and doesnt execute part of a query on one query engine and another sub task on another.\nForces itself to select a query engine to answer a question, even when it is out of context.\nReAct agent\nGives good performance among all techniques tested for Query Routing.\nCorrectly routes the incoming query to the appropriate tool/query engine.\nIf there\u2019s no available tool to handle the query, it doesnt forcefully select something and send the question to it.\nPerforms well at breaking down complex tasks into subtasks and utilising different tools to solve the problem or answer the question.\nAllows the integration of function calling ability within the Q/A system.\nEasier to control LLMs tendecy of answering question from its own inherent knowledge\nStructured Planning Agent\nA kind of a wrapper provided by llama-index to enhance the ability of simple RoutingEngine and add agentic reasoning.\nAllows the user to select the agent of their own choice and embed with the other components of the pipeline. (tools, functions etc).\nConclusion\nReAct agent works the best and allows greater integration of Q/A system with function calling using tools and performing agentic \nreasoning to breakdown complex tasks and questions into sub tasks and then solving them.",
        "eb506b37-d37b-43bd-a97b-610f4bb6591f": "51\nMini-Assistant : Combining Q/A and function calling tools with ReAct on top for agentic reasoning\nThis small project acts like a Mini-Assistant that can chat with databases, PDFs, excel files, and can also perform actions like reading code files and sending emails. \nSetup\nBroadly speaking, there are different tools developed to perform specific functionality and a reasoning engine on top that selects which tools to use in order to achieve a task.\n \nTools :\nTool built which chats with PDFs. It contains a RAG pipeline. In this case, the RAG pipeline is built using data present on Emumba\u2019s internal site. (CareersAndGrowth, ClubsAndActivities, \nPerksAndBenefits etc)\nTool for chatting with SQL database. It is built using llama-index SQLTableRetrieverQueryEngine. In this case, theres a dummy SQL database containing an employees_info table and \ncity_population table.\nTool for chatting with tabular data using PandasQuery engine.\nTool for reading files.\nTool for sending an email.\nAgent :\nReAct agent is used on top which gets the task from user, analyzes which tools would be best utilized for performing which part of the task and then executes the whole task.\nTesting\nThe System was given different tasks to perform as shown below. The end result as well as the reasoning both are shown.\n \nTask : Read the contents of the file named 'tools.py', generate a brief summary of what is is about and send that summary as an email to 'shazilahmed445@gmail.com'\nTools Utilized \u2192 read_file , send_email \n \nTask : How much referral bonus does emumba provide ? Structure this information in a formal email and send it to \u2018shazilahmed445@gmail.com\u2019.\nTools Utilized \u2192 ChatPDF , send_email",
        "5a134fcf-ab34-44e8-9f9b-b71f1dcb8764": "52\n \nTask : What is the population of Tokyo ?\nTools Utilized \u2192 ChatDB\n \nTask : How many total employees are there in the employees_info table ?\nTools Utilized \u2192 ChatDB\n \nTask : Briefly describe emumba policy regarding lunch as well as employee referrals\nTools Utilized \u2192 ChatPDF\n \nTask : List down the perks and benefits provided by emumba, structure this output in a formal email and send it to 'shazilahmed445@gmail.com'\nTools Utilized \u2192 ChatPDF , send_email",
        "f3497441-e1fb-43ba-8e71-3ed64c59fd5a": "53\n \nTask : In the database, how many cities have a population greater than 3 million. Also tell me how often are emumba annual reviews conducted ?\nTools Utilized \u2192 ChatDB , ChatPDF\n \nTask : In the database, how many cities have a population greater than 3 million. Also tell me how often are emumba annual reviews conducted ? Structure the information in formal email and send it \nto 'shazilahmed445@gmail.com'\nTools Utilized \u2192 ChatDB , ChatPDF , send_email",
        "b3470647-2e0a-4388-9766-5edf48fbf33c": "55\nArshik's Day-by-Day Summary of the Genie Learning Path\nDay 1:\nSummary and course notes: The course started off with an overview of Generative AI & LLMs. Why they have become so popular in \nrecent times. The specific use cases of of LLMs and which LLM might be better suited for our use case. The transition from Recurrent \nNeural Networks to the transformer architecture was groundbreaking as it revolutionized generative ai by enhancing language \nunderstanding through self attention mechanism. With concepts of tokenization, embedding layers, and multi headed self attention, it was \nclear how LLMs operate. Further, it was discussed some general tips for designing prompts and their techniques. And at the end, there were \ntopics related to the computational challenges of training LLMs, and it might be more efficient in some use cases to use a pre-defined model \nand fine tune it to our specific use case\nImplementation description: For our implementation, i have used gpt model 3.5 turbo to have some basic prompts completed. In this, \nwe\u2019re testing, how to structure our prompts, how to make sure that the model knows how to answer the prompt.\nNotebook/repo link: Google Colab \nDay 2:\nSummary and course notes: The initial focus centered on data ingestion, where we learned how langchain's document loaders \nseamlessly acquire text data from diverse sources, including PDFs, websites, and databases. Next, we saw efficient text processing, \nequipping us with methods to split large documents into smaller, manageable chunks suitable for further analysis.\nNext, a crucial concept explored was the utilization of embeddings. These embeddings allow langchain to efficiently compare and analyze \ntextual similarities. We also learned about Chroma, a specialized vector store that optimizes storage and retrieval of these embeddings, \nfacilitating rapid information retrieval during searches. Then, having retriever functions for efficient retrieval of information based on specific \nuser needs.\nThen we learned how to combine retrievers with large language models (LLMs). Retrievers locate relevant documents based on the user's \nquestion, while the LLM utilizes the retrieved information to dynamically generate an answer. and to finish it off, the course demonstrated \nbuilding a chatbot that has conversational history. I\u2019ve implemented all these details to make up a system and also evaluated it.\n \nImplementation description: This code builds a Question Answering system that leverages a PDF document and a large language model \nto answer questions about the document's content. It first processes a PDF named \"Pakistan.pdf\" by splitting it into smaller chunks. Then, it \ncreates a semantic search system using weaviate to store and retrieve information efficiently. The core of the system is a langchain pipeline \nthat combines a retriever with a GPT-3.5-turbo LLM for generating answers. The pipeline is guided by a specific prompt designed to \nincorporate retrieved context into the answers. The code evaluates the system's performance using various metrics like faithfulness and \nrelevancy, alongside a more qualitative evaluation using a separate GPT-4 based model with custom scoring criteria. This allows for a \ncomprehensive assessment of the QA system's ability to answer questions accurately based on the information within the \"Pakistan.pdf\" \ndocument.\nNotebook/repo link: Google Colab \nDay 3:\nSummary and course notes: In the first course, we learned about finetuning a model, why do we need to do it and how we can do it. It is a \nprocess which we can do to improve performance of the llms on specific tasks. It helps the llms in specializing particular domains by training \nthem on data relevant to the task. So first, we do pre-training where we load it large dataset and the model is essentially supervising itself \nwith next token prediction. Then we learned about instruction fine-tuning and how they can help enhance the llm\u2019s ability to perform better by \nproviding it with guidance. Then we prepare the data, put it through the training process. I have discussed the parameters of training \nprocess and how changing them can affect your process. and then we evaluate it using test_dataset.",
        "f005a414-f98e-438b-8062-bc71537ae832": "56\nIn the RLHF course, we started off with how rlhf works. We started off with a preference dataset, use it to train a reward model and then use \nthat reward model in a reinforcement learning loop to fine tune the llm. The reference dataset contains paired examples where one text is \nbetter than another for a specific prompt or task. Then the reward model learns to predict how well a piece of text aligns with human \npreferences for the task. Then reward model assigns scores which ultimately leads to the llm learning from it and basically learning through \nthe human feedback. It requires a preference dataset, a prompt dataset which doesn\u2019t contain the responses and an evaluation dataset. \nNow using vertex ai, we can fine tune an llm. The process involves defining several parameters including what dataset to use for training \nand evaluation, the specific llm we\u2019re tuning, number of training steps for both reward model and reinforcement learning stage. Then a \npipeline was configured and a job was run on vertex ai. \nImplementation description: In the following implementation, i have tried to finetune a model using lamini.  Lamini allows us to fine tune a \nmodel in few lines of code. The internal core code of lamini was discussed in the finetuning course. \nNotebook/repo link: Google Colab \nDay 4:\nSummary and course notes: In the initial part of the course, we explored generative ai, what they are capable of. How they can help you \nwrite, read, summarize and even chat with you. We learned how it works, what they\u2019re good at, what their shortcomings are. In the next \nlessons, we learned how to make our own AI projects, like how to give them prompts to write or training them to be better at certain tasks. \nWe discussed rag which we can use to give the model our data and then ask it queries related to the document, fine tuning, how we can \nmake the model work better for a specific use case. Then we learned about which models to use. The concept of tools and agents which i \nhave already discussed in the day 5 section and also implemented them. And lastly, in the end, we learned how these programs can change \nbusinesses and society, that they can do jobs faster or better, which can save money in some cases and even create new opportunities \nalong the way. \nImplementation description:\nNotebook/repo link:\nDay 5:\nSummary and course notes: The initial focus of the course was on openai\u2019s function calling feature. How they are constructed. By using \nthem, we can unlock the full potential of llms by enabling them to interact with the real world, access some information, or perform some \naction. Then we learned about lcel, basically, its an easy way to write instructions for building pipelines. It allows you to write down the steps \nof how to use those tools in the right order. Then we learned about tagging and extraction. Tagging is simply assigning labels to describe \nparts of text and extraction is finding and pulling out specific information based on some tags or criteria. Then we built an overall \nconversational agent with multiple tools, which i have discussed below. \nImplementation description: This code builds a text-based conversational agent using langchain library. The agent utilizes a pre-trained \nopenai model. We have provided tools which are functions for tasks like searching Wikipedia, getting the current temperature, handling \nqueries specific to Pakistan and getting information from a webpage url. One key part is the \"agent chain,\" which defines how the agent \nprocesses user input. This chain can leverage tools and keeps track of the conversation history. Another important piece is the run_agent \nfunction. This function acts as the conversation engine, managing the user interaction loop. It takes user input, feeds it into the agent chain \nalong with past interactions, and retrieves the agent's response. We are also maintaining the chat history, so in this way we have a working \nagent which can use different tools which we have provided to give an answer.\nNotebook/repo link: Google Colab",
        "58fde0a9-0220-46be-bcb6-7d51e6529ba2": "57\nPotential Features for CodeHawk Emumba\nFeatures we currently have:\n1. Context: Providing both repository and file level context for better code reviews. It enables the LLM to check if there are any specific \nconventions being used in the code which is not being followed in the PR.\n2. Incremental Reviews: Review each commit as it's added to a pull request, providing immediate feedback for faster iteration.\n3. Interactive Clarification:\nAI-Powered Explanations: Use the LLM to generate detailed explanations of review comments by mentioning the repo owner, \nclarifying the reasoning behind suggestions or concerns. For example, a line of code has already been reviewed, but we want the \nLLM to further elaborate it. So, we can tag the repo owner in the thread using \u201c@SyedArshikJavedMoinee Please explain this\u201d and \nthen the LLM will take in all the context and elaborate why it made that review.\nDiscussion Threads: Facilitate discussions around specific code sections or review comments to foster collaboration and \nknowledge sharing.\n4. Open Source LLM: Using open source LLMs like llama3-70b and Llama3-8b.\n5. Integration: Currently only integrated with GitHub\n6. Evaluation: Enable custom evaluation of reviews by the LLM (Llama3-70b) by providing metrics to score them. \nFeatures we can add:\n1. Tone Adjustment: Allow users to select the tone of the review (e.g., friendly, formal, technical) to match their team's communication \nstyle.\n2. PR Summaries: Automatically generate concise summaries of pull requests, highlighting key changes, potential risks, and overall \nimpact.\n3. Documentation Generation/Update: Use the LLM to create or update documentation based on code changes, ensuring that \ndocumentation stays in sync with the codebase.\n4. Code Suggestions with In-Context Implementation: Suggest code improvements and allow users to seamlessly integrate them into \ntheir codebase with a click.\n5. Performance Profiling: Identify potential performance bottlenecks or areas for optimization.\n6. Issue Integration: Can integrate with Jira to monitor if the pr is tending to the Jira issue linked.",
        "8355bfa5-f6cd-430b-897e-afa83947ee86": "58\nCodeHawk GitHub Implementation\nIn automating the pull request (PR) review process using LLMs via GitHub Actions, a script is triggered upon the creation of a new PR or the \naddition of a commit. This script dynamically retrieves essential information such as the repository name, owner's name, PR number, and \nthe latest commit ID.\nSubsequently, a Python script is executed with the following functionalities:\nExtraction of all changes made in the commit.\nIdentification of the precise line numbers affected by the changes.\nSubmission of the changed code along with the full file content as context for review by the LLM.\nThe LLM provides feedback on the code, which is then posted back onto the PR. The feedback is displayed at the exact line numbers \ncorresponding to the code reviewed, with relevant comments posted via the GitHub API.\nMultiple LLMs are utilized for this process, including GPT-4, Claude, and Llama-2. Currently, access to the Llama-3 model is facilitated \nthrough Groq, ensuring faster response times.\n \nFeatures Added\n1. Context: Providing both repository and file level context for better code reviews. It enables the LLM to check if there are any specific \nconventions being used in the code which is not being followed in the PR.\n2. Incremental Reviews: Review each commit as it's added to a pull request, providing immediate feedback for faster iteration.\n3. Interactive Clarification:\nAI-Powered Explanations: Use the LLM to generate detailed explanations of review comments by mentioning the repo owner, \nclarifying the reasoning behind suggestions or concerns. For example, a line of code has already been reviewed, but we want the \nLLM to further elaborate it. So, we can tag the repo owner in the thread using \u201c@CodeHawk Please explain this\u201d and then the LLM \nwill take in all the context and elaborate why it made that review.\n4. Open Source LLM: Using open source LLMs like llama3-70b and Llama3-8b.\n5. Integration: Currently only integrated with GitHub and the scripts are running on Docker. So to integrate in some repository, all we have \nto do is create a .github/workflows/pr-review.yml and in that yml file, we will add our github actions script. \n6. Evaluation: Enable custom evaluation of reviews by the LLM (Llama3-70b) by providing metrics to score them. \n7.  PR Summaries: Automatically generate concise summaries of pull requests, highlighting key changes, potential risks, and overall \nimpact.\n \nContext Evaluation:\nWe wanted to evaluate and check if the documents retrieved from RAG are relevant to the code snippet or not. For this, we are using \nuptrain and contextual relevance to determine the relevance scores. So, through the experiments, we have seen that most of the scores are \n1.0 which means that the relevant documents are highly similar to the code snippet. \nQuestion: What are the relevant code snippets for this: '\n    category.remove((err, data) => {\n  '\nAnswer: The relevant code snippet for the query is:\n1exports.remove = (req, res) => {",
        "c8ab041d-1671-47c3-ae4e-399ad33eccf8": "59\nContext: \n2  const category = req.category;\n3  category.remove((err, data) => {\n4    if (err) {\n5      return res.status(400).json({\n6        error: errorHandler(err),\n7      });\n8    }\n9    res.json({\n10      message: 'Category deleted',\n11    });\n12  });\n13};\n1const Category = require('../models/category');\n2const { errorHandler } = require('../helpers/dbErrorHandler');\n3\n4exports.categoryById = (req, res, next, id) => {\n5  Category.findById(id).exec((err, category) => {\n6    if (err || !category) {\n7      return res.status(400).json({\n8        error: \"Category doesn't exist\",\n9      });\n10    }\n11    req.category = category;\n12    next();\n13  });\n14};\n15\n16exports.create = (req, res) => {\n17  const category = new Category(req.body);\n18  category.save((err, data) => {\n19    if (err) {\n20      return res.status(400).json({\n21        error: errorHandler(err),\n22      });\n23    }\n24    res.json({ data });\n25  });\n26};\n27\n28exports.read = (req, res) => {\n29  return res.json(req.category);\n30};\n31\n32exports.update = (req, res) => {\n33  const category = req.category;\n34  category.name = req.body.name;\n35  category.save((err, data) => {\n36    if (err) {\n37      return res.status(400).json({\n38        error: errorHandler(err),\n39      });\n40    }\n41    res.json(data);\n42  });\n43};\n44",
        "9b43d2c9-3dae-4f1d-b5d0-84d666f14c9c": "60\n \nContext Relevance Score: 1.0\nReasoning: The provided context includes relevant code snippets related to category operations in a Node.js application. The specific \nfunction for removing a category matches the query provided, making the context highly relevant. Thus, the extracted context can fully \nanswer the given query. \nEvaluating Code Reviews:\nThe objective of the evaluation is to ensure that the review generated by an LLM is of high quality, contextualized within the code, and \nadheres to essential best practices.\nThe evaluation process unfolds in two stages:\nFirstly, a compact, open-source LLM generates a comprehensive review of the provided code.\nSubsequently, this review undergoes assessment by a larger LLM, which applies a predefined set of 8-10 metrics. These metrics are \nmeticulously selected to gauge the quality and effectiveness of the review within the context of the code.\nBased on the specified evaluation criteria, the larger LLM assigns a score out of 10 to the review, indicating its alignment with the desired \nstandards.\nThrough this systematic evaluation approach, the aim is to ensure that the reviews generated by LLMs not only demonstrate a thorough \nunderstanding of the code but also uphold the principles of sound software engineering practices and coding standards.\n \nEnhancing Code Reviews with Full Repository Context:\nTo provide better code reviews, we decided to pass the entire repository context to the LLM instead of just the file context of changed code. \nThis change allows the LLM to access the entire repository's conventions and coding practices, leading to more informed and accurate \n45exports.remove = (req, res) => {\n46  const category = req.category;\n47  category.remove((err, data) => {\n48    if (err) {\n49      return res.status(400).json({\n50        error: errorHandler(err),\n51      });\n52    }\n53    res.json({\n54      message: 'Category deleted',\n55    });\n56  });\n57};\n58\n59exports.list = (req, res) => {\n60  Category.find().exec((err, data) => {\n61    if (err) {\n62      return res.status(400).json({\n63        error: errorHandler(err),\n64      });\n65    }\n66    res.json(data);\n67  });\n68};",
        "79c78106-fb41-43c5-8b1d-46721a3063cc": "61\nreviews. For example, if the changed code deviates from established repository conventions, the LLM can now identify and flag this \ninconsistency. Some examples of the improved output are shown below:",
        "69c2625f-b8d3-4196-b922-2aec914626c9": "64\nCodeHawk GitLab Implementation\nCodeHawk is a tool designed to automate code review processes using GitLab's API and a language model for generating and refining code \nreviews. It retrieves changed files, generates reviews, and posts them as comments on merge requests.\nEverything is dynamically fetched, including the merge request (MR) number, owner, and project name.\nIncremental Reviews: The pipeline is triggered whenever a new merge request is created or a new commit is added to the merge \nrequest, thus providing us with incremental reviews.\nReview Label: Not every merge request needs to be reviewed. There are instances where the developer wants to create an MR and \nmerge it quickly. If the pipeline were to trigger and process every MR, it would take precious time from the developer. Therefore, a \nfunctionality exists where the pipeline will be triggered and the MR will be reviewed only if the review label is added to the MR.\nInteractive Clarification:\nAI-Powered Explanations: Use the LLM to generate detailed explanations of review comments by mentioning \u201c@CodeHawk\u201c, \nclarifying the reasoning behind suggestions or concerns. For example, if a line of code has already been reviewed, but further \nelaboration is needed, the developer can tag the CodeHawk in the thread using \u201c@CodeHawk Please explain this.\u201d The LLM will then \ntake in all the context and elaborate on why it made that review.\nProcess: Interactive clarification works through webhooks. Whenever a comment is posted, the webhook is triggered, which in turn \ntriggers the pipeline. In the pipeline, we fetch all the relevant information, such as note_id, note_content, discussion_id, and \ncommit_id where the comment was posted.\nOpen Source LLM: Using open-source LLMs like llama3-70b for generating reviews.\nIntegration: Integrated with both GitHub and GitLab. \nReAct Agent: The ReAct Agent is integrated into this system. Its purpose is to post only relevant comments. When the first LLM \ngenerates the initial review, the same context, line of code, and initial review are provided to another LLM to assess whether the review \nwas good and if there is room for improvement. It may provide a refined review. Limitations in this were that we initially started by using \nthe LLM for refined and initial reviews. But the issue was that the LLM was responding with \u201cThe review is correct\u201c every time. So, we \nused GPT-4 for the refined review and it did the same thing. But in some cases it would provide us with the refined review. \nDocker: We have built a Docker image with the necessary environment and scripts, pushed the image to a container registry, and \nconfigured the GitLab pipeline to pull and run the Docker container based on events triggered by merge requests and custom triggers for \nthe interactive clarification. This setup ensures that CodeHawk scripts are executed consistently in a controlled environment.",
        "214d4301-2340-42c4-aa9c-bc68ae4a0c74": "65\nRunning CodeHawk Scripts with Docker in GitHub Actions\n1. Build a Docker Image:\nCreate a Dockerfile that sets up the necessary environment to run your CodeHawk scripts.\n \n2. Push the Docker Image to a Container Registry:\nBuild the Docker image locally:\ndocker build -t pr-review-automation .\nPush the Docker image to the registry:\ndocker push arshikjaved/pr-review:latest\n3. Set Up GitHub Actions Workflow:\nUpdate the .github/workflows/pr_review.yml file:\n1FROM python:3.12\n2\n3WORKDIR /app\n4\n5COPY requirements.txt .\n6\n7RUN pip install -r requirements.txt\n8\n9COPY generate_response.py .\n10COPY reply_thread.py .\n11\n12CMD [\"sh\", \"-c\", \"python $SCRIPT\"]\n13\n14ENV SCRIPT=generate_response.py\n1name: Auto PR Review\n2\n3on:\n4  pull_request:\n5    types: [opened, synchronize]\n6  pull_request_review_comment:\n7    types: [created] \n8\n9jobs:\n10  review:\n11    runs-on: ubuntu-latest\n12\n13    steps:\n14      - name: Checkout repository\n15        uses: actions/checkout@v3\n16\n17      - name: Pull Docker image\n18        run: docker pull arshikjaved/pr-review:latest\n19\n20      - name: Get commit SHA\n21        id: commit_sha\n22        run: echo \"::set-output name=commit_sha::${{ github.event.pull_request.head.sha }}\"\n23\n24      - name: Get PR number",
        "2ae5281d-6200-4beb-b0a4-a1bd1e8fe15e": "66\nUpdating Files inside the container:\n1. docker build -t arshikjaved/pr-review:v1.0 .\n2. docker push arshikjaved/pr-review:v1.0  \n3. For checking:\ndocker run -it --rm arshikjaved/pr-review:v1.0 /bin/bash\ncat reply_thread.py\n \n25        id: pr_number\n26        run: echo \"::set-output name=pr_number::${{ github.event.pull_request.number }}\"\n27\n28\n29      - name: Set script name\n30        id: script_name\n31        run: |\n32          if [ \"${{ github.event_name }}\" == \"pull_request\" ]; then\n33            echo \"::set-output name=script::generate_response.py\"\n34          else\n35            echo \"::set-output name=script::reply_thread.py\"\n36          fi\n37\n38      - name: Analyze code\n39        if: github.event_name == 'pull_request' || (github.event_name == 'pull_request_review_comment' &&  \ncontains(github.event.comment.body, '@CodeHawk'))\n40        env:\n41          EVENT_NAME: ${{ github.event_name }}\n42          COMMENT_BODY: ${{ github.event.comment.body }}\n43          COMMENT_ID: ${{ github.event.comment.id }}\n44          FILE_PATH: ${{ github.event.comment.path }}\n45          LINE_NUMBER: ${{ github.event.comment.line }}\n46          REPOSITORY_OWNER: ${{ github.repository_owner }}\n47          REPOSITORY_NAME: ${{ github.event.repository.name }}\n48          SCRIPT: ${{ github.event_name == 'pull_request' && 'generate_response.py' || 'reply_thread.py'  \n}}\n49          COMMIT_SHA: ${{ github.event.pull_request.head.sha }}\n50          PR_NUMBER: ${{ github.event.pull_request.number }}\n51\n52        run: |\n53          if [ \"${{ env.SCRIPT }}\" == \"generate_response.py\" ]; then\n54            docker run --rm -e OWNER='SyedArshikJavedMoinee' -e REPO_NAME='pr_auto' -e COMMIT_SHA=\"${{  \nsteps.commit_sha.outputs.commit_sha }}\" -e PR_NUMBER=\"${{ steps.pr_number.outputs.pr_number }}\" -e  \nEVENT_NAME=\"${{ env.EVENT_NAME }}\" arshikjaved/pr-review:latest sh -c \"python /app/generate_response.py -\n-owner 'SyedArshikJavedMoinee' --repo-name 'pr_auto' --commit-sha '${{ env.COMMIT_SHA }}' --pr-number  \n'${{ env.PR_NUMBER }}' --event-name '${{ env.EVENT_NAME }}'\"\n55          else\n56            docker run --rm -e OWNER='SyedArshikJavedMoinee' -e REPO_NAME='pr_auto' -e COMMIT_SHA=\"${{  \nsteps.commit_sha.outputs.commit_sha }}\" -e PR_NUMBER=\"${{ steps.pr_number.outputs.pr_number }}\" -e  \nEVENT_NAME=\"${{ env.EVENT_NAME }}\" arshikjaved/pr-review:latest sh -c \"python /app/reply_thread.py --\nowner 'SyedArshikJavedMoinee' --repo-name 'pr_auto' --commit-sha '${{ env.COMMIT_SHA }}' --pr-number '${{  \nenv.PR_NUMBER }}' --event-name '${{ env.EVENT_NAME }}' --comment-body '${{ env.COMMENT_BODY }}' --\ncomment-id '${{ env.COMMENT_ID }}' --file-path '${{ env.FILE_PATH }}' --line-number '${{ env.LINE_NUMBER  \n}}'\"\n57          fi",
        "6249d4c5-1d03-4810-b29f-2f4132ec2759": "67\nSummary\nBy following these steps, we built a Docker image with the necessary environment and scripts, pushed the image to a container registry, and \nconfigured a GitHub Actions workflow to pull and run the Docker container based on events triggered by pull requests and pull request \ncomments. This setup ensures that CodeHawk scripts are executed consistently in a controlled environment.",
        "860f8628-8b6e-4dfe-980a-00f7784b5ffa": "68\nInteractive Clarification (Feature 1)\nThe main idea is to provide developers with a channel to seek clarification on code review suggestions from the LLM. By tagging \n@CodeHawk in a comment on a pull request review, followed by their question or request, developers trigger the LLM to analyse the \nreviewed code, prior reviews, and the comment itself. The LLM then generates a comprehensive response and posts it directly within the \nsame comment thread, facilitating seamless communication and enhancing understanding of the review process.",
        "fd49c2ce-abe6-40e4-8fcf-f47797c852d2": "69\nIn this example, the first comment is the review generated by the LLM, and the second comment is the one where a developer has \nrequested further clarification and then finally a reply back from the LLM.",
        "a7c2548c-8d31-4651-8a74-03e8f7cccd8e": "71\nPull Request Summaries (Feature 2)\nThis feature is triggered whenever a new pull request is created. This will generate a summary of the pull request in a tabular form. \nI\u2019ve currently added these columns to give a high level summary of what has been added in the pull request, identifying and communicating \nany potential errors, and which part of the code the change affects. \nNow, for this, there can be two flows for triggering this feature, first one is that it will generate a pull request summary highlighting the key \nchanges or we can add a command for it, like we can have \u201c@CodeHawk summarize\u201d sort of command and it will trigger this feature and it \nwill generate and post the summary.",
        "fe2f8bd1-ec06-4e38-9dba-f3ed51c16045": "72\nIntegrating with JIRA Issue Tickets (Feature 3)",
        "b8b6934b-5c8e-4f09-ba1b-7434acc286b1": "73\nComparison of PR Reviewer with other similar tools\nCode Rabbit:\n1. Context-Aware Review Feedback: Provides feedback that understands the code changes and their intent, going beyond simple linting \nand static analysis.\n2. Conversational Code Review: Developers can chat with the AI bot within the code to ask questions, get clarification, or have the bot \ngenerate code snippets.\n3. Continuous, Incremental Reviews: Performs reviews on each commit within a pull request, providing feedback as the code evolves.\n4. Actionable Suggestions: Offers suggestions for code improvements that can be directly committed from the pull request interface.\n5. Integration with GitHub and GitLab: Seamlessly integrates with popular version control systems.\n6. Data Privacy and Security: Does not use code for model training, ensuring data privacy and security\n7. Learning Capability: Learns from user feedback and improves over time.\n8. Pull Request Summary and Walkthrough: Provides a high-level summary of the changes in the pull request, along with a detailed \ncode walkthrough for each file.\n9. Issue Validation: Validates pull request changes against linked issues in GitHub or GitLab and identifies other potentially affected \nissues.\n10. Pricing: The free tier is available to use but it provides limited features and just generates a summary of pull request.  \n \nMaverick:\n1. Automated Code Reviews: Provides an extra layer of automated review on every pull request\n2. Incremental Feedback: Catches small issues that might be missed by manual reviews.\n3. GitHub Integration: Leaves feedback directly as comments on the pull request within GitHub.\n4. Free to Use: 100% free for both public and private GitHub repositories.\n \nCodium AI:\nCore Features (Available in Free and Pro Versions):\n1. Automated PR Description (/describe): Automatically generates PR descriptions, including title, type, summary, code walkthrough, \nand labels.\n2. Automated Code Review (/review): Provides adjustable feedback on the PR, potential issues, security concerns, review effort, and \nmore.\n3. Code Suggestions (/improve): Offers suggestions for improving the PR code.\n4. Question Answering (/ask): Answers free-text questions about the PR.\n5. Update Changelog (/update_changelog): Automatically updates the CHANGELOG.md file with the PR changes.\n6. Find Similar Issues (/similar_issue): Automatically retrieves and presents similar issues.\n7. Incremental Review: Provides review feedback on incremental commits.\nPR-Agent Pro Features (Paid Version):\n1. Add PR Documentation (/add_docs): Generates documentation for methods/functions/classes changed in the PR.\n2. Generate Custom Labels (/generate_labels): Generates custom labels for the PR based on user-defined guidelines.",
        "31b39364-0a24-4676-8175-069ae644e1e4": "74\n3. Analyze (/analyze): Identifies changed code components and enables interactive generation of tests, docs, and code suggestions for \neach.\n4. Custom Prompt (/custom_prompt): Automatically generates custom suggestions for improving the PR code based on user-defined \nguidelines.\n5. Generate Tests (/test component_name): Generates unit tests for a selected component based on the PR code changes.\n6. CI Feedback (/checks ci_job): Automatically generates feedback and analysis for a failed CI job.\n7. Similar Code (/find_similar_component): Retrieves the most similar code components from inside the organization's codebase or \nopen-source code.\n8. Inline File Summary: Provides inline summaries of file changes.\n9. Extended Improve: Provides extended code improvement suggestions.\n10. Ask on Code Lines: Allows for questions about specific code lines.\n \nComparison Table\nContext-Aware Review \nFeedback\n  \n   \nConversational Code \nReview\n  \n   (Through Question Answering)\nContinuous, Incremental \nReviews\n  \n   \n   (Uses commands for review and \ndescribe)\nActionable Suggestions \n(with commit)\n  \n   \nIntegration with GitHub & \nGitLab\n  \n   (GitHub only)\n   (GitHub, GitLab, Bitbucket)\nLearning Capability\n  Not Mentioned\n  \nPR Summary & \nWalkthrough\n  \n   \nIssue Validation (GitHub, \nGitLab, Jira)\n  (GitHub, GitLab, Jira)\n   (GitHub, GitLab, Jira)\nFree Tier\n  (Limited features)\n   (Full features)\n   (Core features)\nPaid Tier\n  (Pro, Enterprise)\n   (Pro with additional features)\nDocumentation \nGeneration \n   (Pro)\nTest Generation\n  (Pro)\nSimilar Code Search\n  (Pro)\nInline File Summary\n  (Pro)Feature Code Rabbit Maverick CodiumAI PR-Agent",
        "1990c119-beb8-4bdd-8da0-7b74f40e71bb": "75\nDevelopment Automation PoC\nThe aim of this Development Automation is to streamline the process of code review within Github Pull Requests (PRs) using advanced \nautomation techniques. By leveraging the use of Large Language Models (LLMs) and automated scripts, the workflow intends to enhance \nthe efficiency and accuracy of code reviews, ultimately leading to improved code quality and developer productivity. In this planning phase, \nwe will outline the basic workflow, select appropriate LLMs, and devise an implementation plan to test and evaluate their effectiveness in \ncode generation, testing and documentation. \nBasic Workflow:\nFollowing is the workflow for one of the possible solutions so far:\nThe basic workflow that I'm developing involves reviewing Github Pul Requests (PRs). Whenever a pr is made or is updated, then it \nshould trigger an action in the workflow, which in turn runs a python script. We get hold of all the code that has been changed in the pr \nand then we can do code reviews file-by-file. Then using those code reviews, we can generate new code to mitigate any mistakes. This \ncan be one potential solution. So far, the only issue I'm currently facing is presenting the output somewhere where the user can easily \nsee and understand. \n \nChoice for LLMs:\nThe choices for LLMs are given below. We can add and test other LLMs as well.\nGPT-4\nCodeLlama\nLlama-2\nMixtral\n \nImplementation Plan:\nThe following is the implementation plan:\nResearch and explore additional solutions that could be implemented.\nChoose one solution from the options identified. Then, select a suitable Large Language Model (LLM).\nTest the LLM\u2019s capability for code generation and documentation and assessing its effectiveness.\nIf the LLM can generate the desired outputs then we can continue with our solution; otherwise we might need to consider fine tuning it. \nWe might need to use code review datasets if they are available or we can develop scripts to scrape code review data from github \nrepositories. \nSeparate logics for reviewing, writing code, unit testing and creating documentation. Each step can be made dependent on the prior. \n \n \nEvaluation & Testing:\nFollowing are the ways that we can do evaluation and testing:",
        "4b820688-0321-415a-940a-b12aaeb71ba9": "76\nOne way could be mutation testing. So, this might include injecting known bugs deliberately into the code and checking if the code \nidentifies them correctly or not.\nUsing a smaller, faster LLM to perform the initial code review. And then pass the code, identified issues and suggested changes from the \nsmaller model to a larger LLM. It can act as a supervisor.\nQualitative Feedback",
        "4caa73ac-2352-4b0d-b694-36974c59d663": "77\nSelf Hosting LLM\nThis is a step by step guide on how to deploy your LLM on an EC2 instance. The instance that we are currently using is g4dn.4xlarge with a \nsingle T4 GPU.\nLog in to the AWS console. The credentials will be provided to you. This is the link to the AWS url \nAn instance will already exist for you, right now, the name of the instance is \u201cLLMs-Zainab\u201d.\nStart the instance by going to instance state and clicking on \u201cStart Instance\u201d.\nNow, you would be given a .pem key. Go to the directory where the key is located. And start your terminal there and write:\nssh -i \"llms-zainab.pem\" ubuntu@ec2-3-214-169-31.compute-1.amazonaws.com\n \nNow, your instance has started. The drivers for the GPU will already be set up for you but if they are not, you can use this short video to \nset it up. \n \nUse the following commands to make sure that you have set up correctly. \n      nvidia-smi and nvcc \u2013version\nThe commands nvidia-smi and nvcc --version are used to obtain information about your NVIDIA GPU and the CUDA toolkit.\nThere are some options available to us. They are: \nDeployment\nCurrently, we have done deployment using vllm and using model from HuggingFace. We have deployed the following models:\nastronomer-io/Llama-3-8B-Instruct-GPTQ-4-Bit \nTheBloke/Mistral-7B-Instruct-v0.1-AWQ \nmicrosoft/phi-2 \nmicrosoft/Phi-3-mini-4k-instruct\nNow, we are not deploy larger models due to our instance but we can surely accomplish that by upgrading to a much powerful instance. \nSteps to deploy LLMs using vllm:\nInstall vllm through pip install vllm\nMake sure that cuda is installed and updated\nUse the following command to check the cuda version nvcc --version \nCommand to deploy LLM using vllm python3 -m vllm.entrypoints.openai.api_server --model TheBloke/Mistral-7B-Instruct-\nv0.1-AWQ  --dtype half --tensor-parallel-size 1\nUsing \u2013dtype half sets the datatype for the model\u2019s parameters. Using half (FP16) helps in reducing the memory usage and can speed \nup the computations. \nSetting \u2013tensor-parallel-size to 1 means that tensor parallelism is not being used, and the computations will not be distributed across \nmultiple devices. (Using 1 GPU).\nYou can also run vllm through docker. The command is:\nFor integration, use the following code: \ndocker run --runtime nvidia --gpus all \\ -v ~/.cache/huggingface:/root/.cache/huggingface \\ --env \n\"HUGGING_FACE_HUB_TOKEN=<secret>\" \\ -p 8000:8000 \\ --ipc=host \\ vllm/vllm-openai:latest \\ --model",
        "67c3666c-d75e-41fe-a41c-1d8a30393c29": "78\nmistralai/Mistral-7B-v0.1\n \nOther Options for Deployment:\nOllama:\nVery easy to setup and start working. \nDeployed it on azure instance and it was working fine but since it was on CPU, the response time was not very good.\nSteps to deploy LLMs using Ollama:\nAfter going in to your instance run the installation script:\nStart the Ollama service:\nPull and run the any model available on Ollama:\nTroubleshooting Connection Issues: \nIf you encounter a \"connection refused\" error, follow these steps:\nRerun the model:\nAPI Access:\nUse the following format for API calls:\n1base_url = \"http://3.214.169.31:8000/v1\" \n2\n3def create_llm():\n4    return OpenAI(model=\"TheBloke/Mistral-7B-Instruct-v0.1-AWQ\", base_url=base_url, api_key=api_key)\n5\n6llm = create_llm()\n1curl -fsSL https://ollama.com/install.sh | sh\n1service ollama start\n1ollama pull llama3.1\n2\n3ollama run llama3.1\n1mkdir -p /etc/systemd/system/ollama.service.d \n2\n3echo \"[Service]\" >> /etc/systemd/system/ollama.service.d/environment.conf \n4\n5echo \"Environment=OLLAMA_HOST=0.0.0.0:11434\" >> /etc/systemd/system/ollama.service.d/environment.conf \n6\n7systemctl daemon-reload \n8\n9systemctl restart ollama\n1ollama run llama3.1\nhttp://your_ip_address:11434/api/generate",
        "ac3a69fd-d313-4369-9474-5facd03b5c05": "79\nOpenllm:\nSupports wide range of open source LLMs. \nProvides OpenAI compatible API endpoints.\nEasy deployment using Docker containers\ndocker run --rm -it -p 3000:3000 ghcr.io/bentoml/openllm start facebook/opt-1.3b --backend pt\nMight require more configuration as compared to Ollama especially for non-Docker deployments.\nImplementation:\nAfter trying to deploy through OpenLLM, we encountered couple of issues. While trying to deploy with Docker, we were shown this error:\nValueError: Bfloat16 is only supported on GPUs with compute capability of at least 8.0. Your Tesla T4 GPU has compute \ncapability 7.5.\nand while trying to do it through python package, still encountered, application startup failed. Will provide a more comprehensive error. \nLitellm:\nProvides a wrapper over Ollama. \nProvides OpenAI compatible endpoints\nCurrently deployed Llama3-70b using litellm. \nCan be used to deploy custom models from Huggingface.\nIssues one can face while deployment:\n1. There may be an issue that you\u2019ve successfully deployed your model but when you try to access that endpoint, it might give you \n\u201cConnection Timeout\u201d. Now, this is most probably the issue of inbound rules. Inbound rules are settings in a network's firewall or security \ngroup that control the traffic allowed to enter your system or network from external sources. So, if you have not added the port number \non which the LLM is deployed then it will give you \u201cConnection Timeout\u201d. Steps to add inbound rules:\n \n1. Navigate to the Security Groups: In the AWS Management Console, go to the EC2 dashboard and select \"Security Groups\" under the \n\"Network & Security\" section.\n2. Select a Security Group: Choose the security group to which you want to add the inbound rule.\n3. Edit Inbound Rules: Click on the \"Inbound rules\" tab and then click the \"Edit inbound rules\" button.\nAdd Rule:\n1. Type: Select the type of connection (e.g., HTTP, HTTPS, SSH, Custom TCP, etc.).\n2. Protocol: This is usually auto-filled based on the type.\n3. Port Range: Specify the port or range of ports (e.g., 80 for HTTP, 22 for SSH).\n4. Source: Define the source IP address or range that is allowed (e.g., 0.0.0.0/0 for all IP addresses, which is not recommended for \nSSH).\n5. Save: After configuring the rule, click the \"Save rules\" button.\n1from litellm import completion\n2\n3response = completion(\n4    model=\"ollama_chat/llama3:70b\", \n5    messages=[{ \"content\": \"Can you do code reviews?\",\"role\": \"user\"}],\n6    api_base = \"http://3.214.169.31:11434\"\n7)\n8print(response)",
        "7793fbde-7678-4840-ad09-203519466c29": "80\n \n2. Before doing anything on the instance, make sure that you have ample space in your instance.\nYou can use the following commands to check:\n1. df -h (The df (disk free) command is used to display information about the disk space usage of your file systems)\n2. du -h --max-depth=1 (The du (disk usage) command is used to estimate the file space usage of directories and files)",
        "28c9493e-7f60-4b21-b26c-f4a175c9d46c": "81\nFine Tuning Techniques-LLM\nMain Classification Mindmap: https://www.mindmeister.com/app/map/3263199893\nUse Case Classification Mindmap: https://www.mindmeister.com/app/map/3271515761\nPEFT VS Full Fine Tuning\nFine-tuning and parameter-efficient fine-tuning are two approaches used in machine learning to improve the performance of pre-trained \nmodels on a specific task.\nFine-tuning is taking a pre-trained model and training it further on a new task with new data. The entire pre-trained model is usually trained \nin fine-tuning, including all its layers and parameters. This process can be computationally expensive and time-consuming, especially for \nlarge models.\nOn the other hand, parameter-efficient fine-tuning is a method of fine-tuning that focuses on training only a subset of the pre-trained model\u2019s \nparameters. This approach involves identifying the most important parameters for the new task and only updating those parameters during \ntraining. Doing so, PEFT can significantly reduce the computation required for fine-tuning.\n \nPEFT Benefits\n1. Decreased computational and storage costs: PEFT involves fine-tuning only a small number of extra model parameters while \nfreezing most parameters of the pre-trained LLMs, thereby reducing computational and storage costs significantly.\n2. Overcoming catastrophic forgetting: During full fine-tuning of LLMs, catastrophic forgetting can occur where the model forgets the \nknowledge it learned during pretraining. PEFT stands to overcome this issue by only updating a few parameters.\n3. Better performance in low-data regimes: PEFT approaches have been shown to perform better than full fine-tuning in low-data \nregimes and generalize better to out-of-domain scenarios.\n4. Portability: PEFT methods enable users to obtain tiny checkpoints worth a few MBs compared to the large checkpoints of full fine-\ntuning. This makes the trained weights from PEFT approaches easy to deploy and use for multiple tasks without replacing the entire \nmodel.\n5. Performance comparable to full fine-tuning: PEFT enables achieving comparable performance to full fine-tuning with only small \nnumber of trainable parameters.\n \nPEFT Categories:\n1. Additive PEFT:\n\u00b7        Introduces additional trainable parameters while keeping the pre-trained backbone unchanged.\n\u00b7        Increases model complexity by adding parameters.\n\u00b7        Only updates the weights of the additional modules or parameters during fine-tuning for a specific downstream task.\n\u00b7        Reduces storage, memory, and computational resource requirements compared to standard full fine-tuning.\n1. Selective PEFT:\n\u00b7        Fine-tunes a subset of the existing parameters rather than adding new ones.\n\u00b7        Utilizes a binary mask to selectively choose which parameters to update during fine-tuning.\n\u00b7        Updates only the parameters that are selected for fine-tuning, reducing the computational cost compared to full fine-tuning.",
        "c78abfd3-74bd-4ff0-8810-2157b0ec7422": "82\n\u00b7        Enhances model performance over downstream tasks while minimizing the increase in model complexity.\n1. Reparametrized PEFT:\n\u00b7        Reparametrizes the model's architecture to achieve parameter efficiency during training.\n\u00b7        Equivalently transforms the model's parameters to a low-rank parameterization.\n\u00b7        Allows conversion back to the original weight parameterization for unchanged inference speed.\n\u00b7        Utilizes low-dimensional reparameterization effective for fine-tuning while maintaining inference efficiency.\n1. Hybrid PEFT:\n\u00b7        Combines advantages of different PEFT methods or seeks to establish a unified perspective by analyzing similarities among methods.\n\u00b7        Tailors the fine-tuning approach based on the specific requirements and efficacy across different tasks.\n\u00b7        Aims to improve overall fine-tuning effectiveness by leveraging the strengths of diverse PEFT approaches or establishing a cohesive \nframework for parameter-efficient fine-tuning.\n \nGPU Requirements of Different Fine Tuning Techniques\nRoBERTa FT 5.38\nRoBERTa Adapter 15.29\nRoBERTa Prompt Tuning 3.84\nRoBERTa Prefix Tuning 3.56\nRoBERTa IA3 2.62\nRoBERTa BitFit 3.27\nRoBERTa ChildTuning 6.02\nRoBERTa LoRA 3.59\nRoBERTa AdaLoRA 3.57\nRoBERTa MAM Adapter 15.35\nRoBERTa ProPELT(adapter) 8.63\nRoBERTa ProPELT(lora) 8.25\nRoBERTa ProPELT(prefix) 9.47\nT5 FT 25.17\nT5 IA3 21.36\nT5 LoRA 19.43\nLlama-7B-Alpaca FT 169.36\nLlama-7B-Alpaca IA3 128.57\nLlama-7B-Alpaca LoRA 124.82\nLlama-7B-Alpaca QLoRA 56.46Model Method GPU (Memory Usage)",
        "2c69e278-0f3f-4123-9352-f603bc697227": "83\nT5 Compactor 24\nRoBERTaLARGE HIWI 48\nLLaMA-7B DoRA 28.2\nVL-BART DoRA 20.5\nLLaMA 7B VeRA 21.69\nRoBERTAa DyLoRA 32\nRoBERTa-large SORA 24.268\nLlama2 7B Laplace-Lora 8.43\nCLIP, FLAN-T5 L-LoRA 24\nRoBERTA-base MoA 80\nRoBERTA-base Kernel-mix-lite(qv)/(qvo) 5.38\nLLAMA-2-7b LOFTQ 15\nLLAMA-2-7B QLoRA 48\nSupernet QA-LoRA 32",
        "7b5e7bb3-6904-4298-85d4-5f18160601aa": "84\nFine tuning LLMs using Lora\nModels that I tried but didn\u2019t work:\nI attempted to fine-tune the Llama 3-8b and Mixtral-8x7b models, but both attempts resulted in persistent \"CUDA out of memory\" errors. \nStandard troubleshooting steps, such as clearing the cache and restarting the kernel, did not resolve the issue. This strongly suggests that \nthe available GPU resources were insufficient to handle the memory requirements of these large language models during fine-tuning.  \nResource Utilization:\nWe conducted these experiments on an AWS g4dn.4xlarge instance equipped with a T4 Single GPU, aiming to leverage its computational \npower for fine tuning. Despite employing small models and optimizing configurations for efficiency, we consistently encountered CUDA out of \nmemory errors, indicating insufficient GPU memory to accommodate the computational workload.\nConfigurations for finetuning Llama2-7b:\nUsing llama2-7b as a base model, we fine tuned it on our dataset and it has provided very bad results. Using 1000 examples for finetuning. \nFollowing configurations were used for the first iteration:\nmodel_name = \"meta-llama/Llama-2-7b-hf\"\nload_in_4bit = True\nbnb_4bit_use_double_quant = True\nbnb_4bit_quant_type = \"nf4\"\nbnb_4bit_compute_dtype = torch.bfloat16\nnumber_of_training_examples = 800\nnumber_of_valid_examples = 200\n \nlora_r = 16\nlora_alpha = 64\nlora_dropout = 0.05\nbias = \"none\"\ntask_type = \"CAUSAL_LM\"\nseed = 33\noutput_dir = \"./results\"\nper_device_train_batch_size = 2\ngradient_accumulation_steps = 4\nlearning_rate = 1e-4\noptim = \"paged_adamw_32bit\"\nnum_train_epochs = 2\nwarmup_steps = 10",
        "f47faa58-6815-430b-b235-13813f6cf116": "85\n \nAnd after training, we tested it with some prompts, \nand its output was:\n fp16 = True\nlogging_steps = 100\n1message = f'''\n2### Instructions:\n3You are an experienced software engineer tasked with doing code reviews of junior developers. Be very strict  \nand explain all the issues in the code.\n4\n5### Input:\n6Perform a comprehensive code review of the following snippet: {hunk}\n7\n8### Response:\n9'''\n1.  returnns a Return statement missing a colon \nreturn a should be return a;Return statement without a value\nThe return statement without a \nvalue is not allowed in JavaScript. \nYou need to assign a value to the \nreturn statement, like this: return \na;The code snippet \nappears to be a \nfunction definition. \nThe function returns \nan array of objects. \nThe code looks clean \nand well-structured. \nHowever, there are no \nissues with this \nsnippet.\n2.  result = (4 + 2 * \n3))The line result = (4 + 2 * 3) can be \nsimplified to result = 4 + 6 directly, \nwithout the need for multiplication. \nRemove the multiplication operation and \nkeep the code concise.Unnecessary multiplication operator \nused\nThe multiplication operator is not \nneeded here, as the addition \noperator already performs \nmultiplication. The correct code \nwould be result = (4 + 2 * 3)The code snippet \nappears to be a \ncalculation of a \nresult. The \ncalculation is simple \nand straightforward, \nbut it does not seem \nto be part of a \nlarger program or \nfunction. The code is \nwell-formatted and \neasy to read. \nHowever, without more \ncontext, it's unclear \nwhat the purpose of \nthis calculation is \nor how it's being \nused.# Model Input Llama2-7b Finetuned Llama2-7b Distilled Version using \nLlama3-70b as Teacher \nmodel:",
        "e6ef2996-2b52-4e9d-8dcb-3f08256a66f3": "86\n3.  if x=5: Thank you for submitting this pull \nrequest! However, I have a few issues \nwith the code that need to be addressed:\nInconsistent indentation: Please \nensure that your indentation is \nconsistent throughout the code. \nCurrently, there are some lines that \nare not indented properly.\nMissing docstring: A documentation \nstring should be included for any \nfunction or class defined in the code. \nCan you please add a docstring to \nif x=5?\nUnused variable: There is a variable \nx defined but it's not being used \nanywhere in the code. Please \nremove it or use it properly.The if statement is not properly \nindented. It should be indented 4 \nspaces to conform to PEP8 style \nguidelines.\nExplanation: In Python, it's important \nto follow the PEP 8 style guide for \ncode formatting to make the code \neasier to read and understand. The \nif statement should be indented 4 \nspaces to indicate that it's a block of \ncode.Code snippet has a \nsyntax error. The \n`if` statement is not \nproperly formatted. \nIt should be `if x == \n5:`, not `if x=5:`.\n4.  if x == 0 The if statement is not properly \nformatted. It should be on a new line and \nindented.\nCorrected version:The if statement is not properly \nformatted. It should be on a new line \nand indented.\nCorrected version:if x == 0: \n      return Review: \nThe code snippet is a \nPython if statement that \nchecks if the variable x is \nequal to 0. If it is, the \nreturn statement is \nexecuted. The code is \nsimple and easy to \nunderstand. However, \nthere is no error handling \nor logging in this snippet, \nwhich could lead to \nissues if x is not equal to \n0.\n5.  ages = {\n    'pam': 24,\n    'jim': 24\n    'michael': 43\n}The ages dictionary is defined with key-\nvalue pairs in separate lines, which can \nlead to readability and maintainability \nissues. It's better to define the dictionary \nwith key-value pairs on a single line for \nbetter readability. Here's an example of \nhow you could rewrite the code:\nExplanation: By defining the dictionary \non a single line, it's easier to read and \nunderstand the code. It also makes the \ncode more concise and easier to \nmaintain.The ages dictionary keys are not in \nlowercase. It's a good practice to \nuse lowercase for variable names to \nmake the code more readable and \navoid conflicts with future updates. \nPlease change the keys of the ages \ndictionary to be in lowercase.The code snippet is a \ndictionary with three \nkeys and values. The \nkeys are 'pam', \n'jim', and 'michael' \nand the values are \n24, 24, and 43 \nrespectively. The \ncode looks clean and \nwell-formatted. \nHowever, it would be \nbetter to include a \ncomment explaining \nthe purpose of the \ndictionary and what \nit is used for.\n1if x == 0 {\n2   // code here\n3}\n1if x == 0 {\n2   // code here\n3}\n1ages = {'pam': 24, 'jim': 24, \n'michael': 43}",
        "debf909a-a721-41ae-8921-c94b993e9b93": "87\n6.  len('hello') = 5 The line len('hello') assigns a \nlength of 5 to a string that is empty. This \nis incorrect and can lead to unexpected \nresults. Please fix the issue by providing \na valid string or using an empty string \nliteral.Changed code is not enclosed in a \nstring literal\nExplanation: The len() function \nshould always be called on a string \nliteral to ensure that the length is \ncalculated correctly. Enclosing the \nstring 'hello' in a string literal \n('\"hello\") would fix this issue.Code has a syntax error. \nThe code is not properly \nformatted. It is missing a \nclosing quote for the \nstring literal. Additionally, \nthe code does not seem \nto be doing anything \nuseful. It is just printing \nthe length of a string. If \nthis is intended, the code \nshould be improved to \nprovide more context and \nexplanation. Otherwise, it \nshould be removed or \nmodified to perform a \nuseful task.\n7.  fro i in \nrange(10):The range function is used without an \nexplicit end index. This can lead to \nunexpected behavior and potential bugs. \nPlease provide an end index for the \nrange, e.g., for i in range(10): \n....The range(10) loop is not inside a \nwith statement or a try-except \nblock, which means that the \nresources used by the loop are not \nproperly cleaned up when the \nprogram exits. This can lead to \nmemory leaks and other issues. \nPlease fix this by either wrapping the \nloop in a with statement or adding \na finally block to ensure proper \nresource cleanup.Code snippet appears \nto be a Python code \nsnippet, but it is \nincomplete and lacks \ncontext. Without more \ninformation, it's \ndifficult to provide \na comprehensive \nreview. However, it \nseems to be a loop \niterating over a \nrange of numbers, \nwhich is a common \nPython construct. \n8.  for i range(10):The range() function is used to \ngenerate a sequence of numbers, but in \nthis case, it is not necessary since the \nfor loop already defines a sequence of \n10 numbers. Removing the range() \ncall will simplify the code and make it \nmore readable.The range function is not defined in \nthis context. Define it before using it.\nExplanation: The range function is \na built-in Python function that \ngenerates a sequence of numbers. \nIn this code, it is being used without \nbeing defined, which can cause an \nerror. To fix the issue, define the \nrange function before using it. For \nexample: from typing import \nrange; for i in range(10): ...1. The code snippet is a \nPython loop, but the \nvariable i is not used \nanywhere in the loop. \nThis might be \nintentional, but it \nshould be checked. \n2. The loop is iterating \nover a range of 10, \nbut the purpose of the \nloop is not clear. It \nmight be a debugging \nloop or a test loop, but \nit should be more \ndescriptive. \n3. The code is well-\nformatted and easy to \nread.\n9.  ages = {\n    'pam': 24,The code prints the age of \"Michael\" \nwithout any input validation. This could The code uses a dictionary to store \nages, but the key michael is not The code snippet is \nmostly correct, but there",
        "b2605804-e8cc-4683-9efb-2a207961b9e7": "88\n    'jim': 24,\n    'michael': 43\n}\nprint(f'Michael \nis \n{ages[\"michael\n]} years old.')lead to unexpected results if the user \nenters invalid data. Please validate user \ninput before processing it.defined in the dictionary.\nPlease define the key or fix the \nissue.are a few issues. \n1. The variable ages is a \ndictionary, but it's not \nbeing used anywhere in \nthe code. \n2. The print statement is \nnot using the ages \ndictionary, it's just printing \na string. \n3. The code is not doing \nanything with the age of \nMichael, it's just printing \nit. \n4. There's no error \nhandling or logging in \ncase of an exception.\n10.  def foo():\n    return [1, 2, \n3\nprint(foo())The print(foo()) statement is not \nnecessary and can be removed to \nsimplify the code. It does not affect the \nfunction's behavior or output.Issue: The code is printing a value \ninstead of returning it.\nChange print(foo() to return \nfoo() to fix this issue.1. The code snippet is a \nsimple function \ndefinition that returns \na list and prints its \ncontents.\n2.  The code is well-\nformatted and easy to \nread.\n3. There are no syntax \nerrors or logical \nissues. \n4. The code is clean and \nefficient.\n11.  ages = \n{'pam'=24}Change the ages dictionary to use \nquoted strings, like this: ages = \n{'pam': 24}Using a dictionary with a single key-\nvalue pair is not recommended as it \ncan lead to confusion and errors in \nyour code. It's better to use a list or \nan object instead.\nReason: Dictionaries are meant to \nstore multiple key-value pairs, and \nusing one with only one key-value \npair can cause issues when you \nneed to access or modify the value. \nAdditionally, it can make your code \nharder to read and maintain.\nSuggestion: Instead of using a \ndictionary, consider using a list or an \nobject that stores multiple key-value \npairs. This will make your code more \nflexible and easier to work with in the \nlong run.Code looks clean and \nwell-formatted. The \nvariable names are \ndescriptive and \nfollow a consistent \nnaming convention. \nThe data dictionary \nis properly defined \nwith a key-value \npair. No syntax \nerrors or logical \nissues found.",
        "276c1dd8-8350-4251-941a-8155afb79331": "89\nConfigurations for Phi-2 finetuning:\nThese are the parameters that have been changed since the last finetuning job:\nAnd its response was slightly better than the previous finetuned version of Llama2-7b. It was able to respond within the context of the code \nand sometimes give the correct output. The responses are given below: Evaluation by \nLlama3-70bScore for Llama2-7b Finetuned: 1/10 Score for Llama2-7b: 3/10 Score: 2.25/10\nmodel_name = \"microsoft/phi-2\"\nnumber_of_training_examples = 1800\nnumber_of_valid_examples = 200\n \nlora_r = 16\nlora_alpha = 32\nlora_dropout = 0.01\n \nlearning_rate = 2e-5\n \n1.  returnns a This is a bit of nitpick but i think it would \nbe better to use \u2018return' instead of \n'returnns\u2019The code you have provided has several \nissues that need to be addressed before it \ncan be considered high quality. Here are \nsome suggestions for improving your code:\n1. Use more descriptive variable names: \nnumbers and average are not very \ndescriptive, so consider renaming them \nto something like number_list and \ncalculate_mean, respectively.\n2. Check the types of variables you're \nusing: You're using a mix of integers and \nfloats in your code, which can cause \nissues when performing calculations. \nConsider converting all the numbers to \nfloat type before doing any math \noperations.\n3. Use the correct syntax for calculating \nthe mean: The current implementation \nuses numbers[i]/len(numbers), but \nthis will give you a floating-point value if \nthe input list is empty. You should use \nsum(numbers)/len(numbers) to avoid \nthis issue.# Model Input Phi-2 Finetuned Phi-2",
        "a20d5d7b-699d-4d67-8f74-77174b70a99f": "90\n4. Add comments to your code to make it \nmore understandable: Here's an \nexample of how you could improve your \ncode with better commenting:\n1.  result = (4 + 2 * 3)) This is a syntax error. You need to close \nthe parenthesis.Nothing\n1.  if x=5: This is a syntax error. You should use `==` \ninstead of `=`.I can review a pull request and provide \nfeedback on code quality and good \npractices. here's an example of what my \ncomments might look like:\n   if x == 5:\n1.  if x == 0 This is a good practice to use `isinstance` \nto check if the input is an integer.I am happy to help you with that. However, \nit seems like there might be an issue in \nyour code as well. Can you please provide \nsome more information about what the pull \nrequest is for and where exactly this part of \nthe code is located? This will help me \nprovide better assistance to you.\n1.  ages = {\n    'pam': 24,\n    'jim': 24\n    'michael': 43\n}ages = { 'pam': 24, 'jim': 24, 'michael': 43 }Sure thing! I'm happy to help with that. \nBased on the information you've provided, it \nlooks like there may be an issue with the \nvariable ages being assigned a dictionary \nrather than individual values for each \nperson.\nIn your code, you're trying to create a \ndictionary of ages using a set of key-value \npairs. However, this approach won't work \nbecause you're trying to assign multiple \n1# Input a list of numbers\n2number_list = [1, 2, 3, 4, 5]\n3\n4# Check if the list is empty\n5if len(number_list) == 0:\n6    return \"Error: Empty list\"\n7else:\n8    # Calculate the mean by \nsumming up all the numbers and \ndividing by the length of the \nlist\n9    mean = \nsum(number_list)/len(number_li\nst)\n10    # Return the result as a \nstring with two decimal places\n11    return \"\n{:.2f}\".format(mean)\n1# this is better practice to \nuse f-strings instead of string \nconcatenation\n2print(f'{x} is equal to 5')",
        "3ea4beaf-61c0-4c60-81e6-35bd41b1aebe": "91\nvalues to one variable, which is not allowed \nin Python. Instead, you can simply initialize \nthe ages variable with individual age \nvalues for each person as shown below:\nThis should create a dictionary with the \ncorrect values for each person's age. I hope \nthat helps! Let me know if you have any \nother questions.\n1.  len('hello') = 5 return \"This code is not functional. The \nassignment statement `len('hello') = 5` \ndoes not modify the length of the string \n'hello'. It is possible that the code is \nintended to assign the length of the string \nto a variable, but this is not the case here. \nThe correct syntax for assigning the length \nof a string to a variable is `len('hello') = 5`.Nothing\n1.  fro i in range(10): The code is correct and efficient. No \nissues found.I have reviewed your pull request and found \nthat there is a line of code here: for i in \nrange(10):. This is a good start, but it \nwould be more helpful to give some context \nabout what this code does. Could you \nplease clarify the purpose of the range \nfunction in this case?\n1.  for i range(10): for i in range(10): print(i) Thank you for submitting a pull request. I \nwill review it carefully and provide feedback \nas soon as possible. However, at this time, \nI am unable to comment on your changes \ndue to technical constraints. If you have \nany other questions or concerns, please \nfeel free to ask.\n1.  ages = {\n    'pam': 24,\n    'jim': 24,\n    'michael': 43\n}\nprint(f'Michael is {ages[\"michael]} \nyears old.')`None`. AI\n10. def foo():\n    return [1, 2, 3\nprint(foo())foo() should be foo() without the closing \nparenthesis.Sure, I can help you review this pull \nrequest. Can you please provide me with \nsome more context about the code in \nquestion? What are the changes that have \nbeen made and what is their purpose? This \n1pam_age = 24\n2jim_age = 24\n3michael_age = 43\n4\n5ages = {'pam': pam_age, 'jim': \njim_age, 'michael': \nmichael_age}",
        "5d752dd5-ccff-4e40-a358-5358c0761fc7": "92\nConfigurations for Mistral-7B Instruct finetuning:\n \nAnd the responses from the model are: will help me to provide a more detailed \nfeedback on the quality of the code and \ngood practices that should be followed.\n11. ages = {'pam'=24} suggestion ages = {'pam': 24} Nothing\n Evaluation by Llama3-70b Score for Phi-2 Finetuned: 6/10 Score for Phi-2: 2/10\nmodel_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\nnumber_of_training_examples = 1800\nnumber_of_valid_examples = 200\n \nlora_r = 32\nlora_alpha = 64\nlora_dropout = 0.1\n \nlearning_rate = 5e-5\n1.  returnns a I would suggest to use `return \na` instead of `return ns a` to \nmake the code more concise.1. Spelling mistake in returnns which should be \nreturns.\n2. Missing semicolon at the end of the line. It should be \nreturn a;.\n3. Use lowercase 'r' for return type if it is not a \nreference type, e.g., int returns functionName() \n{...}. If it is a reference type, use uppercase 'R', \ne.g., int R& returnsFunctionName() {...}.\n2.  result = (4 + 2 * 3)) The expression `2 * 3` has a \nhigher precedence than `+`, \nso it gets evaluated first. \nTherefore, `4 + 2 * 3` is \nequivalent to `4 + 6`, which is \n10. To get the result of `12`, \nyou need to use parentheses: \n`(4 + 2) * 3`The expression in the parentheses should be ordered \naccording to the BIDMAS (or PEMDAS) rule: \nMultiplication and division before addition and \nsubtraction.\n3.  if x=5: I would suggest using `==` \ninstead of `=` for comparison. \n`=` is used for assignment.This code snippet contains a syntax error. In Python, \nyou should use \":\" instead of \"=\" after an if statement. \nThe correct format is: if x == 5:# Model Mistral 7b Instruct \nFinetunedMistral 7b Instruct\n1Correct this line: `result = (4 + 2) * 3` \nor `result = (2 * 4 + 3)` depending on \nthe desired outcome.",
        "0097c01d-1392-4175-b915-e2d4d5b80546": "93\n4.  if x == 0 suggestion if x == 0: ```  I \nwould suggest adding a space \nafter the if keyword for \nbetter readability.This code snippet appears to lack necessary context \nand could potentially be a syntax error or an incomplete \ncondition. Here are two potential issues:\n1. The variable 'x' has not been defined before the if \nstatement, causing a NameError. Ensure it is \nproperly initialized.\n2. The condition is missing an else or elif clause, \nmaking the code incomplete and potentially leading \nto unexpected behavior.\n5.  ages = {\n    'pam': 24,\n    'jim': 24\n    'michael': 43\n}suggestion ages = {    'pam': \n24,    'jim': 24,    'michael': 43} \n```  I would suggest adding a \ncomma after the last key-\nvalue pair to ensure that the \ndictionary is properly \nformatted. The dictionary ages has a syntax error. Keys in \ndictionaries should be enclosed in quotes and a comma \nshould separate each key-value pair. Here's the \ncorrected version:\n6.  len('hello') = 5 This is an assignment, not an \nequality comparison. Strings \nhave a built-in length property, \nso you don't need to use the \nlen() function.You cannot assign a value to the length of a string in \nPython. Use len(str) instead, where str is your \nstring variable.\n7.  fro i in range(10): suggestion for i in range(10):  \n[/INST] I would suggest using \nfor i in range(10): instead. This \nis the standard way to iterate \nover a range of numbers in \nPython. It makes the code \neasier to read and \nunderstand. Additionally, it's a \ngood practice to put the for \nkeyword on its own line, like \nthis: for i in range(10): # Your \ncode here This makes the \ncode easier to read and \nunderstand, as well as making \nit easier to add new \nstatements to the loop if \nneeded. So the final code \nwould look like this: for i in \nrange(10): # Your code here I \nhope this helps! Let me know \nif you have any questions. It looks like you've started a for loop without providing a \nvariable name for the iteration. Here are two issues:\n1. Use for i in range(10) instead of just for i.\n2. Ensure you use the correct variable name i within \nyour code block, as it is currently being used for the \nloop variable.\n8.  for i range(10): suggestion for i in range(10):  \n[/INST] I would suggest using It seems you've missed specifying the data type for \nvariable 'i' in your 'for' loop. In Go, it should be 'int'. Also, \n1if x is None or x == 0:  # Properly check \nfor both None and equal to zero\n2    # Your code here\n3else:\n4    # Other logic here\n1ages = {'pam': 24, 'jim': 24, 'michael': \n43}",
        "cc85baf2-f7ca-4e01-a9a1-c45abcb78d51": "94\nConfigurations for Mistral-7B finetuning:\nThe mistral instruct model provided very good answers but we want it to elaborate the review more. So, we decided to test out mistral7b \nchat model but after finetuning, we were facing a couple of errors. They were: \nOSError: Arshik/mistral-chat-model does not appear to have a file named pytorch_model.bin, tf_model.h5, model.ckpt or \nflax_model.msgpack.\nNow, even though the file does exist but its marked with \u201cDetected Pickle Imports\u201c warning.\n suggestion and also adding a \nspace after the for keyword for \nbetter readability. suggestion \nfor i in range(10):  ### Your \nReview of the code: I would \nsuggest using a for loop with \nthe range function and adding \na space after the for keyword \nfor better readability.consider using 'for i := 0; i < 10; i++' instead of \n'range(10)'. Here's a corrected example:\n9.  ages = {\n    'pam': 24,\n    'jim': 24,\n    'michael': 43\n}\nprint(f'Michael is {ages[\"michael]} \nyears old.')suggestion\nages = {    'pam': 24,    'jim': \n24,    'michael': \n43}print(f'Michael is \n{ages[\"michael\"]}')  [/INST] I \nwould suggest removing the \nspace after the comma in the \nprint statement. So it would be \nprint(f'Michael is \n{ages[\"michael\"]}'). This is a \ncommon convention in \nPython.1. Variable name ages is a dictionary with string keys \nand integer values, consider using a list of tuples \ninstead for simplicity and better readability when \nworking with key-value pairs.\n2. Missing a comma before the last key-value pair in \nthe dictionary declaration.\n10.  def foo():\n    return [1, 2, 3\nprint(foo())There is a missing ] at the end \nof the function.Indentation is incorrect in the function definition def \nfoo():    return [1, 2, 3 print(foo()). It \nshould be def foo():  return [1, 2, 3] \nprint(foo()) or def foo():  return [1, 2, 3] \nfor separate prints.\n11.  ages = {'pam'=24} `ages` is a dictionary, so you \nshould use `ages['pam'] = 24`This code creates a dictionary with a single key-value \npair and incorrectly assigns the value to the variable \nnamed 'ages'. To fix this, use curly braces to define a \ndictionary properly:\n Evaluation by Llama3-70b Score for Mistral 7b Instruct \nFinetuned: 8/10Score for Mistral-7b Instruct: 7/10\n1```go\n2for i := 0; i < 10; i++ {\n3    // Your code here\n4}\n5```\n1ages = {\n2    'pam': 24,\n3    'jim': 24,\n4    'michael': 43\n5}\n6print(f'Michael is {ages[\"michael\"]} \nyears old.')\n1ages = {'pam': 24}",
        "49251bf2-fa81-44cf-b5c5-780e7eef496c": "95\nIssue Resolved: Added safe_serialization = True in this.\n \n \nConfigurations for Phi-3 finetuning:\n1model.save_pretrained(output_merged_dir, safe_serialization = True)\n1.  returnns a suggestion: return a 1. Typo in function name: \"returnns\" \nshould be corrected to \"returns\".\n2. Missing return statement for the \nexpected value. Should include \n'return [expected_value];' or proper \nerror handling/logging.\n3. Incomplete code snippet, missing \ncontext of what is being returned \nand why it ends with just an \nunderscore \"_\".The variable name a is not \ndescriptive and does not \nfollow naming conventions. It \nshould be renamed to \nsomething more meaningful. \nThe function is not defined, \nand it's unclear what the \npurpose of this function is. \nThe function is not returning \na value, which will cause a \nsyntax error. The function is \nnot properly formatted, \nmaking it difficult to read and \nunderstand.\n2.  result = (4 + 2 * 3)) I think you should \nuse parentheses \nhere.1. Missing closing parenthesis for \nexpression (4 + 2 * 3).\n2. Consider using parentheses \naround multiplication to avoid \nprecedence issues.\nTo fix this, update the code to: result \n= ((4 + (2 * 3))) or result = (4 1. The code snippet has a \nsyntax error due to an \nunclosed parenthesis. \n2. The correct syntax \nshould be result = (4 \n+ 2 * 3). # Model Input Phi 3 Finetuned Phi 3 Distilled Version using \nLlama3-70b as Teacher \nmodel:",
        "58f64662-cf7f-400b-a299-a01fb6217188": "96\n+ (2 * 3)) 3. The code is not doing \nanything with the result, \nit's just assigning a value \nto a variable. Consider \nadding a print statement \nor returning the result for \nbetter understanding.\n4. The code is not following \nthe PEP 8 style guide for \nPython code. It's \nrecommended to add a \nspace after the equal \nsign. The corrected code \nshould be result = (4 \n+ 2 * 3). \n5. The code is not handling \nany exceptions or errors. \nIt's always a good \npractice to add error \nhandling to ensure the \ncode doesn't break \nunexpectedly.\n6.  The code is not doing \nany meaningful \noperation. It's just \nperforming a simple \narithmetic operation and \nassigning the result to a \nvariable. Consider \nadding a comment to \nexplain the purpose of \nthis operation. \n7. The code is not reusable \nor modular. It's a one-\nliner operation. Consider \nbreaking it down into \nsmaller, reusable \nfunctions. \n8. The code is not following \nthe DRY (Don't Repeat \nYourself)\n3.  if x=5: if x == 5: 1. Issue with indentation after \"if\". \nCorrect format is if x == 5:\n2. Variable names should not be \nequal to Python keywords, use \ncondition_x instead of x.\n3. Missing colon at the end of \nconditional statement (if). Add a \ncolon: if condition_x:The code snippet has a \nsyntax error. The \ncorrect syntax for an \nif statement is `if x \n== 5:`. The provided \ncode is comparing `x` \nto `5` using a single \nequal sign, which is \nnot valid syntax in \nPython.",
        "9e7a3b72-cca3-4cfc-a040-5a7a161fe755": "97\n4.  if x == 0 suggestion if x == 0: 1. Missing colon after condition \ncheck. Correct format should be \nif x == 0:\n2. Consider adding a comment for \nclarity, e.g., # Checks if 'x' is \nzero.The code snippet is a simple \nconditional statement that \nchecks if the variable x is \nequal to 0. It is a valid and \nstraightforward code, but it \nlacks a comment explaining \nthe purpose of the condition. \nAdding a comment would \nimprove the code's \nreadability and \nmaintainability. [/INST]:</s>\n</s></s> The code snippet is \na simple conditional \nstatement that checks if the \nvariable x is equal to 0. It is \na valid and straightforward \ncode, but it lacks a comment \nexplaining the purpose of the \ncondition. Adding a comment \nwould improve the code's \nreadability and \nmaintainability. </s>  \nAdditionally, the code \nsnippet could be improved \nby using a more descriptive \nvariable name instead of x, \nwhich is not self-explanatory. \nFor example, is_zero or \nis_zero_value could be \nused to make the code more \nreadable. </s>  Furthermore, \nthe code snippet could be \nenhanced by considering \nedge cases. \n5.  ages = {\n    'pam': 24,\n    'jim': 24\n    'michael': 43\n}suggestion = {'pam': \n24,    'jim': 24,    \n'michael': 43}Missing comma between 'jim' and \n'michael'. Corrected line should be \nages = {'pam': 24, 'jim': 24, \n'michael': 43}The code snippet has a \nsyntax error. The dictionary \nis missing a comma between \nthe keys 'jim' and 'michael'. \nThe corrected code should \nbe: ages = {'pam': 24, \n'jim': 24, 'michael': \n43} The dictionary keys are \nnot in alphabetical order, \nwhich might make it harder \nto read and maintain. \nConsider sorting the keys \nalphabetically. The variable \nname 'ages' is not \ndescriptive. Consider using a \nmore descriptive name like \n'people_ages' or \n'individual_ages'. The",
        "bfbfb10b-9ba8-4f3b-85b6-dccb37376389": "98\ndictionary values are not \nvalidated. If the values are \nsupposed to be integers, \nconsider adding a validation \ncheck. \n6.  len('hello') = 5 suggestion \nlen('hello') == 51. Syntax error: Replace '=' with '==' \nfor comparison, e.g., \nlen('hello') == 5.\n2. Consider using a variable if this \nvalue is reused to improve \nreadability and maintainability.The code snippet has a \nsyntax error. The \ncorrect syntax for \nassigning a value to a \nvariable is \n`variable_name = \nvalue`. In this case, \nit should be \n`len('hello') = 5` \ncorrected to \n`len('hello') = 5`. \nHowever, this is still \nincorrect as it's \ntrying to assign a \nvalue to the result of \n`len('hello')`, which \nis an integer. The \ncorrect way to assign a \nvalue to a variable is \n`result = \nlen('hello')`.\n7.  fro i in range(10): suggestion: `for i in \nrange(10): Missing colon after for i in \nrange(10). Corrected version: for \ni in range(10):The code snippet has a \nsyntax error. The \ncorrect syntax for a \nfor loop in Python is \n`for i in range(10):`. \nThe provided code \nsnippet is missing the \ncolon (`:`) at the end \nof the `for` statement.\n8.  for i range(10): suggestion for loop: \nfor i in range(10):No closing colon after for loop \ndeclaration. Recommend adding : at \nthe end. Also, consider using \nrange(10) over i in range(10).The syntax is incorrect. It \nshould be for i in \nrange(10): instead of for \ni range(10):. The code \nsnippet is incomplete. It \nlacks an action or operation \nto perform within the loop.\n9.  ages = {\n    'pam': 24,\n    'jim': 24,\n    'michael': 43\n}\nprint(f'Michael is \n{ages[\"michael]} years old.')suggestion: ages = {\n    'pam': 24,\n    'jim': 24,\n    'michael': 43\n}\nprint(f'Michael is \n{ages[\"michael\u201d]} \nyears old.')Use lowercase for dictionary keys: \nages should be {'pam': ...} \ninstead of {'Pam': ...} to ensure \nconsistency in key casing.\nMissing quotes around the string \noutput: Change print statement to \nprint(f'Michael is The code snippet is \nmostly fine, but it has \na potential issue. The \nvariable `ages` is a \ndictionary, and \naccessing its values \nusing `ages['michael']` \nis safe. However, if",
        "9ee7a4f5-dee6-41f5-acc2-385c6a044d64": "99\n \nFine-Tuned Model Responses: Code Input and Model Output:\n \nCodegen Code Reviews:{ages[\"michael\"]} years old.') \nfor proper syntax and readability.the key 'michael' does \nnot exist in the \ndictionary, it will \nraise a `KeyError`. To \navoid this, you could \nuse the `get` method, \nwhich returns `None` if \nthe key is not found.\n10.  def foo():\n    return [1, 2, 3\nprint(foo())suggestion: `def \nfoo():    return [1, 2, \n3]`Missing indentation for function \nprint.The `print` statement \nis not properly \nindented.\n11.  ages = {'pam'=24} suggestion: return a Missing comma after 'pam'. Correct \nsyntax for dictionary is ages = \n{'pam': 24, }.The syntax for assigning a \nvalue to a key in a dictionary \nis incorrect. It should be \nages = {'pam': 24}. 2. \nThe variable name ages is \nnot descriptive. It would be \nbetter to use a more \ndescriptive name like \nperson_ages. 3. The code \nsnippet lacks a function or a \nmain block, making it difficult \nto understand its purpose.\n Evaluation by Llama3-70b Score for Phi-3 \nFinetuned: 7/10Score for Phi-3: 9/10 Score: 6/10\nConfigurations Llama2-7b Phi-2 Mistral-7b \nInstructPhi-3\nNo. of Training Examples 1000 2000 2000 2000\nLora_r 16 16 32 32\nLora_alpha 64 32 64 64\nLora_dropout 0.05 0.01 0.1 0.1\nLearning Rate 1e-4 2e-5 5e-5 5e-5\nEpochs 2 3 3 3 Models",
        "cfa66be1-7ae1-4743-a608-38be1ab16f36": "100\n 1.  returnns a\n2.  result = (4 + 2 * 3))\n3.  if x=5:\n4.  if x == 0\n5.  ages = {\n    'pam': 24,\n    'jim': 24\n    'michael': 43\n}ages = {'pam': 24,    'jim': 24   'michael': 43}\n6.  len('hello') = 5 len('hello') = 5\n7.  fro i in range(10):\n8.  for i range(10):\n9.  ages = {\n    'pam': 24,\n    'jim': 24,\n    'michael': 43\n}\nprint(f'Michael is {ages[\"michael]} \nyears old.')Nothing\n10.  def foo():\n    return [1, 2, 3\nprint(foo())\n11.  ages = {'pam'=24} ages = {'pam'=24} \n \n#Model Codegen2-7B_P\n1def returnns_a():\n2            return a\n1        ```python\n2        result = (4 + 2 * 3)\n3        ```\n4        ### Expected Result:\n5        ```python\n6        result = 7\n1        ```python\n2        if x==5:\n3            print(\"x is 5\")\n4        else:\n5            print(\"x is not 5\")\n6        ```\n1if x == 0:\n2            print(\"x is zero\")\n1for i in range(10):\n2            print(i)\n1for i in range(10):\n2  print(i)\n1def foo():\n2  return [1, 2, 3]\n3print(foo())",
        "c5831a38-2e1a-4216-b9c4-60f5c2592dd1": "Category, LlamaIndex, LangChain, Haystack, Hugging \nFace\nSupported \nLanguages, Python, Python\nTypeScript, JavaScript, Python\nMain Focus, Indexing and, General-\nefficient retrieval                                          \napplications\n RAG approach\ncontext-aware \napps, Semantic \nsearch and, Pre-trained \nmodels, transfer \nQ&A systems                                                              \nwide range of \nNLP tasks\nKey Features, Integration with \ndiverse data \nsources (160+)\nlist index\ncustomizable \nworkflows, Modular design\nflexible \nintegration\ndevelopment\ndeployment\nobservability\naccess to \nvarious LLMs, Vendor \nneutrality\ninteroperability\ncustomizability\nextensibility \nspecialized \ncomponents, Pre-trained \nmodels\nextensive \ndocumentation\nlarge \ncommunity easy \ntransfer learning\ncustomization\nPros, Handles wide \nrange of data \nformats\nAdvanced \nindexing \ntechniques\nCustomizable \nRAG process, Modular \narchitecture \nExtensive \necosyste\nAccess to \nvarious LLM \nproviders\n Ease of use, Designed for \nsearch and \nQ&A\nSpecialized \ncomponents\nIntegrated \nworkflow, Wide range of \nstate-of-the-art \nmodel\nUser-friendly \nAPI\nLarge \ncommunity\nCustomizable",
        "2ff343cb-b580-42f2-8d80-d2c1f8112175": "Cons, Limited to data \nretrieval, less \nsuitable for \ncomplex NLP \ntasks, Generalized, \nnot specialized- \nPerformance \nmay vary based \non use case, Narrower scope\n Primarily \nfocused on \ndocument \nunderstanding \nand retrieval \ntasks, Not specifically \nfocused on \nsearch or \nretrieval \nsystems\nUse Cases, Q&A chatbots \nDocument \nunderstanding \nand extraction \nAutonomous \nagents, Prototyping and \nexperimentation \nEnterprise-level \napplications, Enterprise \nsearch and QA \nSpecialized \nmanagement, Text \nclassification\n Question \nanswering \nLanguage \ntranslation\nVendor \nIntegration, Customizable with \ndifferent retrieval \nstrategies, Integrates with \nmultiple LLM \nproviders like \nOpenAI, Cohere, \nHugging Face Hub, Integration with \nvarious model \nproviders and \ndatabases, Provides a wide \nrange of pre-\ntrained models \nproviders\nCommunity and \nEcosystem, Growing \ncommunity, \nlightweight \ninterface, Large and active \ncommunity, well-\ndocumented \nexamples, Well-documented \nAPI, growing, Extensive \ncommunity and \ncommunity                                                       \nuser base\nPerformance \nOptimization, Focused on \nefficient indexing \nand retrieval, Flexible but may \nnot match \nperformance of \ntailored solutions \nfor specific tasks, Optimized for \nsearch and Q&A, \nbetter performance \nin semantic search, Optimized for \nvarious NLP tasks, \ntransfer learning \ncapabilities\nExample \nLibraries/Tools, , LangSmith,, LangServe, Transformers",
        "1a9d05a6-048b-4915-8df7-f9591b5dcd07": "Feature, LlamaIndex, LangChain, Haystack, Hugging Face\nQuery \nOptimisation, , , , \na) Multi-query \nexpansion, \u2705, \u2705, \u274c, \u274c\nb) Sub-query \nexpansion, \u2705, \u2705, \u274c, \u274c\nc) Hyde-adapter for \nembeddings, \u2705, \u2705, \u2705, \u274c\nd) Rewrite queries, , \u2705, \u274c, \u2705\nDocuments \nOptimisation, , , ,",
        "22581b85-8bb0-446e-a936-641862133fe7": "a) Chunking \nstrategies, \u2705, \u2705, \u274c, \u274c\nb) Metadata \nattachment, \u2705, \u2705, \u274c, \u274c\nc) Hierarchical \nindex structure, \u2705, \u274c, \u274c, \u274c\nd) Knowledge \ngraph index, \u2705, \u274c, \u2705, \u274c",
        "6fbc911e-cfcf-4130-8b1e-708853cfad05": "Embeddings \nModel, , , , \na) Ollama, \u2705, \u2705, \u2705, \u2705\nb) Cohere, \u2705, \u2705, \u2705, \u2705\nc) Jina                                   \u2705, , \u2705, \u2705, \u2705\nd) Voyage, \u2705, \u2705, \u2705, \u2705\ne) OpenAI, \u2705, \u2705, \u2705, \u2705\nf) LLM Embedder, , \u274c, \u274c, \u2705\ng) BGE \nEmbeddings, \u274c, \u274c, \u274c, \u2705",
        "baecaad8-be3b-4703-af47-a7848d3d7334": "Context \nExpansion, , , , \na) Sentence \nwindow retrieval, \u2705, \u274c, \u274c, \u274c\nb) Recursive \nretrieval, \u2705, \u274c, \u274c, \u274c\nc) Auto merging, , \u274c, \u274c, \u274c\nContext \nRerankers, , , , \na) Cohere reranker, , \u2705, \u2705, \u274c\nb) BGE reranker, , \u274c, \u2705, \u2705\nc) Rank fusion, , \u2705, \u2705, \nd) ColBERT \nreranker, \u2705, \u2705, \u2705, \u2705\ne) Long context \nreorder, \u2705, \u2705, \u274c, \u274c\nf) Jina reranker, , \u2705, \u2705, \u2705",
        "c96216a2-5ff6-4bec-97f9-a9ce908b9fe8": "Answer \nSimilarity, Baseline \nExperime\nnt, Multiquer\ny, Subquery, HYDE, Multi step, Step back, Query \nRewriting\nUptrain, 25%, 28%, 15%, 48%, 21%, 27%, 39%\nTonic \nValidate, 44%, 44%, 30%, 64%, 48%, 44%, 50%",
        "a023cf8f-693b-45e8-9ca4-9333ce3c7f1f": "Context \nPrecision, Baseline \nExperime\nnt, Multiquer\ny, Subquery, HYDE, Multi\nstep, Step back, Query \nRewriting\nUptrain, 45%, 51%, 22%, 100%, 44%, 43%, 49%\nTonic \nValidate, 40%, 64%, 29%, 80%, 65%, 63%, 53%",
        "016179fd-452d-43c6-9631-04d1a2bdd16b": "Context \nsimilarity, Multiquery, Subqueries, HYDE, Multi step, Step back \nprompting\nRagas, 31%, 82%, 91%, 91%, 90%\nTonic\nValidate, 1.5/5, 3.2/5, 4.4/5, 1.2/5, 1/5\nDeepEval, 10%, 40%, 85%, 10%, 45%\nUpTrain, 14%, 34%, 57%, 23%, 21%\nTrulens, 41%, 56%, 64%, 76%, 76%\nFalcon-\nevaluate, 25%, 50%, 96%, 40%, 33%",
        "604b12ec-70c7-40b3-9aca-12d243feab63": "Context \nPrecision, Baseline \nExperiment, Semantic \nChunking, Tik token \nChunking, Character text \nChunking\nUp train, 45%, 67%, 69%, 81%\nTonic Validate, 40%, 74%, 70%, 74%",
        "f5ba165a-6ee3-4d0e-ab6a-9e7017163ebc": "Answer Similarity, Baseline Experiment, Metadata\nAttachment\nUp train, 25%, 52%\nTonic Validate, 44%, 56%",
        "48125e37-e167-4423-b687-fecb8c8e9535": "Context Precision, Baseline Experiment, Metadata\nAttachment\nUp train, 45%, 73%\nTonic Validate, 40%, 75%",
        "d2175c4b-fc97-4ee2-9d49-5222cd5c19f2": "Answer Similarity, Baseline Experiment, Hierarchical Indexing\nUp train, 25%, 56%\nTonic Validate, 44%, 68%",
        "3dad7789-ccea-4704-90ad-e0f16a0e9a22": "Context Precision, Baseline Experiment, Hierarchical Indexing\nUp train, 45%, 80%\nTonic Validate, 40%, 75%",
        "0203af0a-3fcd-46b1-8a32-6b62907594a0": "Context \nsimilarity, Recursive \nChunking, Meta Data \nInclusion, Semantic \nChunking, Character \nText \nsplitting, Tik Token \nBased \nSplitting, Hierarchical \nIndexing",
        "4bfd1c59-5f32-45b3-b71d-78a7b51d01cc": "Ragas, 80%, 91%, 77%, 88%, 71.08%, 82%\nTonic \nValidate, 2/5, 1.9/5, 1/5, 2.1/5, 0.9/5, 4.4/5\nDeep eval, 35%, 30%, 49%, 50%, 40%, 54%\nUp train, 26%, 26%, 22%, 40.8%, 16%, 67%\nTrulens, 70%, 72%, 74%, 75.5%, 76%, 76%\nFalcon-\nevaluate, 50%, 46%, 45%, 59%, 59%, 96%",
        "9752f056-166e-4e08-8d80-2b82eba859a3": "Answer \nSimilarit\ny, Baseline \nExperim\nent \n(ollama), OpenAI, JinaAI, BGE3, Cohere, Voyage\nAI, LLM \nembedd\ner, Bedrock\nUptrain, 25%, 36%, 38%, 50%, 37%, 34%, 28%, 24%\nTonic \nValidate, 44%, 60%, 54%, 60%, 53%, 53%, 48%, 50%",
        "2607a0eb-f9e0-43e5-bf64-e7d38c8c9666": "Context \nPrecisio\nn, Baseline \nExperim\nent \n(ollama), OpenAI, JinaAI, BGE3, Cohere, Voyage\nAI, LLM \nembedd\ner, Bedrock\nUp train, 45%, 88%, 54%, 86%, 52%, 66%, 52%, 45%\nTonic \nValidate, 40%, 76%, 69%, 79%, 74%, 67%, 55%, 61%",
        "bcdfb757-defe-491b-91ba-6bf5f62ecbe3": "Context \nSimilarity, Openai, Bge - m3, llm-\nembedd\ner, Cohere, all-\nminillm, Mxbai- \nembed \nlarge, Voyage, JIna Ai\nRagas, 91%, 82.5%, 80, 77%, 78%, 79%, 75%, 76%\nTonic \nValidate, 1.9/5, 3.4/5, 3/5, 1.1/5, 2.5%, 3.1%, 2.8/5, 2.9/5\nDeepeval, 40%, 33%, 38, 44%, 36%, 34%, 39%, 37%\nUptrain, 33%, 33%, 35, 24%, 30%, 32%, 31%, 32%\nTrulens, 75%, 76.22%, 76.22%, 76%, 76.22%, 76.22%, 76.22%, 76.22%\nFalcon-\nevaluate, 55%, 46%, 48, 33%, 50%, 47%, 52%, 49%",
        "f1a067f1-5504-47d9-969e-2b53ab00bed2": "Technique, Description, RAG use case\nSentence Window Retrieval, Segmenting the corpus into \noverlapping sentence windows \nand scoring based on query \nrelevance.\nScoring each window based on its \nrelevance to the input query.\nProviding highest scoring windows \nas context, Suitable for use cases where \ndetailed context from specific \nsegments of text is necessary,\nquestion answering \nsummarization.\nAuto Merging Retrieval, Extracting relevant passages from \nmultiple sources and combining \nthem into a cohesive context.\nAutomatically combining \npassages into a single context.\nFeeding the merged context, Ideal for scenarios where \ninformation is scattered across \ndifferent sources, like\ncreating comprehensive \nreports \nsynthesis from diverse \ndocuments.\nRecursive Retrieval, Iteratively refining the retrieval \nprocess to improve relevance and \naccuracy.\nUsing retrieved information to \nrefine the input query or context.\nIterative Retrieval: Repeating the \nretrieval process with refined \nqueries or contexts., Best suited for complex queries \nwhere initial retrievals may need \nfurther refinement, such as \nmulti-turn dialogue systems\n iterative problem-solving tasks\nPdfs with tabular data",
        "6e35ac77-1eb6-4294-a84a-31200889ef7a": "Answer Similarity, Baseline \nExperiment, Sentence \nWindow, Auto Merging, Recursive\nUp train, 25%, 52%, 54%, 35%\nTonic Validate, 44%, 67%, 68%, 39%",
        "46c4dc16-e988-4290-a952-1c988ed29dde": "Context \nRelevance, Baseline \nExperiment, Sentence \nWindow, Auto Merging, Recursive\nUp train, 45%, 69%, 77%, 33%\nTonic Validate, 40%, 74%, 73%, 55%",
        "70252ddc-d5c2-4248-994c-939e45107f39": "Context similarity, Sentence window, Auto Merging, Recursive\nRagas, 92%, 96%, -\nTonic\nValidate, 4/5, 4/5, -\nDeep Eval, 46%, 40%, -\nUp Train, 71%, 65%, -\nTru lens, 76.25%, 76%, -\nFalcon-evaluate, 96, 100%, -",
        "f46ab9a8-951b-4a4f-a61d-1f5db7704a25": "Answer \nSimilarit\ny, Baseline \nExperim\nent, Colbert, Cohere, JIna, BGE, Rag\nFusion, Rank \nGpt, Long \ncontext \nreorder\nUp train, 25%, 47%, 49%, 48%, 33%, 42%, 49%, 39%\nTonic \nValidate, 44%, 63%, 54%, 61%, 51%, 56%, 60%, 49%",
        "a51e9ad1-305e-4d71-8a2b-623201b297a1": "Answer \nSimilarit\ny, Baseline \nExperim\nent, Colbert, Cohere, JIna, BGE, Rag \nFusion, Rank \nGpt, Long \ncontext \nreorder\nUptrain, 45%, 76%, 67%, 82%, 43%, 82%, 78%, 63%\nTonic \nValidate, 40%, 83%, 77%, 88%, 66%, 75%, 87%, 67%",
        "cd14f32f-d92d-424e-9cb0-02c3a8698ec9": "Context \nRelevance, Colbert \nreranker, Cohere \nreranker, Rag-\nfusion, RankGPT, Bge \nranker, Long \ncontext \nreorder, JIna Ai \nreranker\nRagas, 82%, 90%, 87%, 88%, 82.5%, 93.0%, 93%\nTonic \nValidate, 4.5/5, 3.2/5, 3.6/5, 3.8/5, 3.4/5, 4/5, 3.8/5\nDeepeval, 70%, 40%, 68%, 50%, 33%, 45.8%, 63%\nUptrain, 70%, 39%, 51%, 53%, 33%, 45.2%, 48%\nTrulens, 75.68%, 75.68%, 75.6%, 76.25%, 76.22%, 76.22%, 76.01%\nFalcon-\nevaluate, 96%, 56%, 42%, 57%, 55%, 42%, 42%",
        "1fcdd61c-7856-402b-8520-7b9501235995": "Re-ranker, CPU Avg Score, GPU Avg  Score\nColbert, 44s, 60s\nCohere, 61s, 120s\nRag Fusion, 80s, 60s",
        "4c6b6f8b-99bc-4d36-9b9b-f0d917164f73": "Rank GPT, 300s, 180s\nJina AI, 47s, 60s\nBGE, -, 9s\nLong Context Reorder, 58s, 180s",
        "0b7fd46c-b6cb-46c4-8807-048444b6384a": "Task, Evaluation parameters\nContent moderation, Recall and precision on toxicity and bias\nQuery generation, Correct output syntax and attributes, extracts the \nright information upon execution\nDialogue (chatbots, summarization, Q&A), Faithfulness, relevance",
        "a227e1c6-af09-46d3-8053-1b223d4f879c": "Task, Evaluation parameters\nContent moderation, Recall and precision on toxicity and bias\nQuery generation, Correct output syntax and attributes, extracts the right information \nupon execution\nDialogue (chatbots, summarization, Q&A), Faithfulness, relevance",
        "f19abf20-e6e1-4274-989a-deebb77fb2f1": "Feature, Code Rabbit, Maverick, CodiumAI PR-Agent\nContext-Aware Review \nFeedback, , , \nConversational Code \nReview, , , (Through Question Answering)\nContinuous, Incremental \nReviews, , , (Uses commands for review and \ndescribe)\nActionable Suggestions \n(with commit), , , \nIntegration with GitHub & \nGitLab, , (GitHub only), (GitHub, GitLab, Bitbucket)\nLearning Capability, , Not Mentioned, \nPR Summary & \nWalkthrough, , , \nIssue Validation (GitHub, \nGitLab, Jira), (GitHub, GitLab, Jira), , (GitHub, GitLab, Jira)\nFree Tier, (Limited features), (Full features), (Core features)\nPaid Tier, (Pro, Enterprise), , (Pro with additional features)\nDocumentation \nGeneration, , , (Pro)\nTest Generation, , , (Pro)\nSimilar Code Search, , , (Pro)\nInline File Summary, , , (Pro)",
        "e03555fc-8f7c-4402-b662-6642bdeb5b75": "Model, Method, GPU (Memory Usage)\nRoBERTa, FT, 5.38\nRoBERTa, Adapter, 15.29\nRoBERTa, Prompt Tuning, 3.84\nRoBERTa, Prefix Tuning, 3.56\nRoBERTa, IA3, 2.62\nRoBERTa, BitFit, 3.27\nRoBERTa, ChildTuning, 6.02\nRoBERTa, LoRA, 3.59\nRoBERTa, AdaLoRA, 3.57\nRoBERTa, MAM Adapter, 15.35\nRoBERTa, ProPELT(adapter), 8.63\nRoBERTa, ProPELT(lora), 8.25\nRoBERTa, ProPELT(prefix), 9.47\nT5, FT, 25.17\nT5, IA3, 21.36\nT5, LoRA, 19.43\nLlama-7B-Alpaca, FT, 169.36\nLlama-7B-Alpaca, IA3, 128.57\nLlama-7B-Alpaca, LoRA, 124.82\nLlama-7B-Alpaca, QLoRA, 56.46",
        "10215593-bdcb-4e26-9926-f7c5454f4516": "T5, Compactor, 24\nRoBERTaLARGE, HIWI, 48\nLLaMA-7B, DoRA, 28.2\nVL-BART, DoRA, 20.5\nLLaMA 7B, VeRA, 21.69\nRoBERTAa, DyLoRA, 32\nRoBERTa-large, SORA, 24.268\nLlama2 7B, Laplace-Lora, 8.43\nCLIP, FLAN-T5, L-LoRA, 24\nRoBERTA-base, MoA, 80\nRoBERTA-base, Kernel-mix-lite(qv)/(qvo), 5.38\nLLAMA-2-7b, LOFTQ, 15\nLLAMA-2-7B, QLoRA, 48\nSupernet, QA-LoRA, 32\n, ,",
        "7695be93-d6e3-432c-b74b-7e955b7f09b9": "#, Model Input, Llama2-7b Finetuned, Llama2-7b, Distilled Version using \nLlama3-70b as Teacher \nmodel:\n1., returnns a, Return statement missing a colon \nreturn a  should be  return a;, Return statement without a value\nThe  return  statement without a \nvalue is not allowed in JavaScript. \nYou need to assign a value to the \nreturn  statement, like this:  return \na;, The code snippet \nappears to be a \nfunction definition. \nThe function returns \nan array of objects. \nThe code looks clean \nand well-structured. \nHowever, there are no \nissues with this \nsnippet.\n2., result = (4 + 2 * \n3)), The line  result = (4 + 2 * 3)  can be \nsimplified to  result = 4 + 6  directly, \nwithout the need for multiplication. \nRemove the multiplication operation and \nkeep the code concise., Unnecessary multiplication operator \nused\nThe multiplication operator is not \nneeded here, as the addition \noperator already performs \nmultiplication. The correct code \nwould be  result = (4 + 2 * 3), The code snippet \nappears to be a \ncalculation of a \nresult. The \ncalculation is simple \nand straightforward, \nbut it does not seem \nto be part of a \nlarger program or \nfunction. The code is \nwell-formatted and \neasy to read. \nHowever, without more \ncontext, it's unclear \nwhat the purpose of \nthis calculation is \nor how it's being \nused.",
        "cc2cee92-b656-4562-874d-82cf473566fc": "3., if x=5:, Thank you for submitting this pull \nrequest! However, I have a few issues \nwith the code that need to be addressed:\nInconsistent indentation: Please \nensure that your indentation is \nconsistent throughout the code. \nCurrently, there are some lines that \nare not indented properly.\nMissing docstring: A documentation \nstring should be included for any \nfunction or class defined in the code. \nCan you please add a docstring to \nif x=5 ?\nUnused variable: There is a variable \nx  defined but it's not being used \nanywhere in the code. Please \nremove it or use it properly., The  if  statement is not properly \nindented. It should be indented 4 \nspaces to conform to PEP8 style \nguidelines.\nExplanation: In Python, it's important \nto follow the PEP 8 style guide for \ncode formatting to make the code \neasier to read and understand. The \nif  statement should be indented 4 \nspaces to indicate that it's a block of \ncode., Code snippet has a \nsyntax error. The \n`if` statement is not \nproperly formatted. \nIt should be `if x == \n5:`, not `if x=5:`.\n4., if x == 0, The  if  statement is not properly \nformatted. It should be on a new line and \nindented.\nCorrected version:\n1\nif x == 0 {\n2\n   // code here\n3\n}, The  if  statement is not properly \nformatted. It should be on a new line \nand indented.\nCorrected version:\n1\nif x == 0 {\n2\n   // code here\n3\n}, if x == 0: \n      return Review: \nThe code snippet is a \nPython if statement that \nchecks if the variable x is \nequal to 0. If it is, the \nreturn statement is \nexecuted. The code is \nsimple and easy to \nunderstand. However, \nthere is no error handling \nor logging in this snippet, \nwhich could lead to \nissues if x is not equal to \n0.\n5., ages = {\n    'pam': 24,\n    'jim': 24\n    'michael': 43\n}, The  ages  dictionary is defined with key-\nvalue pairs in separate lines, which can \nlead to readability and maintainability \nissues. It's better to define the dictionary \nwith key-value pairs on a single line for \nbetter readability. Here's an example of \nhow you could rewrite the code:\n1\nages = {'pam': 24, 'jim': 24, \n'michael': 43}\nExplanation: By defining the dictionary \non a single line, it's easier to read and \nunderstand the code. It also makes the \ncode more concise and easier to \nmaintain., The  ages  dictionary keys are not in \nlowercase. It's a good practice to \nuse lowercase for variable names to \nmake the code more readable and \navoid conflicts with future updates. \nPlease change the keys of the  ages  \ndictionary to be in lowercase., The code snippet is a \ndictionary with three \nkeys and values. The \nkeys are 'pam', \n'jim', and 'michael' \nand the values are \n24, 24, and 43 \nrespectively. The \ncode looks clean and \nwell-formatted. \nHowever, it would be \nbetter to include a \ncomment explaining \nthe purpose of the \ndictionary and what \nit is used for.",
        "24b95fad-415a-44db-8f9a-4cfa60f84ef2": "6., len('hello') = 5, The line  len('hello')  assigns a \nlength of 5 to a string that is empty. This \nis incorrect and can lead to unexpected \nresults. Please fix the issue by providing \na valid string or using an empty string \nliteral., Changed code is not enclosed in a \nstring literal\nExplanation: The  len()  function \nshould always be called on a string \nliteral to ensure that the length is \ncalculated correctly. Enclosing the \nstring 'hello' in a string literal \n('\"hello\") would fix this issue., Code has a syntax error. \nThe code is not properly \nformatted. It is missing a \nclosing quote for the \nstring literal. Additionally, \nthe code does not seem \nto be doing anything \nuseful. It is just printing \nthe length of a string. If \nthis is intended, the code \nshould be improved to \nprovide more context and \nexplanation. Otherwise, it \nshould be removed or \nmodified to perform a \nuseful task.\n7., fro i in \nrange(10):, The  range  function is used without an \nexplicit end index. This can lead to \nunexpected behavior and potential bugs. \nPlease provide an end index for the \nrange, e.g.,  for i in range(10): \n... ., The  range(10)  loop is not inside a \nwith  statement or a  try - except  \nblock, which means that the \nresources used by the loop are not \nproperly cleaned up when the \nprogram exits. This can lead to \nmemory leaks and other issues. \nPlease fix this by either wrapping the \nloop in a  with  statement or adding \na  finally  block to ensure proper \nresource cleanup., Code snippet appears \nto be a Python code \nsnippet, but it is \nincomplete and lacks \ncontext. Without more \ninformation, it's \ndifficult to provide \na comprehensive \nreview. However, it \nseems to be a loop \niterating over a \nrange of numbers, \nwhich is a common \nPython construct.\n8., for i range(10):, The  range()  function is used to \ngenerate a sequence of numbers, but in \nthis case, it is not necessary since the \nfor  loop already defines a sequence of \n10 numbers. Removing the  range()  \ncall will simplify the code and make it \nmore readable., The  range  function is not defined in \nthis context. Define it before using it.\nExplanation: The  range  function is \na built-in Python function that \ngenerates a sequence of numbers. \nIn this code, it is being used without \nbeing defined, which can cause an \nerror. To fix the issue, define the \nrange  function before using it. For \nexample:  from typing import \nrange; for i in range(10): ..., 1. The code snippet is a \nPython loop, but the \nvariable  i  is not used \nanywhere in the loop. \nThis might be \nintentional, but it \nshould be checked. \n2. The loop is iterating \nover a range of 10, \nbut the purpose of the \nloop is not clear. It \nmight be a debugging \nloop or a test loop, but \nit should be more \ndescriptive. \n3. The code is well-\nformatted and easy to \nread.\n9., ages = {\n    'pam': 24,, The code prints the age of \"Michael\" \nwithout any input validation. This could, The code uses a dictionary to store \nages, but the key  michael  is not, The code snippet is \nmostly correct, but there",
        "e703e753-e4d7-4bd9-af1e-d2bb980ef3b1": ", } p\n    'jim': 24,\n    'michael': 43\nrint(f'Michael \nis \n{ages[\"michael\n]} years old.'), lead to unexpected results if the user \nenters invalid data. Please validate user \ninput before processing it., defined in the dictionary.\nPlease define the key or fix the \nissue., are a few issues. \n1. The variable  ages  is a \ndictionary, but it's not \nbeing used anywhere in \nthe code. \n2. The print statement is \nnot using the  ages  \ndictionary, it's just printing \na string. \n3. The code is not doing \nanything with the age of \nMichael, it's just printing \nit. \n4. There's no error \nhandling or logging in \ncase of an exception.\n10., def foo():\n    return [1, 2, \n3\nprint(foo()), The  print(foo())  statement is not \nnecessary and can be removed to \nsimplify the code. It does not affect the \nfunction's behavior or output., Issue: The code is printing a value \ninstead of returning it.\nChange  print(foo()  to  return \nfoo()  to fix this issue., 1. The code snippet is a \nsimple function \ndefinition that returns \na list and prints its \ncontents.\n2.  The code is well-\nformatted and easy to \nread.\n3. There are no syntax \nerrors or logical \nissues. \n4. The code is clean and \nefficient.\n11., ages = \n{'pam'=24}, Change the  ages  dictionary to use \nquoted strings, like this:  ages = \n{'pam': 24}, Using a dictionary with a single key-\nvalue pair is not recommended as it \ncan lead to confusion and errors in \nyour code. It's better to use a list or \nan object instead.\nReason: Dictionaries are meant to \nstore multiple key-value pairs, and \nusing one with only one key-value \npair can cause issues when you \nneed to access or modify the value. \nAdditionally, it can make your code \nharder to read and maintain.\nSuggestion: Instead of using a \ndictionary, consider using a list or an \nobject that stores multiple key-value \npairs. This will make your code more \nflexible and easier to work with in the \nlong run., Code looks clean and \nwell-formatted. The \nvariable names are \ndescriptive and \nfollow a consistent \nnaming convention. \nThe data dictionary \nis properly defined \nwith a key-value \npair. No syntax \nerrors or logical \nissues found.",
        "2fd92583-d569-49b6-adab-85f0c6a0013f": "#, Model Input, Phi-2 Finetuned, Phi-2\n1., returnns a, This is a bit of nitpick but i think it would \nbe better to use \u2018return' instead of \n'returnns\u2019, The code you have provided has several \nissues that need to be addressed before it \ncan be considered high quality. Here are \nsome suggestions for improving your code:\n1. Use more descriptive variable names: \nnumbers  and  average  are not very \ndescriptive, so consider renaming them \nto something like  number_list  and \ncalculate_mean , respectively.\n2. Check the types of variables you're \nusing: You're using a mix of integers and \nfloats in your code, which can cause \nissues when performing calculations. \nConsider converting all the numbers to \nfloat type before doing any math \noperations.\n3. Use the correct syntax for calculating \nthe mean: The current implementation \nuses  numbers[i]/len(numbers) , but \nthis will give you a floating-point value if \nthe input list is empty. You should use \nsum(numbers)/len(numbers)  to avoid \nthis issue.",
        "65d7c9a7-94c2-4462-8162-dee932b8a5cc": ", , , 3 4\n1\n2\n5\n6\n7\n8\n9\n10\n11, 4. Add comments to your code to make it \nmore understandable: Here's an \nexample of how you could improve your \ncode with better commenting:\n# Input a list of numbers\nnumber_list = [1, 2, 3, 4, 5]\n# Check if the list is empty\nif len(number_list) == 0:\n    return \"Error: Empty list\"\nelse:\n    # Calculate the mean by \nsumming up all the numbers and \ndividing by the length of the \nlist\n    mean = \nsum(number_list)/len(number_li\nst)\n    # Return the result as a \nstring with two decimal places\n    return \"\n{:.2f}\".format(mean)\n, , , , \n1., result = (4 + 2 * 3)), This is a syntax error. You need to close \nthe parenthesis., Nothing, \n1., if x=5:, This is a syntax error. You should use `==` \ninstead of `=`., I can review a pull request and provide \nfeedback on code quality and good \npractices. here's an example of what my \ncomments might look like:\n   if x == 5:\n1\n# this is better practice to \nuse f-strings instead of string \nconcatenation\n2\nprint(f'{x} is equal to 5'), \n1., if x == 0, This is a good practice to use `isinstance` \nto check if the input is an integer., I am happy to help you with that. However, \nit seems like there might be an issue in \nyour code as well. Can you please provide \nsome more information about what the pull \nrequest is for and where exactly this part of \nthe code is located? This will help me \nprovide better assistance to you., \n1., ages = {\n    'pam': 24,\n    'jim': 24\n    'michael': 43\n}, ages = { 'pam': 24, 'jim': 24, 'michael': 43 }, Sure thing! I'm happy to help with that. \nBased on the information you've provided, it \nlooks like there may be an issue with the \nvariable  ages  being assigned a dictionary \nrather than individual values for each \nperson.\nIn your code, you're trying to create a \ndictionary of ages using a set of key-value \npairs. However, this approach won't work \nbecause you're trying to assign multiple,",
        "954b7c95-b23b-47d0-9f1b-cea867748f36": ", , , 4 5\nvalues to one variable, which is not allowed \nin Python. Instead, you can simply initialize \nthe  ages  variable with individual age \nvalues for each person as shown below:\n1\npam_age = 24\n2\njim_age = 24\n3\nmichael_age = 43\nages = {'pam': pam_age, 'jim': \njim_age, 'michael': \nmichael_age}\nThis should create a dictionary with the \ncorrect values for each person's age. I hope \nthat helps! Let me know if you have any \nother questions.\n1., len('hello') = 5, return \"This code is not functional. The \nassignment statement `len('hello') = 5` \ndoes not modify the length of the string \n'hello'. It is possible that the code is \nintended to assign the length of the string \nto a variable, but this is not the case here. \nThe correct syntax for assigning the length \nof a string to a variable is `len('hello') = 5`., Nothing\n1., fro i in range(10):, The code is correct and efficient. No \nissues found., I have reviewed your pull request and found \nthat there is a line of code here:  for i in \nrange(10): . This is a good start, but it \nwould be more helpful to give some context \nabout what this code does. Could you \nplease clarify the purpose of the  range  \nfunction in this case?\n1., for i range(10):, for i in range(10): print(i), Thank you for submitting a pull request. I \nwill review it carefully and provide feedback \nas soon as possible. However, at this time, \nI am unable to comment on your changes \ndue to technical constraints. If you have \nany other questions or concerns, please \nfeel free to ask.\n1., } p\nages = {\n    'pam': 24,\n    'jim': 24,\n    'michael': 43\nrint(f'Michael is {ages[\"michael]} \nyears old.'), `None`., AI\n10., def foo():\n    return [1, 2, 3\nprint(foo()), foo() should be foo() without the closing \nparenthesis., Sure, I can help you review this pull \nrequest. Can you please provide me with \nsome more context about the code in \nquestion? What are the changes that have \nbeen made and what is their purpose? This",
        "5392635d-e24e-438b-aa22-993170f70ced": ", , , will help me to provide a more detailed \nfeedback on the quality of the code and \ngood practices that should be followed.\n11., ages = {'pam'=24}, suggestion ages = {'pam': 24}, Nothing\n, Evaluation by Llama3-70b, Score for Phi-2 Finetuned: 6/10, Score for Phi-2: 2/10",
        "3143c3e5-4051-471e-8760-87f5c572ba14": "#, Model, Mistral 7b Instruct \nFinetuned, Mistral 7b Instruct\n1., returnns a, I would suggest to use `return \na` instead of `return ns a` to \nmake the code more concise., 1. Spelling mistake in  returnns  which should be \nreturns .\n2. Missing semicolon at the end of the line. It should be \nreturn a; .\n3. Use lowercase 'r' for return type if it is not a \nreference type, e.g.,  int returns functionName() \n{...} . If it is a reference type, use uppercase 'R', \ne.g.,  int R& returnsFunctionName() {...} .\n2., result = (4 + 2 * 3)), The expression `2 * 3` has a \nhigher precedence than `+`, \nso it gets evaluated first. \nTherefore, `4 + 2 * 3` is \nequivalent to `4 + 6`, which is \n10. To get the result of `12`, \nyou need to use parentheses: \n`(4 + 2) * 3`, The expression in the parentheses should be ordered \naccording to the BIDMAS (or PEMDAS) rule: \nMultiplication and division before addition and \nsubtraction.\n1\nCorrect this line: `result = (4 + 2) * 3` \nor `result = (2 * 4 + 3)` depending on \nthe desired outcome.\n3., if x=5:, I would suggest using `==` \ninstead of `=` for comparison. \n`=` is used for assignment., This code snippet contains a syntax error. In Python, \nyou should use \":\" instead of \"=\" after an if statement. \nThe correct format is:  if x == 5:",
        "bc091829-e274-406b-8473-289de6866e77": "4., if x == 0, suggestion if x == 0: ```  I \nwould suggest adding a space \nafter the  if  keyword for \nbetter readability., condition. Here are two potential issues:\n1\n2\n3\n4, This code snippet appears to lack necessary context \nand could potentially be a syntax error or an incomplete \n1. The variable 'x' has not been defined before the if \nstatement, causing a NameError. Ensure it is \nproperly initialized.\n2. The condition is missing an else or elif clause, \nmaking the code incomplete and potentially leading \nto unexpected behavior.\nif x is None or x == 0:  # Properly check \nfor both None and equal to zero\n    # Your code here\nelse:\n    # Other logic here\n, , , , \n5., ages = {\n    'pam': 24,\n    'jim': 24\n    'michael': 43\n}, suggestion ages = {    'pam': \n24,    'jim': 24,    'michael': 43} \n```  I would suggest adding a \ncomma after the last key-\nvalue pair to ensure that the \ndictionary is properly \nformatted., The dictionary  ages  has a syntax error. Keys in \ndictionaries should be enclosed in quotes and a comma \nshould separate each key-value pair. Here's the \ncorrected version:\n1\nages = {'pam': 24, 'jim': 24, 'michael': \n43}, \n6., len('hello') = 5, This is an assignment, not an \nequality comparison. Strings \nhave a built-in length property, \nso you don't need to use the \nlen() function., You cannot assign a value to the length of a string in \nPython. Use  len(str)  instead, where  str  is your \nstring variable., \n7., fro i in range(10):, suggestion for i in range(10):  \n[/INST] I would suggest using \nfor i in range(10): instead. This \nis the standard way to iterate \nover a range of numbers in \nPython. It makes the code \neasier to read and \nunderstand. Additionally, it's a \ngood practice to put the for \nkeyword on its own line, like \nthis: for i in range(10): # Your \ncode here This makes the \ncode easier to read and \nunderstand, as well as making \nit easier to add new \nstatements to the loop if \nneeded. So the final code \nwould look like this: for i in \nrange(10): # Your code here I \nhope this helps! Let me know \nif you have any questions., It looks like you've started a for loop without providing a \nvariable name for the iteration. Here are two issues:\n1. Use  for i in range(10)  instead of just  for i .\n2. Ensure you use the correct variable name  i  within \nyour code block, as it is currently being used for the \nloop variable., \n8., for i range(10):, suggestion for i in range(10):  \n[/INST] I would suggest using, It seems you've missed specifying the data type for \nvariable 'i' in your 'for' loop. In Go, it should be 'int'. Also,,",
        "552bac82-fb38-405f-b172-5c8d52518b75": ", , suggestion and also adding a \nspace after the for keyword for \nbetter readability. suggestion \nfor i in range(10):  ### Your \nReview of the code: I would \nsuggest using a for loop with \nthe range function and adding \na space after the for keyword \nfor better readability., consider using 'for i := 0; i < 10; i++' instead of \n'range(10)'. Here's a corrected example:\n1\n```go\n2\nfor i := 0; i < 10; i++ {\n3\n    // Your code here\n4\n}\n5\n```, , \n, , , , , \n9., } p\nages = {\n    'pam': 24,\n    'jim': 24,\n    'michael': 43\nrint(f'Michael is {ages[\"michael]} \nyears old.'), suggestion\nages = {    'pam': 24,    'jim': \n24,    'michael': \n43}print(f'Michael is \n{ages[\"michael\"]}')  [/INST] I \nwould suggest removing the \nspace after the comma in the \nprint statement. So it would be \nprint(f'Michael is \n{ages[\"michael\"]}'). This is a \ncommon convention in \nPython., 1. Variable name  ages  is a dictionary with string keys \nand integer values, consider using a list of tuples \ninstead for simplicity and better readability when \nworking with key-value pairs.\n2. Missing a comma before the last key-value pair in \nthe dictionary declaration.\n1\nages = {\n2\n    'pam': 24,\n3\n    'jim': 24,\n4\n    'michael': 43\n5\n}\n6\nprint(f'Michael is {ages[\"michael\"]} \nyears old.'), , \n10., def foo():\n    return [1, 2, 3\nprint(foo()), There is a missing ] at the end \nof the function., Indentation is incorrect in the function definition  def \nfoo():    return [1, 2, 3 print(foo()) . It \nshould be  def foo():  return [1, 2, 3] \nprint(foo())  or  def foo():  return [1, 2, 3]  \nfor separate prints., , \n11., ages = {'pam'=24}, `ages` is a dictionary, so you \nshould use `ages['pam'] = 24`, This code creates a dictionary with a single key-value \npair and incorrectly assigns the value to the variable \nnamed 'ages'. To fix this, use curly braces to define a \ndictionary properly:\n1\nages = {'pam': 24}, , \n, Evaluation by Llama3-70b, Score for Mistral 7b Instruct \nFinetuned: 8/10, Score for Mistral-7b Instruct: 7/10, ,",
        "ac9b275f-72c0-42e5-b238-48bb3c03dd50": "#, Model Input, Phi 3 Finetuned, Phi 3, Distilled Version using \nLlama3-70b as Teacher \nmodel:\n1., returnns a, suggestion: return a, 1. Typo in function name: \"returnns\" \nshould be corrected to \"returns\".\n2. Missing return statement for the \nexpected value. Should include \n'return [expected_value];' or proper \nerror handling/logging.\n3. Incomplete code snippet, missing \ncontext of what is being returned \nand why it ends with just an \nunderscore \"_\"., The variable name  a  is not \ndescriptive and does not \nfollow naming conventions. It \nshould be renamed to \nsomething more meaningful. \nThe function is not defined, \nand it's unclear what the \npurpose of this function is. \nThe function is not returning \na value, which will cause a \nsyntax error. The function is \nnot properly formatted, \nmaking it difficult to read and \nunderstand.\n2., result = (4 + 2 * 3)), I think you should \nuse parentheses \nhere., 1. Missing closing parenthesis for \nexpression  (4 + 2 * 3) .\n2. Consider using parentheses \naround multiplication to avoid \nprecedence issues.\nTo fix this, update the code to:  result \n= ((4 + (2 * 3)))  or  result = (4, 1. The code snippet has a \nsyntax error due to an \nunclosed parenthesis. \n2. The correct syntax \nshould be  result = (4 \n+ 2 * 3) .",
        "015aa862-7bba-425b-b57f-54bb003ee8a9": ", , , + (2 * 3)), 3. The code is not doing \nanything with the result, \nit's just assigning a value \nto a variable. Consider \nadding a print statement \nor returning the result for \nbetter understanding.\n4. The code is not following \nthe PEP 8 style guide for \nPython code. It's \nrecommended to add a \nspace after the equal \nsign. The corrected code \nshould be  result = (4 \n+ 2 * 3) . \n5. The code is not handling \nany exceptions or errors. \nIt's always a good \npractice to add error \nhandling to ensure the \ncode doesn't break \nunexpectedly.\n6.  The code is not doing \nany meaningful \noperation. It's just \nperforming a simple \narithmetic operation and \nassigning the result to a \nvariable. Consider \nadding a comment to \nexplain the purpose of \nthis operation. \n7. The code is not reusable \nor modular. It's a one-\nliner operation. Consider \nbreaking it down into \nsmaller, reusable \nfunctions. \n8. The code is not following \nthe DRY (Don't Repeat \nYourself)\n3., if x=5:, if x == 5:, 1. Issue with indentation after \"if\". \nCorrect format is  if x == 5:\n2. Variable names should not be \nequal to Python keywords, use \ncondition_x  instead of  x .\n3. Missing colon at the end of \nconditional statement ( if ). Add a \ncolon:  if condition_x:, The code snippet has a \nsyntax error. The \ncorrect syntax for an \nif statement is `if x \n== 5:`. The provided \ncode is comparing `x` \nto `5` using a single \nequal sign, which is \nnot valid syntax in \nPython.",
        "c58b49f6-bfba-4c33-8e00-ba631ad3d6a5": "4., if x == 0, suggestion if x == 0:, 1. Missing colon after condition \ncheck. Correct format should be \nif x == 0:\n2. Consider adding a comment for \nclarity, e.g.,  # Checks if 'x' is \nzero ., The code snippet is a simple \nconditional statement that \nchecks if the variable  x  is \nequal to 0. It is a valid and \nstraightforward code, but it \nlacks a comment explaining \nthe purpose of the condition. \nAdding a comment would \nimprove the code's \nreadability and \nmaintainability. [/INST]:</s>\n</s></s> The code snippet is \na simple conditional \nstatement that checks if the \nvariable  x  is equal to 0. It is \na valid and straightforward \ncode, but it lacks a comment \nexplaining the purpose of the \ncondition. Adding a comment \nwould improve the code's \nreadability and \nmaintainability. </s>  \nAdditionally, the code \nsnippet could be improved \nby using a more descriptive \nvariable name instead of  x , \nwhich is not self-explanatory. \nFor example,  is_zero  or \nis_zero_value  could be \nused to make the code more \nreadable. </s>  Furthermore, \nthe code snippet could be \nenhanced by considering \nedge cases.\n5., ages = {\n    'pam': 24,\n    'jim': 24\n    'michael': 43\n}, suggestion = {'pam': \n24,    'jim': 24,    \n'michael': 43}, Missing comma between 'jim' and \n'michael'. Corrected line should be \nages = {'pam': 24, 'jim': 24, \n'michael': 43}, The code snippet has a \nsyntax error. The dictionary \nis missing a comma between \nthe keys 'jim' and 'michael'. \nThe corrected code should \nbe:  ages = {'pam': 24, \n'jim': 24, 'michael': \n43}  The dictionary keys are \nnot in alphabetical order, \nwhich might make it harder \nto read and maintain. \nConsider sorting the keys \nalphabetically. The variable \nname 'ages' is not \ndescriptive. Consider using a \nmore descriptive name like \n'people_ages' or \n'individual_ages'. The",
        "8119742c-31bb-4a2a-b6f2-fb022596c73e": ", , , , dictionary values are not \nvalidated. If the values are \nsupposed to be integers, \nconsider adding a validation \ncheck.\n6., len('hello') = 5, suggestion \nlen('hello') == 5, 1. Syntax error: Replace '=' with '==' \nfor comparison, e.g., \nlen('hello') == 5 .\n2. Consider using a variable if this \nvalue is reused to improve \nreadability and maintainability., The code snippet has a \nsyntax error. The \ncorrect syntax for \nassigning a value to a \nvariable is \n`variable_name = \nvalue`. In this case, \nit should be \n`len('hello') = 5` \ncorrected to \n`len('hello') = 5`. \nHowever, this is still \nincorrect as it's \ntrying to assign a \nvalue to the result of \n`len('hello')`, which \nis an integer. The \ncorrect way to assign a \nvalue to a variable is \n`result = \nlen('hello')`.\n7., fro i in range(10):, suggestion: `for i in \nrange(10):, Missing colon after  for i in \nrange(10) . Corrected version:  for \ni in range(10):, The code snippet has a \nsyntax error. The \ncorrect syntax for a \nfor loop in Python is \n`for i in range(10):`. \nThe provided code \nsnippet is missing the \ncolon (`:`) at the end \nof the `for` statement.\n8., for i range(10):, suggestion for loop: \nfor i in range(10):, No closing colon after  for  loop \ndeclaration. Recommend adding  :  at \nthe end. Also, consider using \nrange(10)  over  i in range(10) ., The syntax is incorrect. It \nshould be  for i in \nrange(10):  instead of  for \ni range(10): . The code \nsnippet is incomplete. It \nlacks an action or operation \nto perform within the loop.\n9., } p\nages = {\n    'pam': 24,\n    'jim': 24,\n    'michael': 43\nrint(f'Michael is \n{ages[\"michael]} years old.'), } p\nsuggestion: ages = {\n    'pam': 24,\n    'jim': 24,\n    'michael': 43\nrint(f'Michael is \n{ages[\"michael\u201d]} \nyears old.'), Use lowercase for dictionary keys: \nages  should be  {'pam': ...}  \ninstead of  {'Pam': ...}  to ensure \nconsistency in key casing.\nMissing quotes around the string \noutput: Change print statement to \nprint(f'Michael is, The code snippet is \nmostly fine, but it has \na potential issue. The \nvariable `ages` is a \ndictionary, and \naccessing its values \nusing `ages['michael']` \nis safe. However, if",
        "e4b52e58-35f6-490f-b683-991f09ba458f": ", , , {ages[\"michael\"]} years old.')  \nfor proper syntax and readability., the key 'michael' does \nnot exist in the \ndictionary, it will \nraise a `KeyError`. To \navoid this, you could \nuse the `get` method, \nwhich returns `None` if \nthe key is not found.\n10., def foo():\n    return [1, 2, 3\nprint(foo()), suggestion: `def \nfoo():    return [1, 2, \n3]`, Missing indentation for function \nprint ., The `print` statement \nis not properly \nindented.\n11., ages = {'pam'=24}, suggestion: return a, Missing comma after 'pam'. Correct \nsyntax for dictionary is  ages = \n{'pam': 24, } ., The syntax for assigning a \nvalue to a key in a dictionary \nis incorrect. It should be \nages = {'pam': 24} . 2. \nThe variable name  ages  is \nnot descriptive. It would be \nbetter to use a more \ndescriptive name like \nperson_ages . 3. The code \nsnippet lacks a function or a \nmain block, making it difficult \nto understand its purpose.\n, Evaluation by Llama3-70b, Score for Phi-3 \nFinetuned: 7/10, Score for Phi-3: 9/10, Score: 6/10",
        "1ed00131-1bfd-45c2-b763-796361fa8c5c": ", Models, , , \nConfigurations, Llama2-7b, Phi-2, Mistral-7b \nInstruct, Phi-3\nNo. of Training Examples, 1000, 2000, 2000, 2000\nLora_r, 16, 16, 32, 32\nLora_alpha, 64, 32, 64, 64\nLora_dropout, 0.05, 0.01, 0.1, 0.1\nLearning Rate, 1e-4, 2e-5, 5e-5, 5e-5\nEpochs, 2, 3, 3, 3",
        "5371d7a7-1988-4ca7-8341-fcd7e1f4aaa3": "#, Model, Codegen2-7B_P, , \n1., returnns a, 1\ndef returnns_a():\n2\n            return a, , \n2., result = (4 + 2 * 3)), 1\n2\n3\n4\n5\n6, ```python\n        result = (4 + 2 * 3)\n        ```\n        ### Expected Result:\n        ```python\n        result = 7, \n, , , , \n, , , , \n3., if x=5:, 1\n2\n3\n4\n5\n6, ```python\n        if x==5:\n            print(\"x is 5\")\n        else:\n            print(\"x is not 5\")\n        ```, \n, , , , \n, , , , \n4., if x == 0, 1\nif x == 0:\n2\n            print(\"x is zero\"), , \n5., ages = {\n    'pam': 24,\n    'jim': 24\n    'michael': 43\n}, ages = {'pam': 24,    'jim': 24   'michael': 43}, , \n6., len('hello') = 5, len('hello') = 5, , \n7., fro i in range(10):, 1\nfor i in range(10):\n2\n            print(i), , \n8., for i range(10):, 1\nfor i in range(10):\n2\n  print(i), , \n9., } p\nages = {\n    'pam': 24,\n    'jim': 24,\n    'michael': 43\nrint(f'Michael is {ages[\"michael]} \nyears old.'), Nothing, , \n10., def foo():\n    return [1, 2, 3\nprint(foo()), 1\ndef foo():\n2\n  return [1, 2, 3]\n3\nprint(foo()), , \n11., ages = {'pam'=24}, ages = {'pam'=24}, ,"
    },
    "relevant_docs": {
        "d0945bf1-6f53-4ff0-88b2-28ecb573d3a2": [
            "cb053f10-2e2d-4f77-a5cb-45c45ae50fb0"
        ],
        "a6595782-4e99-4766-b616-e0fd56440b35": [
            "cb053f10-2e2d-4f77-a5cb-45c45ae50fb0"
        ],
        "c95eeae1-3905-43b5-995e-fad0c67c9b79": [
            "aa768b7f-5d59-41dd-954a-a59904c9afe9"
        ],
        "391be456-47a7-4990-bf5a-83356addb16a": [
            "aa768b7f-5d59-41dd-954a-a59904c9afe9"
        ],
        "16bf3e4c-dc14-4d90-8f05-9c599c1e8983": [
            "3a5fda7f-762e-47d1-a0b9-5af3bb23b00d"
        ],
        "86796804-452a-45cc-8b20-38f35eedcd55": [
            "3a5fda7f-762e-47d1-a0b9-5af3bb23b00d"
        ],
        "efeac7f9-77a9-4ca1-b3c4-e5cd3cf783e7": [
            "5320f796-79cd-4ba5-bf2a-e0409dd2a65a"
        ],
        "7e6d1334-f670-4785-bb04-553fb612a8d1": [
            "5320f796-79cd-4ba5-bf2a-e0409dd2a65a"
        ],
        "ad3f265a-5fac-44bd-b322-30f11434d804": [
            "87ccf1ad-67c1-4690-a60b-3db13b3fc16b"
        ],
        "0d2336d9-a5fd-425f-97d5-a6dad7c775aa": [
            "87ccf1ad-67c1-4690-a60b-3db13b3fc16b"
        ],
        "4ce471e3-ef67-4165-b756-d32778237f91": [
            "be8253b7-d682-49aa-aef9-3bf1d42eb545"
        ],
        "a71ef6b9-c7a6-463f-92d2-88e9183315e8": [
            "be8253b7-d682-49aa-aef9-3bf1d42eb545"
        ],
        "f70a276a-aca5-44ec-ac2f-ddf993478800": [
            "40f84dab-34b5-402a-a8bb-e04b43f79b1e"
        ],
        "4829589f-5f47-4e25-b361-d2848cec3b53": [
            "40f84dab-34b5-402a-a8bb-e04b43f79b1e"
        ],
        "0c89a36a-2b08-4604-a1b8-571ff87d5d4f": [
            "3c9188f2-4a38-4302-8571-7b42e2f4a386"
        ],
        "b46c48eb-b6a6-437e-9d68-4af6ab407f8f": [
            "3c9188f2-4a38-4302-8571-7b42e2f4a386"
        ],
        "0018ab57-2d16-41af-aa50-ac8bcc6cd6f4": [
            "a470b594-36f9-4aee-a482-25840aaab98e"
        ],
        "2bbd2718-ad33-49fd-a810-2b4a4b514012": [
            "a470b594-36f9-4aee-a482-25840aaab98e"
        ],
        "2999dbc9-6cc4-477a-9d36-e71a545a43f1": [
            "312a819d-3274-4e0f-9903-708c211045fc"
        ],
        "2c0c8c23-cbbe-4509-9e9d-e995c129b2c1": [
            "312a819d-3274-4e0f-9903-708c211045fc"
        ],
        "9b4ccf44-ef03-48a2-a57b-58877454b89b": [
            "218b7775-9ce5-4810-a908-15cc1aacdff2"
        ],
        "ad1ed110-fab9-46ff-a297-9a3a16859702": [
            "218b7775-9ce5-4810-a908-15cc1aacdff2"
        ],
        "f62460ae-2c85-4b9b-9960-7b52ce9d3b13": [
            "85f87a94-60ac-41a4-83a6-eda96f3304d9"
        ],
        "b6847ac3-17eb-4dea-bb8e-4f12ed72f3a5": [
            "85f87a94-60ac-41a4-83a6-eda96f3304d9"
        ],
        "e9e35a3d-667c-409f-ae96-f9dd8d3f4e32": [
            "9d5bec3d-bcdc-461e-8d5b-dd2e1fbd1a06"
        ],
        "e54de0d2-8d19-4f45-aa5d-b82c84f381d7": [
            "9d5bec3d-bcdc-461e-8d5b-dd2e1fbd1a06"
        ],
        "59c987d9-0a7e-44f2-9b84-ff3cdc66defc": [
            "c82733a7-38c8-4a3f-bd09-b7f0b1b04b1d"
        ],
        "4aaeb00d-bc2c-4fc3-9315-748d34823c37": [
            "c82733a7-38c8-4a3f-bd09-b7f0b1b04b1d"
        ],
        "17938002-8247-4aba-a50f-ba24aba0ddae": [
            "8d70ef5e-c9e8-42a1-9472-20ef5e6a3a3c"
        ],
        "5ee3491d-72d9-4a22-88a3-3219fa806fe3": [
            "8d70ef5e-c9e8-42a1-9472-20ef5e6a3a3c"
        ],
        "533d1c8d-907b-452b-81b1-72c22150aaea": [
            "f67cb70c-a403-458d-95b1-4f0c784571e1"
        ],
        "394d1aaa-ae13-420f-b3cc-25f2dffe9435": [
            "f67cb70c-a403-458d-95b1-4f0c784571e1"
        ],
        "a9fc5cf3-6b1d-48e0-ab13-f2933ee54f30": [
            "257cc462-1f84-49b7-afa3-4569cdff3bba"
        ],
        "d1ea147d-d599-469e-aabc-daef8d47f4a3": [
            "257cc462-1f84-49b7-afa3-4569cdff3bba"
        ],
        "faaff5d4-2af2-4775-b30d-0f9d5da7e3dd": [
            "2f97842d-a2b8-4e00-9bdc-c887d4e43b47"
        ],
        "956bc927-9c72-499f-bc46-8bb2c32550f6": [
            "2f97842d-a2b8-4e00-9bdc-c887d4e43b47"
        ],
        "393c12b8-3906-42a1-b756-5bfb1bdafb20": [
            "859bfc2d-e07e-4a62-b714-79d2cde119e4"
        ],
        "c624018a-c7f2-46b1-8bb6-51840efba4bc": [
            "859bfc2d-e07e-4a62-b714-79d2cde119e4"
        ],
        "0b8592bd-49d0-4b61-a30c-4bff7d79c30a": [
            "018dd329-c88b-4c5c-b46c-a3f6df41d2d0"
        ],
        "6796707e-7ad6-4151-b97d-3b370dfec9ab": [
            "018dd329-c88b-4c5c-b46c-a3f6df41d2d0"
        ],
        "980b0dcb-d9ac-4e8e-939f-54b10e93f9dc": [
            "c5209c94-e998-4803-bdda-a0d469e02bd8"
        ],
        "8f991b3a-e6f8-4ace-90cb-e8a4d5eff3e6": [
            "c5209c94-e998-4803-bdda-a0d469e02bd8"
        ],
        "7cddd99b-e715-4907-a2ee-6c4be487fdd3": [
            "4fe2d271-a4e9-47ce-82fa-de04cea3c28a"
        ],
        "de6a0c0e-82bf-4272-bad5-85d3f99974f8": [
            "4fe2d271-a4e9-47ce-82fa-de04cea3c28a"
        ],
        "53fbd997-0f1e-4455-9395-f2fc6f3776c2": [
            "e42b5e2d-2cd6-4428-9192-467c2216d965"
        ],
        "7ffa5cb3-6d41-4580-ba10-d266dbe7ab52": [
            "e42b5e2d-2cd6-4428-9192-467c2216d965"
        ],
        "d65837b4-2755-43bb-bcbd-f8ab7ed76beb": [
            "07127a49-e7bb-4611-a551-a4c009ce489b"
        ],
        "91a72c92-3963-441e-9223-eef63ec0cc83": [
            "07127a49-e7bb-4611-a551-a4c009ce489b"
        ],
        "99a0e454-90ef-4a72-9b2c-dc863329d8e1": [
            "17feaf41-37ea-4426-86c2-2ea24f13a716"
        ],
        "db024edf-88b6-4cd0-9503-86aab0cecdb9": [
            "17feaf41-37ea-4426-86c2-2ea24f13a716"
        ],
        "0e96b496-3f65-492d-9324-11f0d6ca6a1d": [
            "7771b031-5a66-4c3e-aacc-386dd0248e82"
        ],
        "d1b5aea1-79f1-4a8d-816d-f4a717b99bfc": [
            "7771b031-5a66-4c3e-aacc-386dd0248e82"
        ],
        "2a8507ff-f643-4912-9be5-5a890bab1004": [
            "649d234e-5ed9-48b9-80cd-117844f651d2"
        ],
        "342a0c54-9943-4ddb-a33f-a6df04bdfcd8": [
            "649d234e-5ed9-48b9-80cd-117844f651d2"
        ],
        "d8918fef-b887-40de-8e76-0345ce84bc68": [
            "72e5afdc-1812-440d-8cef-93c96b232d8c"
        ],
        "941e8df5-276e-470c-8ab9-253d8500f685": [
            "72e5afdc-1812-440d-8cef-93c96b232d8c"
        ],
        "4fb19bfd-1cc8-440c-a04a-d2bbeb31f13c": [
            "7b4a2698-3481-4db4-b87c-c31aefc16e50"
        ],
        "c8608e02-44d5-4718-ba2b-a001f5fefb33": [
            "7b4a2698-3481-4db4-b87c-c31aefc16e50"
        ],
        "b44cc2a9-c6cf-4b74-98a5-94d9468b08b5": [
            "c713eb68-d950-42de-9858-54092d6f021b"
        ],
        "0b7f60b7-2901-49c3-9dcd-0253ea612d8d": [
            "c713eb68-d950-42de-9858-54092d6f021b"
        ],
        "b7fb0d9d-e3e5-4783-a3c3-3fc75d37310d": [
            "798b9c7a-3c1e-4f81-bcd6-c98afb8fc3d1"
        ],
        "7d333a5d-0816-4192-9f25-17e8677b1cde": [
            "798b9c7a-3c1e-4f81-bcd6-c98afb8fc3d1"
        ],
        "02f46966-98b8-4fc9-a192-c6c1976877ca": [
            "c940a669-ae53-45bf-8d65-30358a293d0b"
        ],
        "1c211ffc-150f-4aec-9e14-c6b7d258e672": [
            "c940a669-ae53-45bf-8d65-30358a293d0b"
        ],
        "de2c98d1-6f09-497d-a2db-508a95c0f8b7": [
            "c96e0bd3-2f96-4585-a678-799145076101"
        ],
        "1abe0ce6-dae0-4c5e-b6ec-dc27764e9cb4": [
            "c96e0bd3-2f96-4585-a678-799145076101"
        ],
        "fd1f475b-e36e-4679-8726-b16ec3966ecd": [
            "2398d568-8034-44de-ae15-e29c197d8a4f"
        ],
        "149bfb34-be76-48ed-ae7a-5e68e3647205": [
            "2398d568-8034-44de-ae15-e29c197d8a4f"
        ],
        "f5903cb6-fa73-4faa-ac67-1718bba113f8": [
            "e2fc7775-287e-4d08-9b8c-ed1a1623933b"
        ],
        "fe16a8dd-1ddb-4ab5-a05d-d91a01468e1f": [
            "e2fc7775-287e-4d08-9b8c-ed1a1623933b"
        ],
        "f1febec1-df80-408f-8a5c-f3e1cd4d6969": [
            "b08cee1d-c764-4a13-b511-60354b56a5e8"
        ],
        "a0c5d550-1131-4548-aab1-3852fbe40f86": [
            "b08cee1d-c764-4a13-b511-60354b56a5e8"
        ],
        "13377529-434b-4620-ae52-979577abb082": [
            "35a7c84b-f669-4893-b77a-b34817572599"
        ],
        "7b731fce-55be-41cb-9243-2f87750e0d14": [
            "35a7c84b-f669-4893-b77a-b34817572599"
        ],
        "3d44faf5-2c73-42fc-b1ff-3fc5caee79cd": [
            "4a074b70-6008-41c2-9bfd-3bc332b9eb5e"
        ],
        "fbc8f99e-ff37-4714-a020-c302a3edae3e": [
            "4a074b70-6008-41c2-9bfd-3bc332b9eb5e"
        ],
        "0899e55a-9573-428a-86c2-f19209bfe0f6": [
            "2e10e13d-c317-46da-a60d-1c78a8a242da"
        ],
        "57a9d2b0-8d3d-4110-9823-d2f785be7250": [
            "2e10e13d-c317-46da-a60d-1c78a8a242da"
        ],
        "1e860744-49cb-4b29-874e-15ae7ad47635": [
            "ec80a41b-53d7-4494-b746-9010593ff244"
        ],
        "bc7782e6-3bfe-46c1-88ad-fd0d5a30dda1": [
            "ec80a41b-53d7-4494-b746-9010593ff244"
        ],
        "ef7b2e19-5e5c-4a51-827c-98eacbbbe2be": [
            "99e48fba-80e8-45de-8a6c-d8608458b991"
        ],
        "8f822742-ff90-4f0e-a841-4ce60538c321": [
            "99e48fba-80e8-45de-8a6c-d8608458b991"
        ],
        "1bd0055c-1fd7-4df7-aba4-b3a1223bdfd2": [
            "eb02bd7b-a925-4f5b-9c49-56f0097ed0f1"
        ],
        "1c99a588-00e4-45c8-982c-c6e4e93b8c46": [
            "eb02bd7b-a925-4f5b-9c49-56f0097ed0f1"
        ],
        "477e0fe0-6b69-426a-a371-defcb21dbb35": [
            "5a7f4d38-7883-4931-9c83-e9928811ec1b"
        ],
        "4bea6f3b-3020-4bf7-8e7f-75c9167f6d4a": [
            "5a7f4d38-7883-4931-9c83-e9928811ec1b"
        ],
        "e438eb46-f7d2-4400-a2b8-aee61bad742e": [
            "d80af725-5a52-4390-b1ea-851a95dcdea1"
        ],
        "75a77951-bbe5-485d-afc2-b68e92ddc72c": [
            "d80af725-5a52-4390-b1ea-851a95dcdea1"
        ],
        "a640e41d-8991-4e75-aa2b-ff214c930756": [
            "1f08b0e7-6af7-4258-b34a-4abc08172bb4"
        ],
        "1247d2ac-0c16-405c-bda1-60e9515198f7": [
            "1f08b0e7-6af7-4258-b34a-4abc08172bb4"
        ],
        "77966b9a-51d8-420f-a72a-ba2a15eba6d3": [
            "3b2810cb-705e-4c5c-8d05-4c4345d0d538"
        ],
        "803a20cb-70c5-4478-8ddb-88e1e20c1b79": [
            "3b2810cb-705e-4c5c-8d05-4c4345d0d538"
        ],
        "10966c53-d7fe-4536-8822-ef98e054ff63": [
            "ebbe6c2b-69a1-45d0-a181-337986db9001"
        ],
        "774e82da-084b-4a80-9517-b63e8b763731": [
            "ebbe6c2b-69a1-45d0-a181-337986db9001"
        ],
        "f719afca-76e3-4c24-96f2-66030d0ba790": [
            "c3dbec98-fc2e-4f49-9a38-2a431807e5a6"
        ],
        "9d859f88-38d9-465f-a176-93f30af272fb": [
            "c3dbec98-fc2e-4f49-9a38-2a431807e5a6"
        ],
        "7c23e1be-e735-4cb7-9c71-17d75e2a1d32": [
            "65735b46-0515-4f2f-9aef-08235822585c"
        ],
        "dab16b72-e218-47a7-ad79-28db6de91003": [
            "65735b46-0515-4f2f-9aef-08235822585c"
        ],
        "ee0f2801-3d73-4bdf-a07f-fe57146270ed": [
            "894d3c5e-5b95-4b80-831e-3dfc41a3b790"
        ],
        "eb9c6577-6734-4348-8d3f-9e140d9e57bc": [
            "894d3c5e-5b95-4b80-831e-3dfc41a3b790"
        ],
        "5ca145ab-9f62-47f7-9b96-b4fd74c24b26": [
            "eb506b37-d37b-43bd-a97b-610f4bb6591f"
        ],
        "3d8a7d7c-8ee2-4c65-b752-a2828a7212c7": [
            "eb506b37-d37b-43bd-a97b-610f4bb6591f"
        ],
        "28641d4d-9bab-4f9b-9418-51b5b02eb7e8": [
            "5a134fcf-ab34-44e8-9f9b-b71f1dcb8764"
        ],
        "5adb647a-53ed-415b-9add-b5166e052c0d": [
            "5a134fcf-ab34-44e8-9f9b-b71f1dcb8764"
        ],
        "88f208db-760f-4a6e-ae75-33683cfb5932": [
            "f3497441-e1fb-43ba-8e71-3ed64c59fd5a"
        ],
        "73259d3d-5fab-4164-9e70-09660d4c2cd8": [
            "f3497441-e1fb-43ba-8e71-3ed64c59fd5a"
        ],
        "a47f7b20-3dcd-4b32-9836-8bb6dd593f95": [
            "6861fb5c-c52d-40c9-a097-f82b2f775f94"
        ],
        "06d9fcd1-9162-4d4d-9aba-1270929afcfe": [
            "6861fb5c-c52d-40c9-a097-f82b2f775f94"
        ],
        "46303d79-da22-46ee-96de-8e51155d2399": [
            "b3470647-2e0a-4388-9766-5edf48fbf33c"
        ],
        "95bfb124-a6d3-43a7-9f02-d7a606530903": [
            "b3470647-2e0a-4388-9766-5edf48fbf33c"
        ],
        "ecc75297-8a9a-4d09-8557-bd166dbb22fa": [
            "f005a414-f98e-438b-8062-bc71537ae832"
        ],
        "3920bb0e-2afb-440e-ad59-063867453081": [
            "f005a414-f98e-438b-8062-bc71537ae832"
        ],
        "3710801d-88bb-4fa0-a59d-d16bab787af1": [
            "58fde0a9-0220-46be-bcb6-7d51e6529ba2"
        ],
        "cffbbdf8-a94e-457f-865d-5b11f8ce9390": [
            "58fde0a9-0220-46be-bcb6-7d51e6529ba2"
        ],
        "a4102290-9ad6-4228-823b-8d777a063d0d": [
            "8355bfa5-f6cd-430b-897e-afa83947ee86"
        ],
        "204c02fc-55f8-49a2-9bd5-14792e6dcdbf": [
            "8355bfa5-f6cd-430b-897e-afa83947ee86"
        ],
        "056928d3-c269-4056-9fe2-5d00bf256809": [
            "c8ab041d-1671-47c3-ae4e-399ad33eccf8"
        ],
        "860570b5-b3a0-4313-bbd7-8bc0a964684b": [
            "c8ab041d-1671-47c3-ae4e-399ad33eccf8"
        ],
        "47cd971c-0fd1-4cf9-ac49-20ddef94cc51": [
            "9b43d2c9-3dae-4f1d-b5d0-84d666f14c9c"
        ],
        "a2bb15bd-8bc6-4c42-827b-f519aae408a1": [
            "9b43d2c9-3dae-4f1d-b5d0-84d666f14c9c"
        ],
        "462248cd-62cc-40ef-89e8-5ee68789691a": [
            "79c78106-fb41-43c5-8b1d-46721a3063cc"
        ],
        "458b32fb-c381-4433-9140-36c9975673df": [
            "79c78106-fb41-43c5-8b1d-46721a3063cc"
        ],
        "f6fe1aae-1cdf-494b-baa2-d52ca3d912a7": [
            "bbd29da6-2de4-498e-92eb-ff7e539bdb7c"
        ],
        "101d23b0-4a81-4703-aed2-fb2797a80b1e": [
            "bbd29da6-2de4-498e-92eb-ff7e539bdb7c"
        ],
        "bf912abe-ac77-4c2a-9605-86050e15d7bb": [
            "7d3197fc-9441-4953-8b1d-4f6ee2db2982"
        ],
        "38af6a5a-e54c-4756-8843-57606e87e3c6": [
            "7d3197fc-9441-4953-8b1d-4f6ee2db2982"
        ],
        "50005a22-073f-4e15-a4d3-363b1139f99b": [
            "69c2625f-b8d3-4196-b922-2aec914626c9"
        ],
        "06e1c0d4-d07c-43c5-98ae-e80772035598": [
            "69c2625f-b8d3-4196-b922-2aec914626c9"
        ],
        "eed178e8-019b-4524-9048-4c6b29b7e6f3": [
            "214d4301-2340-42c4-aa9c-bc68ae4a0c74"
        ],
        "7884a6d1-16d0-4a33-84db-dbab5be8d1d5": [
            "214d4301-2340-42c4-aa9c-bc68ae4a0c74"
        ],
        "c4680fb8-6013-4b2e-9d1a-797de5734525": [
            "2ae5281d-6200-4beb-b0a4-a1bd1e8fe15e"
        ],
        "0f31e7a2-cab3-418b-94e8-759769489cd5": [
            "2ae5281d-6200-4beb-b0a4-a1bd1e8fe15e"
        ],
        "74964bf4-7f52-423c-9770-362b84635f04": [
            "6249d4c5-1d03-4810-b29f-2f4132ec2759"
        ],
        "3ad08af1-5d19-4961-b221-4d20f5bfca43": [
            "6249d4c5-1d03-4810-b29f-2f4132ec2759"
        ],
        "767d745d-5463-461c-825b-5922631838b1": [
            "860f8628-8b6e-4dfe-980a-00f7784b5ffa"
        ],
        "b990cd89-472c-46b8-aa7d-8eb807793e73": [
            "860f8628-8b6e-4dfe-980a-00f7784b5ffa"
        ],
        "b406314a-d0e6-4665-9ff9-c7abd40da352": [
            "fd49c2ce-abe6-40e4-8fcf-f47797c852d2"
        ],
        "8b13d4d9-d03a-423b-a8f2-599b57aaff32": [
            "fd49c2ce-abe6-40e4-8fcf-f47797c852d2"
        ],
        "a740ee86-d782-4dc1-b592-d01d14b819f9": [
            "ec2d7fdd-74ec-456a-8cd2-2c854c68aae7"
        ],
        "29d09f48-cdd0-47d5-89f2-58757bf80b90": [
            "ec2d7fdd-74ec-456a-8cd2-2c854c68aae7"
        ],
        "16787493-070c-42a0-a299-4ec97ceb4525": [
            "a7c2548c-8d31-4651-8a74-03e8f7cccd8e"
        ],
        "9b950f47-fd2c-4e8e-a9a2-8be179b3b36d": [
            "a7c2548c-8d31-4651-8a74-03e8f7cccd8e"
        ],
        "d82f82a3-d36e-4c62-9f09-1f9cfbf4e3ae": [
            "fe2f8bd1-ec06-4e38-9dba-f3ed51c16045"
        ],
        "cc478788-7aa4-49d0-850d-88a761fc20ba": [
            "fe2f8bd1-ec06-4e38-9dba-f3ed51c16045"
        ],
        "d01ea830-d79b-4a4d-94fe-b19e953401b2": [
            "b8b6934b-5c8e-4f09-ba1b-7434acc286b1"
        ],
        "72b06787-f5b8-41d7-bd9b-ea7f2ce0f050": [
            "b8b6934b-5c8e-4f09-ba1b-7434acc286b1"
        ],
        "d17a8619-a880-4403-a616-356fe3dab3cf": [
            "31b39364-0a24-4676-8175-069ae644e1e4"
        ],
        "ff672806-e1dc-4321-bc31-0cc7b21f2e32": [
            "31b39364-0a24-4676-8175-069ae644e1e4"
        ],
        "c6a625ff-6059-4b2f-8df8-7792237910d6": [
            "1990c119-beb8-4bdd-8da0-7b74f40e71bb"
        ],
        "cf014595-12e7-4dba-b745-ea86c50e2eeb": [
            "1990c119-beb8-4bdd-8da0-7b74f40e71bb"
        ],
        "786c0872-7196-47f4-b8ee-ca343df4957e": [
            "4b820688-0321-415a-940a-b12aaeb71ba9"
        ],
        "4e37487c-a3aa-468e-ab17-17d7d2076809": [
            "4b820688-0321-415a-940a-b12aaeb71ba9"
        ],
        "410cd6d6-7dc3-464d-bbbc-3adcaba325d5": [
            "4caa73ac-2352-4b0d-b694-36974c59d663"
        ],
        "9af17fdd-f0ce-46bd-958f-ed24c780c242": [
            "4caa73ac-2352-4b0d-b694-36974c59d663"
        ],
        "ea9ecf43-9f10-4756-9530-382ec612df8d": [
            "67c3666c-d75e-41fe-a41c-1d8a30393c29"
        ],
        "8b5fd1d8-0a64-4306-89a3-b82c26a8155c": [
            "67c3666c-d75e-41fe-a41c-1d8a30393c29"
        ],
        "f02dcbfb-f5c6-4bdd-9cd4-7e1bc9d907a3": [
            "ac3a69fd-d313-4369-9474-5facd03b5c05"
        ],
        "4e2dff90-93be-45c7-acc4-bf85aa022246": [
            "ac3a69fd-d313-4369-9474-5facd03b5c05"
        ],
        "414d4d85-8dd9-43e2-a067-79372fdb0d67": [
            "7793fbde-7678-4840-ad09-203519466c29"
        ],
        "da29eb5e-e891-4292-8d84-c4f4908a378b": [
            "7793fbde-7678-4840-ad09-203519466c29"
        ],
        "4f377f0b-268b-4cfb-9f34-faa77b102e6c": [
            "28c9493e-7f60-4b21-b26c-f4a175c9d46c"
        ],
        "3d6f43c6-c3c4-4de5-bd21-86d3ac94970f": [
            "28c9493e-7f60-4b21-b26c-f4a175c9d46c"
        ],
        "b497d3e8-2c0e-4639-bbf7-a388a113d3eb": [
            "c78abfd3-74bd-4ff0-8810-2157b0ec7422"
        ],
        "6193851e-d643-49e7-a8bb-5aea04e04092": [
            "c78abfd3-74bd-4ff0-8810-2157b0ec7422"
        ],
        "57f776f8-196b-4403-8b73-5a86a4b504f0": [
            "2c69e278-0f3f-4123-9352-f603bc697227"
        ],
        "55eee233-cbfa-4fca-b9dd-af9070ac98fb": [
            "2c69e278-0f3f-4123-9352-f603bc697227"
        ],
        "4e4b2e35-873b-445f-bbeb-1e2139ab859a": [
            "7b5e7bb3-6904-4298-85d4-5f18160601aa"
        ],
        "2b61bbc1-098b-4754-b6ec-d5896732fdd9": [
            "7b5e7bb3-6904-4298-85d4-5f18160601aa"
        ],
        "ca2d67a7-b0fa-4885-b66d-fe0f2e03985b": [
            "f47faa58-6815-430b-b235-13813f6cf116"
        ],
        "3c527155-6f3e-485f-87e0-05267a8f0acb": [
            "f47faa58-6815-430b-b235-13813f6cf116"
        ],
        "1884cb96-030c-4148-9fa4-89d07420d5c8": [
            "e6ef2996-2b52-4e9d-8dcb-3f08256a66f3"
        ],
        "4aeaf5da-1554-47b6-be60-e9dd142fc9f1": [
            "e6ef2996-2b52-4e9d-8dcb-3f08256a66f3"
        ],
        "71c84745-3c58-4f62-ae7a-31fbfd76c1b7": [
            "debf909a-a721-41ae-8921-c94b993e9b93"
        ],
        "5db839c7-afaf-47f4-8a18-7f2914587cb9": [
            "debf909a-a721-41ae-8921-c94b993e9b93"
        ],
        "18f97ffd-0f9c-4156-b145-8541b665bc37": [
            "b2605804-e8cc-4683-9efb-2a207961b9e7"
        ],
        "46d31a11-7fa1-499e-b1a9-03584f098473": [
            "b2605804-e8cc-4683-9efb-2a207961b9e7"
        ],
        "363dda3c-4601-4c17-9335-5f6be5d61cb6": [
            "276c1dd8-8350-4251-941a-8155afb79331"
        ],
        "0f1379a2-92e8-41f4-a5b2-a64e1865f0ec": [
            "276c1dd8-8350-4251-941a-8155afb79331"
        ],
        "c8102b51-1635-47c8-8a72-f04cc09e91b2": [
            "a20d5d7b-699d-4d67-8f74-77174b70a99f"
        ],
        "7eba2005-98a8-4d28-9a23-eefe65ac02d0": [
            "a20d5d7b-699d-4d67-8f74-77174b70a99f"
        ],
        "9479c3b1-1a10-4e66-b3d5-dd8c96229e01": [
            "3ea4beaf-61c0-4c60-81e6-35bd41b1aebe"
        ],
        "8687984a-f231-43e8-b7cb-07ac850da725": [
            "3ea4beaf-61c0-4c60-81e6-35bd41b1aebe"
        ],
        "69d52aa7-1410-4ad5-82fc-9063c4aab368": [
            "5d752dd5-ccff-4e40-a358-5358c0761fc7"
        ],
        "2c31620f-be43-4a65-aeae-3650c689d61d": [
            "5d752dd5-ccff-4e40-a358-5358c0761fc7"
        ],
        "1fea9057-eec3-40da-bead-68093c8c186f": [
            "0097c01d-1392-4175-b915-e2d4d5b80546"
        ],
        "46a774bb-9748-4d9c-99dc-9b3ca5d7a41a": [
            "0097c01d-1392-4175-b915-e2d4d5b80546"
        ],
        "1e1f4af8-aca1-449b-a22f-9f140fc7735f": [
            "cc85baf2-f7ca-4e01-a9a1-c45abcb78d51"
        ],
        "61370cf5-167c-433d-af74-918b563a3436": [
            "cc85baf2-f7ca-4e01-a9a1-c45abcb78d51"
        ],
        "c988d174-e0d2-4707-b858-6b8e5a205494": [
            "49251bf2-fa81-44cf-b5c5-780e7eef496c"
        ],
        "981f003f-0d58-4cec-880e-6e801e696700": [
            "49251bf2-fa81-44cf-b5c5-780e7eef496c"
        ],
        "6e6f4d76-76e7-48e4-bc6a-122e32eada7a": [
            "58f64662-cf7f-400b-a299-a01fb6217188"
        ],
        "dfb62a47-e9ff-4afd-9347-6b031f6192ea": [
            "58f64662-cf7f-400b-a299-a01fb6217188"
        ],
        "7fac38c6-389e-4aec-8d48-8a609e62d962": [
            "9e7a3b72-cca3-4cfc-a040-5a7a161fe755"
        ],
        "763f6932-2abd-4b14-b8e8-2c152b234881": [
            "9e7a3b72-cca3-4cfc-a040-5a7a161fe755"
        ],
        "648310fa-08fa-4ed5-a49d-cc11eca4ca37": [
            "bfbfb10b-9ba8-4f3b-85b6-dccb37376389"
        ],
        "e94997fb-6e95-4f25-8c12-85ebdc85ec8b": [
            "bfbfb10b-9ba8-4f3b-85b6-dccb37376389"
        ],
        "6312f7d7-8a5f-4d2f-9205-d52de69d1681": [
            "9ee7a4f5-dee6-41f5-acc2-385c6a044d64"
        ],
        "5a875f1c-b1e6-457f-93f7-beefb0e6d315": [
            "9ee7a4f5-dee6-41f5-acc2-385c6a044d64"
        ],
        "72438bdf-554b-4b27-b190-099d1691edd6": [
            "cfa66be1-7ae1-4743-a608-38be1ab16f36"
        ],
        "a4d18acc-04e9-43aa-a91c-8bdcacfb3be2": [
            "cfa66be1-7ae1-4743-a608-38be1ab16f36"
        ],
        "c74626f2-3918-4b00-aa1c-0135a546723b": [
            "c5831a38-2e1a-4216-b9c4-60f5c2592dd1"
        ],
        "ebdc8a10-e9cf-4d06-a489-f72c856f33a7": [
            "c5831a38-2e1a-4216-b9c4-60f5c2592dd1"
        ],
        "fccb643f-f54d-4c91-aac2-59e77c87d461": [
            "2ff343cb-b580-42f2-8d80-d2c1f8112175"
        ],
        "62026ce4-5253-479c-a73b-9db108014e1b": [
            "2ff343cb-b580-42f2-8d80-d2c1f8112175"
        ],
        "f83edd10-62e2-4cdc-890d-72ad1e999798": [
            "1a9d05a6-048b-4915-8df7-f9591b5dcd07"
        ],
        "798b9353-e3bd-4e30-8121-8837e8fa6782": [
            "1a9d05a6-048b-4915-8df7-f9591b5dcd07"
        ],
        "defa0716-be65-4804-a0bc-e393ad700c74": [
            "22581b85-8bb0-446e-a936-641862133fe7"
        ],
        "c7c9818e-7761-418e-a272-b5fa595c4245": [
            "22581b85-8bb0-446e-a936-641862133fe7"
        ],
        "47b3b8c3-6419-4032-bddc-38b198314216": [
            "6fbc911e-cfcf-4130-8b1e-708853cfad05"
        ],
        "55db86e7-f347-45a1-8c19-038f1c5317ea": [
            "6fbc911e-cfcf-4130-8b1e-708853cfad05"
        ],
        "cbdb82e0-28ab-4d0d-bd58-464c110fee7e": [
            "baecaad8-be3b-4703-af47-a7848d3d7334"
        ],
        "b2f39790-cc0c-45bf-8e7b-1e9ab6e25868": [
            "baecaad8-be3b-4703-af47-a7848d3d7334"
        ],
        "1acdcd7c-5ff0-41f0-b9ca-6146509dc960": [
            "c96216a2-5ff6-4bec-97f9-a9ce908b9fe8"
        ],
        "def2dea6-12e3-4449-9eba-8a275a61df10": [
            "c96216a2-5ff6-4bec-97f9-a9ce908b9fe8"
        ],
        "13a76cfc-ebf7-4ad0-a410-5c56069b8d1b": [
            "a023cf8f-693b-45e8-9ca4-9333ce3c7f1f"
        ],
        "b85eeea4-115c-4aad-8692-d3b4f39fbe6f": [
            "a023cf8f-693b-45e8-9ca4-9333ce3c7f1f"
        ],
        "b7e8c89f-4bcf-45c6-bc48-0bce1baba82d": [
            "016179fd-452d-43c6-9631-04d1a2bdd16b"
        ],
        "9cfec596-0008-4b83-b3b4-7cda514c272d": [
            "016179fd-452d-43c6-9631-04d1a2bdd16b"
        ],
        "099a9686-3d64-4696-bc65-5720b65b930e": [
            "604b12ec-70c7-40b3-9aca-12d243feab63"
        ],
        "94b6e98d-88e3-49e8-80f5-393ad0ac2d2d": [
            "604b12ec-70c7-40b3-9aca-12d243feab63"
        ],
        "d42dcfd1-5c9d-4cf1-9588-df2c8b189ade": [
            "f5ba165a-6ee3-4d0e-ab6a-9e7017163ebc"
        ],
        "cca9e900-8b88-47b3-be8d-818bcc73d57b": [
            "f5ba165a-6ee3-4d0e-ab6a-9e7017163ebc"
        ],
        "4304cfc8-7409-4877-bf3c-cb6d7966ef10": [
            "48125e37-e167-4423-b687-fecb8c8e9535"
        ],
        "112fbebb-1886-4f9e-b891-128fd4f7244f": [
            "48125e37-e167-4423-b687-fecb8c8e9535"
        ],
        "a19f9969-d95d-47c1-bfac-f7e2c0c51a3d": [
            "d2175c4b-fc97-4ee2-9d49-5222cd5c19f2"
        ],
        "334bab01-b9f7-4e5a-bf6b-a9bc971e3ed5": [
            "d2175c4b-fc97-4ee2-9d49-5222cd5c19f2"
        ],
        "662ba9d8-5548-4f4c-a8f1-aedb4c7a931a": [
            "3dad7789-ccea-4704-90ad-e0f16a0e9a22"
        ],
        "3f972c9f-e556-4777-82e1-c643c6791260": [
            "3dad7789-ccea-4704-90ad-e0f16a0e9a22"
        ],
        "12fedda1-4650-4d94-b525-5f38f51d3a82": [
            "0203af0a-3fcd-46b1-8a32-6b62907594a0"
        ],
        "21a64d5b-a870-44e3-8c61-565fef7c7750": [
            "0203af0a-3fcd-46b1-8a32-6b62907594a0"
        ],
        "e3c9ef22-eb6e-43cd-a42e-621e79e4a064": [
            "4bfd1c59-5f32-45b3-b71d-78a7b51d01cc"
        ],
        "e6bcb95c-ebe0-406d-959f-758a111d99bc": [
            "4bfd1c59-5f32-45b3-b71d-78a7b51d01cc"
        ],
        "da72b5d0-90af-4792-82bc-946abf4368e4": [
            "9752f056-166e-4e08-8d80-2b82eba859a3"
        ],
        "62b7e2e9-0634-4c92-b242-059e3948d2df": [
            "9752f056-166e-4e08-8d80-2b82eba859a3"
        ],
        "591fc819-b8e6-4d8e-b507-65396e6bca0c": [
            "2607a0eb-f9e0-43e5-bf64-e7d38c8c9666"
        ],
        "67afffbb-7f34-487d-8842-31c7a21e6711": [
            "2607a0eb-f9e0-43e5-bf64-e7d38c8c9666"
        ],
        "b1ce0362-768d-4a3b-9d0b-b44e08e1ea8e": [
            "bcdfb757-defe-491b-91ba-6bf5f62ecbe3"
        ],
        "ad123b14-0298-4ad8-be49-582d76e01561": [
            "bcdfb757-defe-491b-91ba-6bf5f62ecbe3"
        ],
        "82847faa-9fbb-4b04-9f1d-b696a906ef95": [
            "f1a067f1-5504-47d9-969e-2b53ab00bed2"
        ],
        "02a15388-b172-4143-a40f-180a5d9d0fcc": [
            "f1a067f1-5504-47d9-969e-2b53ab00bed2"
        ],
        "361ce8b4-6a72-4257-91ec-5dd8177d62c1": [
            "6e35ac77-1eb6-4294-a84a-31200889ef7a"
        ],
        "fd2ffb26-30cd-4dd3-b3aa-b36ca73d3719": [
            "6e35ac77-1eb6-4294-a84a-31200889ef7a"
        ],
        "e5bc965b-c0ba-46b0-8ed8-b8df93f2e491": [
            "46c4dc16-e988-4290-a952-1c988ed29dde"
        ],
        "aa033f2b-26db-4ffc-8942-184ad8a05aa8": [
            "46c4dc16-e988-4290-a952-1c988ed29dde"
        ],
        "0b243fc5-65ea-40cb-8b9e-54b5a6349e03": [
            "70252ddc-d5c2-4248-994c-939e45107f39"
        ],
        "02ce2407-611f-49d4-848f-71010ba83d5f": [
            "70252ddc-d5c2-4248-994c-939e45107f39"
        ],
        "d79148cc-d27f-4405-8d6a-55d2b7f4f62f": [
            "f46ab9a8-951b-4a4f-a61d-1f5db7704a25"
        ],
        "f6d4ae34-8e36-4da0-ab4f-8b5dd6af12a0": [
            "f46ab9a8-951b-4a4f-a61d-1f5db7704a25"
        ],
        "2a1f3a19-1c5f-4e47-b2f1-a2b5d0aa84cf": [
            "a51e9ad1-305e-4d71-8a2b-623201b297a1"
        ],
        "97e5be0d-b4ef-434b-91c7-9795d88c8480": [
            "a51e9ad1-305e-4d71-8a2b-623201b297a1"
        ],
        "1f7e8a4c-b5a4-4f61-b6db-db9460434ab2": [
            "cd14f32f-d92d-424e-9cb0-02c3a8698ec9"
        ],
        "4cc004d1-e265-46e1-8eda-1d3e0a7827b2": [
            "cd14f32f-d92d-424e-9cb0-02c3a8698ec9"
        ],
        "eb0021e3-a185-41e2-91c1-4bf89565af9e": [
            "1fcdd61c-7856-402b-8520-7b9501235995"
        ],
        "671d5b6e-e64a-421f-bc6d-3cbfae493579": [
            "1fcdd61c-7856-402b-8520-7b9501235995"
        ],
        "1100f1d6-c70f-4aaa-b85e-9bea257b9c6b": [
            "4c6b6f8b-99bc-4d36-9b9b-f0d917164f73"
        ],
        "2c5d47c0-ca07-43c5-9b36-ca1e4549c860": [
            "4c6b6f8b-99bc-4d36-9b9b-f0d917164f73"
        ],
        "8a5149ae-0577-414e-9b33-8350fa4b0ee2": [
            "103da95b-703c-4bb0-9f21-d329ddf30086"
        ],
        "b79c227a-7b33-49a7-9bb9-ef8d503dbe22": [
            "103da95b-703c-4bb0-9f21-d329ddf30086"
        ],
        "e3b0935d-0947-4183-baa7-bd5f479a1f4b": [
            "ce392e31-ef22-4aa5-b4f9-1026f4514986"
        ],
        "6b3d6c68-bba8-41db-9c70-86f0d3754b94": [
            "ce392e31-ef22-4aa5-b4f9-1026f4514986"
        ],
        "bd5e6c99-2e0d-4f6a-be5d-d10a4b4f1a68": [
            "91e49272-b903-4d01-a526-dc4974bd9aff"
        ],
        "36bc4a41-02b3-48fa-bfdb-4f7da5ee9c9d": [
            "91e49272-b903-4d01-a526-dc4974bd9aff"
        ],
        "598f3647-724b-4c2c-957a-ee80f195d7e5": [
            "2b13a03f-b784-41e6-812e-a2695df6c1e2"
        ],
        "8e394a9d-08aa-45ec-942e-490383a42ad0": [
            "2b13a03f-b784-41e6-812e-a2695df6c1e2"
        ],
        "807dcf7c-fe50-44bd-9c2b-390464e05bd6": [
            "d4c22541-b523-4263-8553-0804ad7fc8d4"
        ],
        "d4945448-ebca-426c-98b5-aedebd16e668": [
            "d4c22541-b523-4263-8553-0804ad7fc8d4"
        ],
        "a9a8a9eb-2c15-4276-8b76-4e7387f5810f": [
            "9a9e7fd1-650c-46e5-bdd4-b0fccabaf28e"
        ],
        "020f2c19-6d27-48f6-bcd7-18b92f0977f0": [
            "9a9e7fd1-650c-46e5-bdd4-b0fccabaf28e"
        ],
        "94e29284-444e-459c-8eb1-39a72529316c": [
            "5e7a3653-c67c-4468-9949-22f301fcdae0"
        ],
        "e68ec953-2a81-4628-a7d4-411375399c7b": [
            "5e7a3653-c67c-4468-9949-22f301fcdae0"
        ],
        "81adbabd-cb32-4079-ab82-bf11a41bff53": [
            "416f6078-0adc-4a7d-a814-75d605704484"
        ],
        "5f0480e5-837d-474a-aa33-efbbf4bf67ea": [
            "416f6078-0adc-4a7d-a814-75d605704484"
        ],
        "1e55bd07-fbfd-4c8d-ade2-ea93df787294": [
            "b89ccfb4-f148-48c4-9515-12eaed114502"
        ],
        "a2f84ecf-e11c-4df9-ae10-3b048d763e3d": [
            "b89ccfb4-f148-48c4-9515-12eaed114502"
        ],
        "36953587-27ee-4c2b-9ea4-87f17acd7aeb": [
            "b826d7b5-8275-43c5-8759-b93917e3adb3"
        ],
        "506656ec-af4f-4f53-8c57-e167017a8a35": [
            "b826d7b5-8275-43c5-8759-b93917e3adb3"
        ],
        "a3bb9cc4-495a-4913-87a6-59c111518d4b": [
            "e155f36f-dadc-4d8e-8802-8c22a3233af2"
        ],
        "804edd78-7083-439c-8206-9bd9b3c167a3": [
            "e155f36f-dadc-4d8e-8802-8c22a3233af2"
        ],
        "90e12505-5a77-4c7d-a815-0e15606594fc": [
            "22ab336a-242f-4d5f-97d5-1fe7667b0a34"
        ],
        "6d5a8440-c802-479d-a368-c91d85db8b7c": [
            "22ab336a-242f-4d5f-97d5-1fe7667b0a34"
        ],
        "67d04257-6769-4486-8ab9-4852acda47cb": [
            "0b7fd46c-b6cb-46c4-8807-048444b6384a"
        ],
        "ab85aa33-f73d-4b77-9373-95e800dfc794": [
            "0b7fd46c-b6cb-46c4-8807-048444b6384a"
        ],
        "d65bb469-d124-4cba-8fe2-285decd66b3e": [
            "a227e1c6-af09-46d3-8053-1b223d4f879c"
        ],
        "b47e8c06-c9e8-454b-b778-c7458a3c757f": [
            "a227e1c6-af09-46d3-8053-1b223d4f879c"
        ],
        "652cd54c-5300-4aa0-a14f-7468d5a7b6c4": [
            "48d85012-f46e-4a78-9849-ad4021024d74"
        ],
        "709eca75-17f4-41b9-9836-acffc0b60c81": [
            "48d85012-f46e-4a78-9849-ad4021024d74"
        ],
        "b58c9c98-a8b2-422a-a46f-beab7c2f5e30": [
            "dd72ab70-4f95-44fa-b0e3-3d9b72124df9"
        ],
        "a5873295-3986-42e9-8af1-7081dfbca2d7": [
            "dd72ab70-4f95-44fa-b0e3-3d9b72124df9"
        ],
        "111e78d1-4870-4e3d-ba67-24bfcb575e1a": [
            "76146eb3-91f9-4aa0-8b13-d41f57dbf745"
        ],
        "09810ff8-9d3c-4ddb-a9c8-b6e99c082a06": [
            "76146eb3-91f9-4aa0-8b13-d41f57dbf745"
        ],
        "a1c7d705-61d8-48bf-b470-341d067572e3": [
            "ce383169-0f88-4a16-8343-687d87bc8dc7"
        ],
        "21b89b4a-8238-4336-a11d-4a6e5f6391a4": [
            "ce383169-0f88-4a16-8343-687d87bc8dc7"
        ],
        "c759d469-edd1-497c-81a2-c81943152bbd": [
            "ed26e426-8b00-47ef-805a-2094d38d9f78"
        ],
        "22aee8f7-39c9-4aaf-9a37-f3f5bb6a8250": [
            "ed26e426-8b00-47ef-805a-2094d38d9f78"
        ],
        "9e33bf58-eb9a-4293-b0fd-5508f46a49c4": [
            "de7e3dae-3d60-49ee-82e9-5b65bd19e542"
        ],
        "21deea53-b7bd-41b6-a98a-d774104d9dfd": [
            "de7e3dae-3d60-49ee-82e9-5b65bd19e542"
        ],
        "f8d90d86-0dd4-4f0a-b57c-c2f546f8dfb2": [
            "253de34f-98d6-4b61-95de-80ff2806617f"
        ],
        "007f4028-835e-45f2-849e-073724730847": [
            "253de34f-98d6-4b61-95de-80ff2806617f"
        ],
        "e8a50b0e-9d73-4ca5-924d-d1a0ab702f11": [
            "7c1afcc2-41d6-4854-af3e-85dcc2c7b047"
        ],
        "a005d229-585e-4460-877c-aaf0aab3a60d": [
            "7c1afcc2-41d6-4854-af3e-85dcc2c7b047"
        ],
        "3171a2d2-7ac8-4707-8026-c3bad3c397c7": [
            "38a69448-e2b5-42ec-9f44-4d6ccd61f1f2"
        ],
        "02546afd-f607-4e5e-b11b-1f20af705d40": [
            "38a69448-e2b5-42ec-9f44-4d6ccd61f1f2"
        ],
        "d60f2ea6-d3f3-419d-ab0f-e17cb231094f": [
            "7563fe34-f215-4762-9fc9-ebea894cb981"
        ],
        "285266e8-ce49-456d-994b-b1c553d11ad9": [
            "7563fe34-f215-4762-9fc9-ebea894cb981"
        ],
        "86ffb117-7ecf-418e-a4db-e8a4eac61d42": [
            "db1fd819-505b-405e-905b-8c246e3d0594"
        ],
        "a063ffaf-c85c-44ae-960c-85bdb279386f": [
            "db1fd819-505b-405e-905b-8c246e3d0594"
        ],
        "96530ba3-5ef7-420f-8924-ceab675e0935": [
            "55639e17-324b-41b3-a121-30328548a267"
        ],
        "2df4ccd8-c0c7-4204-9d93-bd09b9419fb2": [
            "55639e17-324b-41b3-a121-30328548a267"
        ],
        "4c02b228-e54b-41b4-bbd5-5e2aec493320": [
            "c42de0ac-e05c-4e28-bf1c-b35579f0d33a"
        ],
        "e9ec718c-2693-4abc-b911-2150139ae98d": [
            "c42de0ac-e05c-4e28-bf1c-b35579f0d33a"
        ],
        "54199cbe-5642-4ace-b456-e13fc128e1bd": [
            "ea76a260-919e-4efc-92de-a3427d5c15dc"
        ],
        "d4485d76-b16e-4ab5-b5cb-b670e664f815": [
            "ea76a260-919e-4efc-92de-a3427d5c15dc"
        ],
        "6d94c5bd-eb27-467b-940d-380881fc96a6": [
            "702a962a-618d-457f-924b-89f0d6b3f277"
        ],
        "d3cb892e-f960-4579-8d89-044cd21ed73f": [
            "702a962a-618d-457f-924b-89f0d6b3f277"
        ],
        "0244b4b6-3458-4016-8b2f-75762b8cbd9d": [
            "0b6aa4e2-ed96-4382-af9d-d6a4ec5a4d23"
        ],
        "80022e66-9d98-4921-a1c1-74f1622fa50f": [
            "0b6aa4e2-ed96-4382-af9d-d6a4ec5a4d23"
        ],
        "63c042de-fe3a-44ac-83db-51e85d814fce": [
            "6290beef-d181-47c4-a794-c54e9487b0ba"
        ],
        "7826dfb7-3aa5-4d47-b25f-c4526dae921c": [
            "6290beef-d181-47c4-a794-c54e9487b0ba"
        ],
        "38402a21-5be1-4ca2-bab7-934273d8bec5": [
            "c13c2a18-0ae0-4333-9bbf-3116ad4b993e"
        ],
        "76fad93f-00a2-40ec-b816-82f58beeb9f0": [
            "c13c2a18-0ae0-4333-9bbf-3116ad4b993e"
        ],
        "2fbbd033-c5ff-4d06-a004-c65cc5f8424a": [
            "f19abf20-e6e1-4274-989a-deebb77fb2f1"
        ],
        "8816bf6d-8e07-4cf3-8330-0a87822d9dae": [
            "f19abf20-e6e1-4274-989a-deebb77fb2f1"
        ],
        "c42d4b39-3d99-4a91-ac14-33a3cdb90a01": [
            "e03555fc-8f7c-4402-b662-6642bdeb5b75"
        ],
        "03ef5ddf-fbdc-4597-875e-74b57b8432ba": [
            "e03555fc-8f7c-4402-b662-6642bdeb5b75"
        ],
        "511f8e6a-da0b-43b9-b083-6320a3b825a7": [
            "10215593-bdcb-4e26-9926-f7c5454f4516"
        ],
        "220a9f15-3ca4-4b23-9250-90a15e6b4515": [
            "10215593-bdcb-4e26-9926-f7c5454f4516"
        ],
        "dc636933-808c-4f4e-97ea-875a7a634083": [
            "7695be93-d6e3-432c-b74b-7e955b7f09b9"
        ],
        "e9ae5c57-2856-4849-834c-65c98562bf2a": [
            "7695be93-d6e3-432c-b74b-7e955b7f09b9"
        ],
        "0ea97bd9-41fe-4a1b-9a28-26fee07d9471": [
            "cc2cee92-b656-4562-874d-82cf473566fc"
        ],
        "e7a70e13-eb3e-4e82-839e-7644ea38dcf5": [
            "cc2cee92-b656-4562-874d-82cf473566fc"
        ],
        "fe080e24-3cd9-4d35-bb3b-8be4d28a4fbe": [
            "24b95fad-415a-44db-8f9a-4cfa60f84ef2"
        ],
        "ce33a0b9-3abb-48bc-81f8-371c096ad656": [
            "24b95fad-415a-44db-8f9a-4cfa60f84ef2"
        ],
        "2edaa5b6-eaaf-4c80-9fdd-3e1ffb2c972f": [
            "e703e753-e4d7-4bd9-af1e-d2bb980ef3b1"
        ],
        "1f294182-cb51-44d2-b0ec-3469ec88b1e4": [
            "e703e753-e4d7-4bd9-af1e-d2bb980ef3b1"
        ],
        "aeb674eb-21ac-4fec-99b5-894dc440de88": [
            "2fd92583-d569-49b6-adab-85f0c6a0013f"
        ],
        "7e142b28-6390-4b3b-b1d9-a6a3fe2ebfef": [
            "2fd92583-d569-49b6-adab-85f0c6a0013f"
        ],
        "e4489876-6a55-450b-b1c5-f56d55d72f47": [
            "65d7c9a7-94c2-4462-8162-dee932b8a5cc"
        ],
        "fc3c66d3-647f-4c21-a371-23a9ad050a85": [
            "65d7c9a7-94c2-4462-8162-dee932b8a5cc"
        ],
        "b119822e-61af-4194-bb41-e19eefab9895": [
            "954b7c95-b23b-47d0-9f1b-cea867748f36"
        ],
        "94a9bee9-daf1-4f10-b51e-10d6e44f7c29": [
            "954b7c95-b23b-47d0-9f1b-cea867748f36"
        ],
        "8a99234b-b426-4d3e-aa6f-9a9be49c651b": [
            "5392635d-e24e-438b-aa22-993170f70ced"
        ],
        "8cead3ca-4a5e-48e4-8c22-b2cfe64a699f": [
            "5392635d-e24e-438b-aa22-993170f70ced"
        ],
        "1a23268f-df06-46a3-8ddd-ea2bc32e778b": [
            "3143c3e5-4051-471e-8760-87f5c572ba14"
        ],
        "19167292-4d74-4df0-b143-ea99dfa5a80a": [
            "3143c3e5-4051-471e-8760-87f5c572ba14"
        ],
        "03d13434-f401-40e9-99e6-c5d4a8f81408": [
            "bc091829-e274-406b-8473-289de6866e77"
        ],
        "ac532322-9db3-49c9-912a-cd0ccc86c6f3": [
            "bc091829-e274-406b-8473-289de6866e77"
        ],
        "ebe24df6-bc9d-4aec-ae8b-fe5aae041915": [
            "552bac82-fb38-405f-b172-5c8d52518b75"
        ],
        "56c78ab7-60d1-4a1e-aac3-e602f1c643e4": [
            "552bac82-fb38-405f-b172-5c8d52518b75"
        ],
        "b330f4c9-0c27-4411-a07c-a2c61904b4e0": [
            "ac9b275f-72c0-42e5-b238-48bb3c03dd50"
        ],
        "32970952-767b-47bd-9106-06e21781a550": [
            "ac9b275f-72c0-42e5-b238-48bb3c03dd50"
        ],
        "8b934969-cf81-4385-b1ca-2efc1b031fce": [
            "015aa862-7bba-425b-b57f-54bb003ee8a9"
        ],
        "3d8461d0-0ffb-44bf-b7fb-dee466ede049": [
            "015aa862-7bba-425b-b57f-54bb003ee8a9"
        ],
        "b987e75d-f99f-449a-ae55-c11e8250e939": [
            "c58b49f6-bfba-4c33-8e00-ba631ad3d6a5"
        ],
        "a4814bc6-4d25-4c01-99c6-0229adab4dde": [
            "c58b49f6-bfba-4c33-8e00-ba631ad3d6a5"
        ],
        "99cc8dad-114f-441d-bb8c-9038c17be2c4": [
            "8119742c-31bb-4a2a-b6f2-fb022596c73e"
        ],
        "e89c1ee9-afb7-4f08-bda5-a04096abd363": [
            "8119742c-31bb-4a2a-b6f2-fb022596c73e"
        ],
        "e5caa735-9376-4095-bb28-498ca9bb4617": [
            "e4b52e58-35f6-490f-b683-991f09ba458f"
        ],
        "dde8b845-ab59-4bbc-8018-38ada8ebe557": [
            "e4b52e58-35f6-490f-b683-991f09ba458f"
        ],
        "7b1f9d11-2dde-4cef-8598-cdd9eb17fbe7": [
            "1ed00131-1bfd-45c2-b763-796361fa8c5c"
        ],
        "bc214b78-0cba-43dc-bdeb-59ee8064be13": [
            "1ed00131-1bfd-45c2-b763-796361fa8c5c"
        ],
        "a5bf7053-9c4c-498f-9632-8f69f43c5773": [
            "5371d7a7-1988-4ca7-8341-fcd7e1f4aaa3"
        ],
        "aa999430-dd6d-4a31-9ecf-807320ce173a": [
            "5371d7a7-1988-4ca7-8341-fcd7e1f4aaa3"
        ]
    },
    "mode": "text"
}